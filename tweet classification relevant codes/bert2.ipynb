{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jafar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "import transformers as ppb # pytorch transformers\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text)   \n",
    "\n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', '', text)\n",
    "\n",
    "    #removing some special charachter  \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`;,@:~!=%&'’]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', '', text) \n",
    "    \n",
    "    #removing numbers\n",
    "    text=re.sub(\"(\\d+)\",' ',text)\n",
    "    \n",
    "    #removing numbers (not attached)\n",
    "#    text=re.sub(r\"(^|\\s)-?(\\d+)\\.?(\\d*)(\\s)\", '', text)\n",
    "#    text=re.sub(\"(^||\\d+)\\.(\\d+) \",'',text)\n",
    "#    text=re.sub(\"[0-9+]\\.?(\\d+)(\\.)\",' ',text)\n",
    "#    text=re.sub(\"[0-9+]\\.?(\\d+)($)\",' ',text)\n",
    "#    print(text)\n",
    "    \n",
    "#    return text\n",
    "\n",
    "#def lower_case (text):\n",
    "#    text = text.lower() \n",
    "#    return text_lower\n",
    "\n",
    "#def tokenization (text):\n",
    "#    text= nltk.word_tokenize(text)\n",
    "#    print (text)\n",
    "#    return text_tokens\n",
    "\n",
    "#def removing_stopwords (text):\n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()]\n",
    "#    text = [word for word in text if word not in stopwords.words()]\n",
    "#    print(text)\n",
    "#    return text_without_sw\n",
    "\n",
    "#def stemming (text):\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    #text_stems=stemmer.stem(tokens_without_sw)\n",
    "#    text = [stemmer.stem(word) for word in text]\n",
    "#    print(text)\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    print(text,'\\n')   \n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['There is framework generates embeddings for each input sentence',\n",
    "            'Sentences are passed as a list of string.', \n",
    "            'The quick brown wolf  jumps over the lazy dog.',\n",
    "            'I like lion alot I admire it ',\n",
    "            'many people love lion ',\n",
    "            'lion is powerful',\n",
    "            'lions are powerful',\n",
    "            'I LOVE LION',\n",
    "            'There ’s framework generates embeddings for each input sentence']\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_embeddings))\n",
    "print(len(sentence_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emed_ind=[]\n",
    "for sen in sentences:\n",
    "    emed_ind.append(model.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(emed_ind))\n",
    "print(len(emed_ind[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emed_ind[0][0]==sentence_embeddings[0] #false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor sentence, embedding in zip(sentences, sentence_embeddings):\\n    print(\"Sentence:\", sentence)\\n #   print(\"Embedding:\", embedding)\\n    print(\"\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    " #   print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9629046320915222\n",
      "0.8583279848098755\n",
      "0.9233848452568054\n",
      "0.9927574992179871\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "diff = 1 - cosine(sentence_embeddings[5], sentence_embeddings[6])\n",
    "print(diff)\n",
    "same = 1 - cosine(sentence_embeddings[4], sentence_embeddings[3])\n",
    "print(same)\n",
    "\n",
    "same2 = 1 - cosine(sentence_embeddings[3], sentence_embeddings[7])\n",
    "print(same2)\n",
    "\n",
    "same3 = 1 - cosine(sentence_embeddings[0], sentence_embeddings[8])\n",
    "print(same3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom scipy.spatial.distance import cosine\\n\\ndiff = 1 - cosine(sentence_embeddings[5], sentence_embeddings[6])\\nprint(diff)\\nsame = 1 - cosine(sentence_embeddings[4], sentence_embeddings[3])\\nprint(same)\\n\\nsame2 = 1 - cosine(sentence_embeddings[3], sentence_embeddings[7])\\nprint(same2)\\n\\nsame3 = 1 - cosine(sentence_embeddings[0], sentence_embeddings[8])\\nprint(same3)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sentences1=[]\n",
    "for sen in sentences:\n",
    "    sentences1.append(cleaning(sen))\n",
    "sentences1[0]\n",
    "'''\n",
    "\n",
    "'''\n",
    "sentence_embeddings = model.encode(sentences1)\n",
    "for sentence, embedding in zip(sentences1, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    " #   print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "'''    \n",
    "\n",
    "'''\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "diff = 1 - cosine(sentence_embeddings[5], sentence_embeddings[6])\n",
    "print(diff)\n",
    "same = 1 - cosine(sentence_embeddings[4], sentence_embeddings[3])\n",
    "print(same)\n",
    "\n",
    "same2 = 1 - cosine(sentence_embeddings[3], sentence_embeddings[7])\n",
    "print(same2)\n",
    "\n",
    "same3 = 1 - cosine(sentence_embeddings[0], sentence_embeddings[8])\n",
    "print(same3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenc1 = ['There is framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown wolf  jumps over the lazy dog.',\n",
    "            'I like lion alot I admire it ',\n",
    "            'many people love lion ',\n",
    "            'lion is powerful']\n",
    "sentenc2 = ['lions are powerful',\n",
    "            'I LOVE LION',\n",
    "            'There ’s framework generates embeddings for each input sentence']\n",
    "sentence_embedding1 = model.encode(sentenc1)\n",
    "sentence_embedding2 = model.encode(sentenc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9629046320915222\n",
      "0.8583279848098755\n",
      "0.9233848452568054\n",
      "0.9927574992179871\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "diff = 1 - cosine(sentence_embedding1[5], sentence_embedding2[0])\n",
    "print(diff)\n",
    "same = 1 - cosine(sentence_embedding1[4], sentence_embedding1[3])\n",
    "print(same)\n",
    "\n",
    "same2 = 1 - cosine(sentence_embedding1[3], sentence_embedding2[1])\n",
    "print(same2)\n",
    "\n",
    "same3 = 1 - cosine(sentence_embedding1[0], sentence_embedding2[2])\n",
    "print(same3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_embedding2))\n",
    "print(len(sentence_embedding2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_embedding2[0]==sentence_embeddings[6] # false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['There is framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown wolf  jumps over the lazy  dog.',\n",
    "            'I like lion alot I admire it ',\n",
    "            'many people love lion ',\n",
    "            'lion is powerful',\n",
    "            'lions are powerful',\n",
    "            'I LOVE LION',\n",
    "            'There ’s framework generates embeddings for each input sentence',\n",
    "            'I   LOVE         LION     ',\n",
    "            '',\n",
    "            'There ’ s framework generates embeddings for each input sentence']\n",
    "sentence_embed=[]\n",
    "for sentence in sentences:\n",
    "    sentence_embed.append(model.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9629045724868774\n",
      "0.8583279252052307\n",
      "0.9233848452568054\n",
      "0.9927576780319214\n",
      "1.0\n",
      "0.25023797154426575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "diff = 1 - cosine(sentence_embed[5], sentence_embed[6])\n",
    "print(diff)\n",
    "same = 1 - cosine(sentence_embed[4], sentence_embed[3])\n",
    "print(same)\n",
    "\n",
    "same2 = 1 - cosine(sentence_embed[3], sentence_embed[7])\n",
    "print(same2)\n",
    "\n",
    "same3 = 1 - cosine(sentence_embed[0], sentence_embed[8])\n",
    "print(same3)\n",
    "\n",
    "same4 = 1 - cosine(sentence_embed[9], sentence_embed[7])\n",
    "print(same4)\n",
    "\n",
    "same5 = 1 - cosine(sentence_embed[9], sentence_embed[10])\n",
    "print(same5)\n",
    "\n",
    "1 - cosine(sentence_embed[8], sentence_embed[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['There is framework generates embeddings for each input sentence',\n",
    "            'Sentences are passed as a list of string.', \n",
    "            'The quick brown wolf  jumps over the lazy  dog.',\n",
    "            'I like lion alot I admire it ',\n",
    "            'many people love lion ',\n",
    "            'lion is powerful',\n",
    "            'lions are powerful',\n",
    "            'I LOVE LION',\n",
    "            'There ’s framework generates embeddings for each input sentence',\n",
    "#            'the see is blue'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_embedding = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lion_embedding = model.encode(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5269836187362671,\n",
       " 0.6499285101890564,\n",
       " -0.1370430886745453,\n",
       " 0.1616545170545578,\n",
       " 0.04988108202815056,\n",
       " 0.19501909613609314,\n",
       " 0.16941632330417633,\n",
       " 0.17842039465904236,\n",
       " 0.5310317873954773]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "sim=[]\n",
    "for sen in sen_embedding:\n",
    "     ans= 1 - cosine(sen, lion_embedding)\n",
    "     sim.append(ans)\n",
    "sim   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 1 1 2 2 1 0]\n",
      "0.007979869842529297\n",
      "[        inf 10.86120027 19.27667554 21.53402804  8.77033528 11.91414628\n",
      "  4.6796679   6.64826383  1.93722667]\n",
      "[0 8 1 3 7 4 5 6 2]\n",
      "[ 1.93722667 10.86120027 19.27667554  6.64826383  8.77033528  4.6796679\n",
      "  4.6796679   6.64826383  1.93722667]\n",
      "[-1  8  6  8  7  7  5  3  0]\n",
      "[[0 2]\n",
      " [3 5]\n",
      " [6 8]\n",
      " [3 8]\n",
      " [0 8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "import time \n",
    "\n",
    "t0 = time.time()\n",
    "clustering = OPTICS(min_samples=2).fit(sen_embedding)\n",
    "print(clustering.labels_)\n",
    "print( time.time() - t0)\n",
    "print(clustering.reachability_)\n",
    "print(clustering.ordering_)\n",
    "print(clustering.core_distances_)\n",
    "print(clustering.predecessor_)\n",
    "print(clustering.cluster_hierarchy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bar\n"
     ]
    }
   ],
   "source": [
    "[\"foo\", \"bar\", \"baz\"].index(\"bar\")\n",
    "xs=[\"foo\", \"bar\", \"baz\"]\n",
    "#[i for i, j in enumerate(['foo', 'bar', 'baz']) if j == 'bar']\n",
    "for i, j in enumerate(['foo', 'bar', 'baz']):\n",
    "    if j == 'bar':\n",
    "        print(i,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 8]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "aa=clustering.labels_\n",
    "\n",
    "bb=np.where(aa==0)\n",
    "list(bb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help([\"foo\", \"bar\", \"baz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import time \n",
    "\n",
    "t0 = time.time()\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(sen_embedding)\n",
    "print(clustering.labels_)\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02792525291442871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "t0 = time.time()\n",
    "clustering = kmeans.fit(sen_embedding)\n",
    "print( time.time() - t0)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02599126,  0.13011003,  1.20615665, ..., -0.29086496,\n",
       "        -0.41890414,  0.64663762],\n",
       "       [ 0.22536715,  0.41066814,  0.1160614 , ..., -0.12130176,\n",
       "        -0.11097692, -0.38311562],\n",
       "       [-0.7870037 ,  0.7854169 , -1.81107652, ..., -0.12256152,\n",
       "         1.11515939, -0.1423967 ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323.74769768403075"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "gmm = GMM(n_components=3).fit(sen_embedding)\n",
    "labels = gmm.predict(sen_embedding)\n",
    "print(labels.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
