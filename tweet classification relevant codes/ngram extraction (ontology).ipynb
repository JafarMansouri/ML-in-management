{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import html\n",
    "import xml.sax.saxutils as saxutils\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "import random\n",
    "import operator\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "import dataframe_image as dfi\n",
    "import math\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer    \n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    #text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    text=re.sub(\"\\+\",' + ', text)\n",
    "    text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text) \n",
    "    '''\n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "#    text=re.sub('(^)rt ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningA (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    #text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    #text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    #text=re.sub('Dollar|Dollars|Yen|Yens|Euros', ' money ', text)   # not euro \n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    text = re.sub(r'(HTTP://|HTTPS://)\\S+', '', text)\n",
    "\n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    text=re.sub(r'WWW\\.\\S+', '', text)\n",
    "\n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    '''\n",
    "    '''\n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    #text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"United States\",'USA', text)\n",
    "    text=re.sub(\"United Kingdom\",'UK', text)\n",
    "    text=re.sub(\" the US \",' USA ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"U\\.S\\.A\", 'USA', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    '''\n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text)  \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    #text=re.sub('(^)RT | RT ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47604376, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "Number of tweets in ent: 47604376\n",
      "Memory size of ent: 402267520\n"
     ]
    }
   ],
   "source": [
    "#df_ent = pd.read_csv('/archives1/Datasets/TweetsWorld/ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_ent0 = pd.read_csv('ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_ent0.shape)\n",
    "print(df_ent0.columns)\n",
    "#print(df_ent0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_ent=list(df_ent0[['user_id', 'tweet','tweet_created_at']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_ent= list(zip(df_ent0.user_id, df_ent.tweet))\n",
    "#rows_ent=df_ent0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "#df_ent = df_ent0[['user_id', 'tweet','tweet_created_at']]   \n",
    "\n",
    "del df_ent0\n",
    "\n",
    "print(\"Number of tweets in ent:\",len(rows_ent))  #rows_ent0\n",
    "print('Memory size of ent:',sys.getsizeof(rows_ent)) #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46502302, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "46502302\n",
      "memry size of mng: 402267520\n"
     ]
    }
   ],
   "source": [
    "df_mng0 = pd.read_csv('mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#df_mng0 = pd.read_csv('/archives1/Datasets/TweetsWorld/mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "print(df_mng0.shape)\n",
    "print(df_mng0.columns)\n",
    "#print(df_mng0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_mng=list(df_mng0[['user_id', 'tweet','tweet_created_at']].itertuples(index=False, name=None)) #rows_mng0\n",
    "#rows_mng= list(zip(df_mng0.user_id, df_mng.tweet))\n",
    "#rows_mng=df_mng0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "#df_mng = df_mng0[['user_id', 'tweet','tweet_created_at']]\n",
    "\n",
    "del df_mng0\n",
    "\n",
    "print(len(rows_mng)) #rows_mng0\n",
    "print('memry size of mng:', sys.getsizeof(rows_mng)) #rows_mng0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72182875, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "72182875\n",
      "memory size of public: 644355008\n"
     ]
    }
   ],
   "source": [
    "df_public = pd.read_csv('public_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "#df_public = pd.read_csv('/archives1/Datasets/TweetsWorld/public_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "\n",
    "print(df_public.shape)\n",
    "print(df_public.columns)\n",
    "#print(df_public.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_public00=list(df_public[['user_id', 'tweet', 'tweet_created_at']].itertuples(index=False, name=None))\n",
    "#rows_public= list(zip(df_public.user_id, df_public.tweet))\n",
    "#rows_public=df_public[['user_id','tweet']].apply(tuple, axis=1) \n",
    "del df_public\n",
    "print(len(rows_public00))\n",
    "print('memory size of public:', sys.getsizeof(rows_public00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53069041\n",
      "memory size of public: 477085840\n"
     ]
    }
   ],
   "source": [
    "rows_public=[]\n",
    "for i in rows_public00:\n",
    "    if ('2021' not in i[2] ) and ('2020-12' not in i[2]) and ('2020-11' not in i[2]) and ('2020-10' not in i[2]):\n",
    "        rows_public.append(i)\n",
    "\n",
    "        \n",
    "print(len(rows_public))\n",
    "del rows_public00\n",
    "\n",
    "\n",
    "print('memory size of public:',sys.getsizeof(rows_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet=' i lime the book and, book. It is good. #book '\n",
    "#sentences=nltk.sent_tokenize(tweet)\n",
    "\n",
    "#sentences\n",
    "\n",
    "#ddd=tknzr.tokenize(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "N=3\n",
    "for n in range(N-1):\n",
    "    output = list(ngrams(ddd, n+2))\n",
    "    print('------')\n",
    "    print(output)\n",
    "    for xx in output:\n",
    "        res=' '.join(xx)\n",
    "        print(res)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows_ent[0], cleaning(rows_ent[0][1]),cleaning(rows_ent[1][1]),cleaning(rows_ent[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ent_filtering.txt\", \"rb\") as fp:   \n",
    "    ent_filter = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621.1797671318054\n"
     ]
    }
   ],
   "source": [
    "N=10 # number of grams\n",
    "year='2014'\n",
    "ontology_ent=dict()\n",
    "\n",
    "t0=time.time()\n",
    "n_e=0\n",
    "for i, tweet_inf in enumerate(rows_ent):\n",
    "    if year in tweet_inf[2]:\n",
    "        if str(tweet_inf[0]) not in ent_filter:\n",
    "        \n",
    "            text=tweet_inf[1]\n",
    "            tweet=cleaning(text)\n",
    "            #print(tweet)\n",
    "            #print('----')\n",
    "            if (tweet !='') and  (tweet!=' ') and (tweet !=[]):\n",
    "                n_e +=1\n",
    "            \n",
    "            all_sentences=nltk.sent_tokenize(tweet)\n",
    "            for sentence in all_sentences:\n",
    "                sentence= re.sub(\"[\\\"\\“\\”\\.\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]{}`′’‘;•«,@:~!\\=%&]+\", ' ',sentence) \n",
    "                #print(sentence)\n",
    "                #print('+++++++++')\n",
    "                words=tknzr.tokenize(sentence)\n",
    "                #words=sentence.split()\n",
    "                for word in words:\n",
    "                    #ontology_words.setdefault(word, []).append(i)\n",
    "                \n",
    "                    if word in ontology_ent:\n",
    "                        ontology_ent[word].add(i)\n",
    "                    else:\n",
    "                        ontology_ent[word]={i}\n",
    "                \n",
    "                for n in range(N-1):\n",
    "                    outputs = list(ngrams(words, n+2))\n",
    "                \n",
    "                    for out in outputs:\n",
    "                        #print(out)\n",
    "                        result=' '.join(out)\n",
    "                        #print(result)\n",
    "                        #print('-----------')\n",
    "                        #ontology_ngram.setdefault(result, []).append(i)\n",
    "                    \n",
    "                        if result in ontology_ent:\n",
    "                            ontology_ent[result].add(i)\n",
    "                        else:\n",
    "                            ontology_ent[result]={i}\n",
    "                    \n",
    "print(time.time()-t0)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867005"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36894844"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ontology_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ontology_ent_new=ontology_ent\n",
    "\n",
    "ent_1=[]\n",
    "for key ,value in ontology_ent_new.items():\n",
    "    if len(value)==1:\n",
    "        ent_1.append(key) \n",
    "\n",
    "for key in ent_1:\n",
    "    del ontology_ent_new[key]\n",
    "\n",
    "len_ent=len(ontology_ent_new)\n",
    "len_ent\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_density=dict(sorted(ontology_ent.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "ent_density=dict((k, len(v)/n_e) for k,v in ent_density.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mng_filtering.txt\", \"rb\") as fp:   \n",
    "    mng_filter = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980.6644105911255\n"
     ]
    }
   ],
   "source": [
    "N=10 # number of grams\n",
    "year='2014'\n",
    "ontology_mng=dict()\n",
    "\n",
    "t0=time.time()\n",
    "n_m=0\n",
    "for i, tweet_inf in enumerate(rows_mng):\n",
    "    #print(tweet_inf[0])\n",
    "    if year in tweet_inf[2]:\n",
    "        if str(tweet_inf[0]) not in mng_filter:\n",
    "        \n",
    "            text=tweet_inf[1]\n",
    "            tweet=cleaning(text)\n",
    "      \n",
    "            if (tweet !='') and  (tweet!=' ') and (tweet !=[]):\n",
    "                n_m +=1\n",
    "            \n",
    "            all_sentences=nltk.sent_tokenize(tweet)\n",
    "            for sentence in all_sentences:\n",
    "                sentence= re.sub(\"[\\\"\\“\\”\\.\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]{}`′’‘;•«,@:~!\\=%&]+\", ' ',sentence) \n",
    "                words=tknzr.tokenize(sentence)\n",
    "                #words=sentence.split()\n",
    "                for word in words:\n",
    "                    #ontology_words.setdefault(word, []).append(i)\n",
    "                \n",
    "                    if word in ontology_mng:\n",
    "                        ontology_mng[word].add(i)\n",
    "                    else:\n",
    "                        ontology_mng[word]={i}\n",
    "                \n",
    "                for n in range(N-1):\n",
    "                    outputs = list(ngrams(words, n+2))\n",
    "                \n",
    "                    for out in outputs:\n",
    "                        result=' '.join(out)\n",
    "                        #ontology_ngram.setdefault(result, []).append(i)\n",
    "                    \n",
    "                        if result in ontology_mng:\n",
    "                            ontology_mng[result].add(i)\n",
    "                        else:\n",
    "                            ontology_mng[result]={i}\n",
    "                    \n",
    "print(time.time()-t0)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "806873"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36534335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ontology_mng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ontology_mng_new=ontology_mng\n",
    "\n",
    "mng_1=[]\n",
    "for key ,value in ontology_mng_new.items():\n",
    "    if len(value)==1:\n",
    "        mng_1.append(key) \n",
    "\n",
    "for key in mng_1:\n",
    "    del ontology_mng_new[key]\n",
    "\n",
    "len_mng=len(ontology_mng_new)\n",
    "len_mng\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_density=dict(sorted(ontology_mng.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "mng_density=dict((k, len(v)/n_m) for k,v in mng_density.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"public_filtering.txt\", \"rb\") as fp:   \n",
    "    public_filter = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996.3038105964661\n"
     ]
    }
   ],
   "source": [
    "N=10 # number of grams\n",
    "year='2014'\n",
    "ontology_public=dict()\n",
    "\n",
    "t0=time.time()\n",
    "n_p=0\n",
    "for i, tweet_inf in enumerate(rows_public):\n",
    "    if year in tweet_inf[2]:\n",
    "        if str(tweet_inf[0]) not in public_filter:\n",
    "\n",
    "            text=tweet_inf[1]\n",
    "            tweet=cleaning(text)\n",
    "        \n",
    "            if (tweet !='') and  (tweet!=' ') and (tweet !=[]):\n",
    "                n_p +=1\n",
    "        \n",
    "            all_sentences=nltk.sent_tokenize(tweet)\n",
    "            for sentence in all_sentences:\n",
    "                sentence= re.sub(\"[\\\"\\“\\”\\.\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]{}`′’‘;•«,@:~!\\=%&]+\", ' ',sentence) \n",
    "                words=tknzr.tokenize(sentence)\n",
    "                #words=sentence.split()\n",
    "                for word in words:\n",
    "                    #ontology_words.setdefault(word, []).append(i)\n",
    "                \n",
    "                    if word in ontology_public:\n",
    "                        ontology_public[word].add(i)\n",
    "                    else:\n",
    "                        ontology_public[word]={i}\n",
    "                \n",
    "                for n in range(N-1):\n",
    "                    outputs = list(ngrams(words, n+2))\n",
    "                \n",
    "                    for out in outputs:\n",
    "                        result=' '.join(out)\n",
    "                        #ontology_ngram.setdefault(result, []).append(i)\n",
    "                    \n",
    "                        if result in ontology_public:\n",
    "                            ontology_public[result].add(i)\n",
    "                        else:\n",
    "                            ontology_public[result]={i}\n",
    "                    \n",
    "print(time.time()-t0)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363278"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13455921"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ontology_public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ontology_public_new=ontology_public\n",
    "\n",
    "public_1=[]\n",
    "for key ,value in ontology_public_new.items():\n",
    "    if len(value)==1:\n",
    "        public_1.append(key) \n",
    "\n",
    "for key in public_1:\n",
    "    del ontology_public_new[key]\n",
    "\n",
    "len_public=len(ontology_public_new)\n",
    "len_public\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_density=dict(sorted(ontology_public.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "public_density=dict((k, len(v)/n_p) for k,v in public_density.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_p = 0.5/n_p\n",
    "ent_public_ratio=dict()\n",
    "for key, value in ent_density.items():\n",
    "    if key in public_density:\n",
    "        ent_public_ratio[key]= math.log10(value/public_density[key])\n",
    "    else:\n",
    "        ent_public_ratio[key]= math.log10(value/epsilon_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_public_ratio=dict(sorted(ent_public_ratio.items(), key=lambda x: x[1], reverse=True))\n",
    "ent_public_ratio_list=sorted(ent_public_ratio.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('for gemini', 3.579345693549572),\n",
       " ('more for gemini', 3.579345693549572),\n",
       " ('for pisces', 3.5001644475019473),\n",
       " ('more for pisces', 3.5001644475019473),\n",
       " ('for scorpio', 3.2609063825628826),\n",
       " ('more for scorpio', 3.2609063825628826),\n",
       " ('#wearethepeople', 3.2031424715483783),\n",
       " ('#tlt', 3.107938922354339),\n",
       " ('#kyadillikyalahore', 2.7769457033129146),\n",
       " ('#x0024', 2.770202816556564),\n",
       " ('inceptive', 2.752551264367765),\n",
       " ('inceptive solutions', 2.7506167645905655),\n",
       " ('bubblews', 2.7334800095318244),\n",
       " ('#irishbizparty', 2.732807206172008),\n",
       " ('bubblews via', 2.726704607185154),\n",
       " ('added this', 2.7099989136823015),\n",
       " ('added this to', 2.7099989136823015),\n",
       " ('closet on', 2.7092887017792946),\n",
       " ('just added this', 2.7092887017792946),\n",
       " ('just added this to', 2.7092887017792946)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_public_ratio_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_p = 0.5/n_p\n",
    "mng_public_ratio=dict()\n",
    "for key, value in mng_density.items():\n",
    "    if key in public_density:\n",
    "        mng_public_ratio[key]= math.log10(value/public_density[key])\n",
    "    else:\n",
    "        mng_public_ratio[key]= math.log10(value/epsilon_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_public_ratio=dict(sorted(mng_public_ratio.items(), key=lambda x: x[1], reverse=True))\n",
    "mng_public_ratio_list=sorted(mng_public_ratio.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('for scorpio', 3.405020919881844),\n",
       " ('more for scorpio', 3.404866996618881),\n",
       " ('chick on', 3.0640424573679015),\n",
       " ('her username is', 3.0633670381308282),\n",
       " ('kik wants', 3.0630289341963493),\n",
       " ('chick on kik', 3.0630289341963493),\n",
       " ('on kik wants', 3.0630289341963493),\n",
       " ('kik wants to', 3.0630289341963493),\n",
       " ('wants to chat', 3.0630289341963493),\n",
       " ('you her username', 3.0630289341963493),\n",
       " ('chick on kik wants', 3.0630289341963493),\n",
       " ('on kik wants to', 3.0630289341963493),\n",
       " ('kik wants to chat', 3.0630289341963493),\n",
       " ('wants to chat with', 3.0630289341963493),\n",
       " ('chat with you her', 3.0630289341963493),\n",
       " ('with you her username', 3.0630289341963493),\n",
       " ('you her username is', 3.0630289341963493),\n",
       " ('chick on kik wants to', 3.0630289341963493),\n",
       " ('on kik wants to chat', 3.0630289341963493),\n",
       " ('kik wants to chat with', 3.0630289341963493)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mng_public_ratio_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_m = 0.5/n_p\n",
    "ent_mng_ratio=dict()\n",
    "for key, value in ent_density.items():\n",
    "    if key in mng_density:\n",
    "        ent_mng_ratio[key]= math.log10(value/mng_density[key])\n",
    "    else:\n",
    "        ent_mng_ratio[key]= math.log10(value/epsilon_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_mng_ratio=dict(sorted(ent_mng_ratio.items(), key=lambda x: x[1], reverse=True))\n",
    "ent_mng_ratio_list=sorted(ent_mng_ratio.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#wearethepeople', 3.2031424715483783),\n",
       " ('#tlt', 3.107938922354339),\n",
       " ('#irishbizparty', 2.7783432957084933),\n",
       " ('#kyadillikyalahore', 2.7769457033129146),\n",
       " ('#x0024', 2.770202816556564),\n",
       " ('closet on', 2.7548247913157797),\n",
       " ('inceptive', 2.752551264367765),\n",
       " ('inceptive solutions', 2.7506167645905655),\n",
       " ('poshmark', 2.7085773265475073),\n",
       " ('to my closet', 2.7085773265475073),\n",
       " ('my closet on', 2.7085773265475073),\n",
       " ('on poshmark', 2.7078647841696157),\n",
       " ('closet on poshmark', 2.7078647841696157),\n",
       " ('i just added this', 2.7078647841696157),\n",
       " ('this to my closet', 2.7078647841696157),\n",
       " ('to my closet on', 2.7078647841696157),\n",
       " ('my closet on poshmark', 2.7078647841696157),\n",
       " ('i just added this to', 2.7078647841696157),\n",
       " ('added this to my closet', 2.7078647841696157),\n",
       " ('this to my closet on', 2.7078647841696157)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_mng_ratio_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_e = 0.5/n_p\n",
    "mng_ent_ratio=dict()\n",
    "for key, value in mng_density.items():\n",
    "    if key in ent_density:\n",
    "        mng_ent_ratio[key]= math.log10(value/ent_density[key])\n",
    "    else:\n",
    "        mng_ent_ratio[key]= math.log10(value/epsilon_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_ent_ratio=dict(sorted(mng_ent_ratio.items(), key=lambda x: x[1], reverse=True))\n",
    "mng_ent_ratio_list=sorted(mng_ent_ratio.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('her username', 3.140119546594088),\n",
       " ('her username is', 3.140119546594088),\n",
       " ('kik wants', 3.139781442659609),\n",
       " ('chick on kik', 3.139781442659609),\n",
       " ('on kik wants', 3.139781442659609),\n",
       " ('kik wants to', 3.139781442659609),\n",
       " ('with you her', 3.139781442659609),\n",
       " ('you her username', 3.139781442659609),\n",
       " ('chick on kik wants', 3.139781442659609),\n",
       " ('on kik wants to', 3.139781442659609),\n",
       " ('kik wants to chat', 3.139781442659609),\n",
       " ('chat with you her', 3.139781442659609),\n",
       " ('with you her username', 3.139781442659609),\n",
       " ('you her username is', 3.139781442659609),\n",
       " ('chick on kik wants to', 3.139781442659609),\n",
       " ('on kik wants to chat', 3.139781442659609),\n",
       " ('kik wants to chat with', 3.139781442659609),\n",
       " ('wants to chat with you', 3.139781442659609),\n",
       " ('to chat with you her', 3.139781442659609),\n",
       " ('chat with you her username', 3.139781442659609)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mng_ent_ratio_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
