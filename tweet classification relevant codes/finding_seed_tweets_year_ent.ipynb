{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "#### export PYTHONIOENCODING=utf-8  # at cmd of linux\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "import html\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "import operator\n",
    "from statistics import mean, median ,mode,variance,stdev, pvariance, pstdev\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import fpdf\n",
    "import pickle\n",
    "import xml.sax.saxutils as saxutils\n",
    "import sys\n",
    "import pandas\n",
    "import csv\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(cachedStopWords))\n",
    "#print(len(cachedStopWords))\n",
    "#print(type(cachedStopWords))\n",
    "nltk_stopwords=set(cachedStopWords)\n",
    "\n",
    "english_alghabet=['b','c','e','f','g','h','j','k','l','n','p','q','r','u','v','w','x','z']\n",
    "\n",
    "numbers_remove=['one','two','three','four','five','six','seven','eight','nine','ten','tens','twenty',\n",
    "                'fourty','fifty','sixty','seventy','eighty','ninety','hundred','hundreds','million','billion','trillion',\n",
    "                'millions','thousand','thousands','second','third','forth','tenth','billions','trillions'] \n",
    "\n",
    "\n",
    "miscellaneous_remove=['absolutely', 'actually', 'adieu', 'ain', \"ain't\", 'aint', 'almost',\n",
    "                       'awesome','awfully','amazing','interesting',\n",
    "                       'alright','alrighty', 'amoungst', 'anybody', 'anymore', 'anyways', 'apart', 'apparently', 'anytime',\n",
    "                       'appropriate',  'approximately', 'arent', 'behold', 'better', 'bravo','briefly','bad','best','brilliant',\n",
    "                       'bye', 'cant', 'certainly', 'chrissakes', 'clearly', 'completely',\n",
    "                       'congrat', 'congrats','congratulation', 'congratulations', 'consequently', 'cool', 'couldnt',\n",
    "                       'darnit', 'de','dear', 'definitely','disappointing', 'didn', 'doesn', 'don', 'downwards',\n",
    "                       'disgusting','dude','down','eg',\"e.g.\",'i.e.',\n",
    "                       'encore','entirely', 'especially', 'et', 'etc', 'everybody', 'ex', 'exactly', 'excellent',\n",
    "                       'fantastic','far', 'farewell','funny',\n",
    "                       'felicitation', 'felicitations','finally', 'fully','furthermore', 'gadzooks', \n",
    "                       'good', 'goodby','goodness', 'gracious', 'great', \n",
    "                       'greetings', 'hallo', 'hardly', 'hasnt', 'haven', 'hello', 'here','hi', 'hither','higher','hopefully',\n",
    "                       'here','there','including',\n",
    "                       'howbeit', 'ie', 'immediately', 'inasmuch', 'inner', 'insofar', 'instead', 'inward', 'important',\n",
    "                       'indeed','just', \"it'd\", \"it'll\", 'inside','kertyschoo', 'kg', 'km', 'lackaday', \n",
    "                       'largely', 'lately', 'later','lovely','large','big','small',\n",
    "                       'lest', 'let', 'lets', 'likely', 'little', 'ltd', 'lower','magnificent', 'mainly', 'marvelous',\n",
    "                       'myself','yourself','yourselves','himself','herself','hisself','ourselves','themsleves',\n",
    "                       'maybe', 'meantime', 'merely', 'minus', 'near', 'nearly', 'necessary', 'never', \n",
    "                       'non', 'normally', 'obviously', 'ok', 'okay', 'ones', 'outside', 'over','other','others','only',\n",
    "                       'overall', 'particular', 'particularly', 'please', 'plus', 'poorly', 'possible','up',\n",
    "                       'possibly', 'potentially', 'predominantly', 'presumably', 'previously','primarily', 'probably',\n",
    "                       'promising',\n",
    "                       'promptly', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref',\n",
    "                       'refs', 'regardless', 'related', 'relatively', 'respectively', 'resulting', 'right', 'sec', \n",
    "                       'secondly','self', 'selves', 'seriously', 'shall', 'shucks','somebody', 'somethan','sorry',\n",
    "                       'somewhat', 'soon', 'late' , 'sorry', 'stupid', 'sub', 'substantially', 'successfully', 'sufficiently',\n",
    "                       'useful',\n",
    "                       'super', 'sure', \"t's\", 'th', 'thank', 'thanks', 'thanx', \"that've\", 'thats', 'there', \"there'll\",\n",
    "                       \"there've\", 'thered', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyre', 'thorough',\n",
    "                       'then','thankfully','too','today','yesterday','tomorrow','night',\"morning\",'afternoon','noon','tonight',\n",
    "                       'evening','day','everyday', 'everynight','todays','nights','mornings','noons','afternoons','days',\n",
    "                       'evenings','week','month','year',\n",
    "                       'thoroughly', 'tnx', 'too','truly', 'twice', 'undoubtedly','unfortunately', 'unlike','unlikely',\n",
    "                       'unto',  'usually', 'vs', 'welcome', 'well', 'went', 'werent', 'what', 'whatever', 'wheres', 'widely',\n",
    "                       'wonderful', 'wont', 'wouldnt', 'wrong', 'worst','worse','www', 'yes', 'youd', 'youre', 'yummy', \n",
    "                       'zoinks','shit','literally','literal','pleasure','effective','fabulous','delighted',\n",
    "                       'saturday','sunday','monday','tuesday','wednesday','thursday', 'friday','past','future','suitable',\n",
    "                       'much','many','less','least','few','lots','lot','fewer','fewset','therefore','pm',\n",
    "                       'afaik', 'br', 'idk','smh','qotd', 'ftw','bfn','yw', 'icymi','fomo','smdh', 'b4','imho',\n",
    "                       'urdddd','fab' ,'delightful','absolute','pleasure','huge','latest','nowadays',\n",
    "                       'january','february','april','june','july','august','september','october',\n",
    "                       'november','december', 'autumn' ,'spring','winter','summer',\n",
    "                       'mr','madam','sir','mrs','easy', 'difficult',\n",
    "                       'weekend','south','north','west','east','asia','africa','europe','america','totally',\n",
    "                       'come', 'comes', 'coming', 'came', 'seems', 'gives', 'gave', 'makes', 'made', 'keeps', 'kept', \n",
    "                       'calls', 'called', 'says', 'saying', 'said', 'goes', 'went', 'gone', 'got', 'saw', 'seen', 'shows',\n",
    "                       'shown', 'took', 'taken', 'uses', 'moved', 'moves', 'puts',\n",
    "                       'using','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "                       'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use',\n",
    "                       'find', 'finds', 'finding','aka',\n",
    "                       'lol' , 'brb', 'lmk', 'ama', 'tbh', 'irl', \"tl;dr\", 'fml', 'bfn' ,' br', 'ht', \"hth\",'j/k', 'lmao' ] #cool\n",
    "                  \n",
    "                      \n",
    "interjection_remove=['aaaahh', 'aaah', 'aaargh', 'aaay', 'aagh', 'aah',\n",
    "                   'aargh', 'achoo', 'adios', 'ah', 'aha', 'ahem', 'ahh', 'ahhh',\n",
    "                   'ahoy', 'alas', 'allo', 'amen', 'areet', 'argh', 'arrggh',\n",
    "                   'arrividerci', 'asap', 'attaboy', 'avaunt', 'aw', 'aw', 'aww',\n",
    "                   'awww', 'ay', 'ay', 'aye', 'ayeaugh', 'bada', 'badum', 'bah',\n",
    "                   'bahaha', 'bam', 'bazinga', 'behold', 'bingce', 'bingo', 'blah',\n",
    "                   'blech', 'bleh', 'blimey', 'bonjour', 'boo', 'booh', 'boohoo',\n",
    "                   'booyah', 'bravo', 'brr', 'brrrr', 'btw', 'bwahaha', 'capeesh',\n",
    "                   'capisce', 'cheerio', 'cheers', 'ciao', 'cor', 'cowabunga',\n",
    "                   'crikey', 'cripes', 'da', 'dabba', 'dah', 'dammit', 'damn', 'dang',\n",
    "                   'darn', 'de', 'dee', 'di', 'dizamn', 'doh', 'doo', 'drat', 'duh',\n",
    "                   'dum', 'eeeek', 'eek', 'eep', 'egad', 'egads', 'eh', 'ehem', 'em',\n",
    "                   'er', 'eureka', 'eww', 'ewww', 'eyh', 'fiddledeedee', 'fie',\n",
    "                   'fore', 'foul', 'fuff', 'gah', 'gak', 'gee', 'geez', 'gesundheit',\n",
    "                   'giddyap', 'golly', 'gosh', 'grr', 'grrrr', 'ha', 'hah', 'haha',\n",
    "                   'hahaha', 'hallelujah', 'halloa', 'harrumph', 'harumph', 'haw',\n",
    "                   'heck', 'heck', 'heeey', 'heh', 'hehe', 'hey', 'hhh', 'hic', 'hm',\n",
    "                   'hmm', 'hmmm', 'hmmmm', 'hmmph', 'hmpf', 'ho', 'hola', 'hoo',\n",
    "                   'hooray', 'howdy', 'hrmm', 'hrmph', 'hrmph', 'hrrmph', 'hu', 'huh',\n",
    "                   'hullo', 'humph', 'hurrah', 'huzza', 'huzzah', 'ich', 'ick',\n",
    "                   'ixnay', 'jeepers', 'jeez', 'kaboom', 'kapow', 'kerwham', 'la',\n",
    "                   'lala', 'lo', 'lordy', 'meh', 'mhm', 'ml', 'mm', 'mmh', 'mmhm',\n",
    "                   'mmm', 'muahaha', 'mwah', 'mwahaha', 'na','nay','nah', 'nanu', 'nooo', 'nope',\n",
    "                   'nuh', 'oh', 'ohh', 'oho', 'oi', 'okeydoke', 'om', 'oof', 'ooh',\n",
    "                   'oomph', 'oooh', 'ooooh', 'oops', 'ouch', 'ow', 'oww', 'oy',\n",
    "                   'oyez', 'oyh', 'pew', 'pff', 'pffh', 'pfft', 'phew', 'phut',\n",
    "                   'phweep', 'phwoar', 'phwoarr', 'poof', 'poogh', 'prethee',\n",
    "                   'prithee', 'prosit', 'pssh', 'psst', 'queep', 'roger', 'salaam',\n",
    "                   'salam', 'sheesh', 'shh', 'shhh', 'shitfire', 'shoo', 'shoop',\n",
    "                   'shush', 'sigh', 'sssh', 'strewth', 'ta', 'tarnations', 'tchah',\n",
    "                   'teehee', 'tish', 'touché', 'tsk', 'tss', 'tut', 'uggh', 'ugh',\n",
    "                   'uh', 'uhh', 'uhm', 'um', 'umm', 'ummm', 'umph', 'unh', 'upadaisy',\n",
    "                   'upsadaisy', 'ur', 'urgh', 'vay', 'vayf', 'viva', 'voila', 'waa',\n",
    "                   'waaaaah', 'waah', 'wah', 'wahey', 'wassup', 'weee', 'welp',\n",
    "                   'wham', 'whamo', 'whee', 'whew', 'whizz', 'whoa',\n",
    "                   'whoo', 'whoopee','whoop', 'whoops', 'whoopsy', 'whoosh', 'woah', 'woo',\n",
    "                   'woohoo', 'wotcha', 'wotcher', 'wow', 'wowsers', 'wowsers',\n",
    "                   'wuzzup', 'wuzzup', 'wuzzup', 'ya', 'yabba', 'yada', 'yadda',\n",
    "                   'yak', 'yarooh', 'yay', 'yea', 'yeah', 'yech', 'yee', 'yeeeeaah',\n",
    "                   'yeehaw', 'yeow', 'yes', 'yessiree', 'yew', 'yikes', 'yippee',\n",
    "                   'yo', 'yoo', 'yoohoo', 'yow', 'yowza', 'yuck', 'yuh', 'zing',\n",
    "                   'zoiks', 'zomfg', 'zomg', 'zounds', 'zut']\n",
    "             \n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "type(spacy_stopwords)\n",
    "#spacy_exclude=['using','name','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "#               'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use']# serious\n",
    "\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words1 = get_stop_words('english')\n",
    "#print(type(stop_words1))\n",
    "#print()\n",
    "#print(stop_words1)\n",
    "lib_stopwords=set(stop_words1)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#print(type(ENGLISH_STOP_WORDS))\n",
    "#print()\n",
    "#print(set(ENGLISH_STOP_WORDS))\n",
    "#sklearn_exclude=['find','get','found','go','see','seem','seems','give','seemed','take','keep','show','put','made'] # system  cry\n",
    "sklearn_stopwords=set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "#spacy_stopwords.difference_update(set(spacy_exclude))\n",
    "#sklearn_stopwords.difference_update(set(sklearn_stopwords))\n",
    "#for removing \"just\" one item, use \"remove\"\n",
    "\n",
    "temp_1=set(nltk_stopwords)\n",
    "temp_1.update(lib_stopwords)\n",
    "temp_1.update(sklearn_stopwords)\n",
    "temp_1.update(spacy_stopwords)\n",
    "temp_1.update(set(english_alghabet))\n",
    "temp_1.update(set(numbers_remove)) \n",
    "temp_1.update(set(miscellaneous_remove))\n",
    "temp_1.update(set(interjection_remove))\n",
    "cachedStopWords=temp_1\n",
    "#print(cachedStopWords)\n",
    "cachedStopWords.update(['rt','be','will','was','were','is','am','are','have','has','had','do','does','done'])\n",
    "len(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "#print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "#print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "#print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower    \n",
    " \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '''\"''', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"' \", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "    \n",
    "    #removing common expressions\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "#    text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "#    text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"\\+\",' + ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    #text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    #text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    #text= re.sub(\"(!)+\", '! ',text)     \n",
    "    #text= re.sub(\"(\\.\\.)+\", ' ',text)  \n",
    "    \n",
    "    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "    text= re.sub(\"[\\\"\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}''_';•«»,@:~!\\=%&]+\", ' ',text) \n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)      \n",
    "\n",
    "    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "    text=' '.join(text)\n",
    "    text=re.sub(\"''\",' ', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "    text=re.sub(\"``\",' ', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we have buy the book'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_sentence(\"we have bought the books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT user_id, tweet from ent_world_tweets_ph2_light ;\") # ORDER BY floor(random() *47604376  LIMIT 3000000;\")\n",
    "#cur.execute(\"SELECT user_id, tweet from ent_2019_1000k \")\n",
    "rows_ent = cur.fetchall()\n",
    "# there is no repetative tweet\n",
    "con.close()\n",
    "\n",
    "print(len(rows_ent))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47604376, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "47604376\n",
      "size of ent: 402267520\n"
     ]
    }
   ],
   "source": [
    "#df_ent = pandas.read_csv('/archives1/Datasets/TweetsWorld/ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_ent = pandas.read_csv('ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_ent.shape)\n",
    "print(df_ent.columns)\n",
    "#print(df_ent.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_ent=list(df_ent[['user_id', 'tweet','tweet_created_at']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_ent= list(zip(df_ent.user_id, df_ent.tweet))\n",
    "#rows_ent=df_ent[['user_id','tweet']].apply(tuple, axis=1) \n",
    "del df_ent\n",
    "print(len(rows_ent))  #rows_ent0\n",
    "print('size of ent:',sys.getsizeof(rows_ent)) #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rand_inds1=np.sort(random.sample(range(0, 47604376), 3000000))\n",
    "rows_ent=[]\n",
    "for i in rand_inds1:\n",
    "    rows_ent.append(rows_ent0[i])\n",
    "print(len(rows_ent) )  \n",
    "'''\n",
    "\n",
    "#del rows_ent0\n",
    "\n",
    "#with open(\"rand_inds1.txt\", \"wb\") as fp:   \n",
    "#  pickle.dump(rand_inds1, fp , protocol=4)\n",
    "\n",
    "#print('size of ent:',sys.getsizeof(rows_ent))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT user_id, tweet, tweet_created_at from public_world_tweets_ph2_light;\") # ORDER BY floor(random() *72182875  LIMIT 4000000;\")\n",
    "#cur.execute(\"SELECT user_id, tweet from public_2019_1000k \")\n",
    "rows_public = cur.fetchall()\n",
    "con.close()\n",
    "\n",
    "print(len(rows_public))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72182875, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "72182875\n",
      "size of public: 644355008\n"
     ]
    }
   ],
   "source": [
    "df_public = pandas.read_csv('public_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "#df_public = pandas.read_csv('/archives1/Datasets/TweetsWorld/public_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "\n",
    "print(df_public.shape)\n",
    "print(df_public.columns)\n",
    "#print(df_public.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_public0=list(df_public[['user_id', 'tweet', 'tweet_created_at']].itertuples(index=False, name=None))\n",
    "#rows_public= list(zip(df_public.user_id, df_public.tweet))\n",
    "#rows_public=df_public[['user_id','tweet']].apply(tuple, axis=1) \n",
    "del df_public\n",
    "print(len(rows_public0))\n",
    "print('size of public:', sys.getsizeof(rows_public0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_public=[]\n",
    "for i in rows_public0:\n",
    "    if ('2021' not in i[2] ) and ('2020-12' not in i[2]) and ('2020-11' not in i[2]) and ('2020-10' not in i[2]):\n",
    "        rows_public.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rows_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rand_inds3=np.sort(random.sample(range(0, 72182875), 4000000))  \n",
    "rows_public=[]\n",
    "for i in rand_inds3:\n",
    "    rows_public.append(rows_public0[i])\n",
    "print(len(rows_public))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rows_public0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"rand_inds3.txt\", \"wb\") as fp:   \n",
    "#  pickle.dump(rand_inds3, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of public:',sys.getsizeof(rows_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ent_filtering.txt\", \"rb\") as fp:   \n",
    "    ent_filter = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.366678476333618\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "ent_year_ind_all=set()\n",
    "#ent_users_year=set()\n",
    "ent_users_rows=[]\n",
    "#ent_tweets_rows=[]\n",
    "\n",
    "for k, i in enumerate(rows_ent):\n",
    "    ent_users_rows.append(i[0])\n",
    "    #ent_tweets_rows.append(cleaning(i[1]))\n",
    "    if year in i[2]:\n",
    "        if str(i[0]) not in ent_filter:\n",
    "            ent_year_ind_all.add(k)\n",
    "            #ent_users_year.add(i[0])\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ent_users_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4024615"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_year_ind_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "ent_year_ind_all_list=list(ent_year_ind_all)\n",
    "rand_2M_e=np.unique(np.sort(random.sample(range(0, len(ent_year_ind_all)), 2000000)))\n",
    "\n",
    "ent_year_ind=[]\n",
    "for i in rand_2M_e:\n",
    "    ent_year_ind.append(ent_year_ind_all_list[i])\n",
    "print(len(ent_year_ind) )  \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_year_ind=ent_year_ind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"_rand_2M_e.txt\", \"wb\") as fp:   \n",
    "#    pickle.dump(rand_2M_e,fp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_rand_2M_e.txt\", \"rb\") as fp:   \n",
    "    rand_2M_e=pickle.load(fp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"wb\") as fp:   \n",
    "#    pickle.dump(ent_year_ind, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.281065940856934\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "ent_year_ind_set=set(ent_year_ind)\n",
    "ent_users_year=set()\n",
    "for k, i in enumerate(rows_ent):\n",
    "    if year in i[2]:\n",
    "        if str(i[0]) not in ent_filter:\n",
    "            if k in ent_year_ind_set:\n",
    "                ent_users_year.add(i[0])\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11440"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_users_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "time.sleep(5*60)\n",
    "\n",
    "with open(\"ent_cleaning1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(ent_tweets_rows, fp , protocol=4)\n",
    "\n",
    "time.sleep(5*60)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ALL/ent_cleaning1.txt\", \"rb\") as fp:\n",
    "    ent_tweets_rows = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47604376"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_tweets_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_users_rows=[]\n",
    "for i in rows_ent:\n",
    "    ent_users_rows.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ent users: 23902\n"
     ]
    }
   ],
   "source": [
    "ent_users_rows_np=np.array(ent_users_rows)  \n",
    "ent_users=np.unique(ent_users_rows_np)\n",
    "print('number of ent users:',len(ent_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ent_all_words=[]\n",
    "for i in ent_tweets_rows:\n",
    "    ent_all_words.extend(i)\n",
    "print(len(ent_all_words))   \n",
    "\n",
    "dic_ent_user=dict()\n",
    "for user in ent_users_rows:\n",
    "    if user not in dic_ent_user:\n",
    "        dic_ent_user[user]=1\n",
    "    else:\n",
    "        dic_ent_user[user]+=1   \n",
    "sorted_x = sorted(dic_ent_user.items(), key=operator.itemgetter(1), reverse=False) \n",
    "#f = open(\"ent_user.txt\", \"w\")\n",
    "#for xx in sorted_x:\n",
    "#    f.write(str(xx)+'\\n')\n",
    "#f.close()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.29351162910461"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "ent_users_ind=dict()\n",
    "for k , i in enumerate(rows_ent):\n",
    "    if str(i[0]) not in ent_filter:\n",
    "        if i[0] in ent_users_ind:\n",
    "            ent_users_ind[i[0]].append(k)\n",
    "        else:\n",
    "            ent_users_ind[i[0]]=[k]\n",
    "time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21842"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_users_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2060"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"public_filtering.txt\", \"rb\") as fp:   \n",
    "    public_filter = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.75144839286804\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "public_year_ind_all=set()\n",
    "#public_users_year=set()\n",
    "public_users_rows=[]\n",
    "#public_tweets_rows=[]\n",
    "\n",
    "for k, i in enumerate(rows_public):\n",
    "    public_users_rows.append(i[0])\n",
    "#    public_tweets_rows.append(cleaning(i[1]))\n",
    "    if year in i[2]:\n",
    "        if str(i[0]) not in public_filter:\n",
    "            public_year_ind_all.add(k)\n",
    "            #public_users_year.add(i[0])\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2208768"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(public_year_ind_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "public_year_ind_all_list=list(public_year_ind_all)\n",
    "rand_2M_p=np.unique(np.sort(random.sample(range(0, len(public_year_ind_all)), 2000000)))\n",
    "\n",
    "public_year_ind=[]\n",
    "for i in rand_2M_p:\n",
    "    public_year_ind.append(public_year_ind_all_list[i])\n",
    "print(len(public_year_ind) )  \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"rand_2M_p.txt\", \"wb\") as fp:   \n",
    "#    pickle.dump(rand_2M_p, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"public_year_ind.txt\", \"wb\") as fp:   \n",
    "#    pickle.dump(public_year_ind, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"rand_2M_p.txt\", \"rb\") as fp:   \n",
    "    rand_2M_p=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"public_year_ind.txt\", \"rb\") as fp:   \n",
    "    public_year_ind=pickle.load( fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 36680627)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(public_year_ind),public_year_ind[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_users_year=set()\n",
    "public_year_ind_set=set(public_year_ind)\n",
    "\n",
    "for k, i in enumerate(rows_public):\n",
    "#    public_tweets_rows.append(cleaning(i[1]))\n",
    "    if year in i[2]:\n",
    "        if str(i[0]) not in public_filter:\n",
    "            if k in public_year_ind_set:\n",
    "                public_users_year.add(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7724"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(public_users_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "time.sleep(5*60)\n",
    "\n",
    "with open(\"public_cleaning1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(public_tweets_rows, fp , protocol=4)\n",
    "\n",
    "time.sleep(5*60)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ALL/public_cleaning1.txt\", \"rb\") as fp:   \n",
    "    public_tweets_rows = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_users_rows=[]\n",
    "for i in rows_public:\n",
    "    public_users_rows.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of public users: 34769\n"
     ]
    }
   ],
   "source": [
    "public_users_rows_np=np.array(public_users_rows)  \n",
    "public_users=np.unique(public_users_rows_np)\n",
    "print('number of public users:',len(public_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "public_all_words=[]\n",
    "for i in public_tweets_rows:\n",
    "    public_all_words.extend(i)\n",
    "print(len(public_all_words))\n",
    "\n",
    "dic_public_user=dict()\n",
    "for user in public_users_rows:\n",
    "    if user not in dic_public_user:\n",
    "        dic_public_user[user]=1\n",
    "    else:\n",
    "        dic_public_user[user]+=1   \n",
    "sorted_y = sorted(dic_public_user.items(), key=operator.itemgetter(1), reverse=False) \n",
    "#f = open(\"public_user.txt\", \"w\")\n",
    "#for yy in sorted_y:\n",
    "#    f.write(str(yy)+'\\n')\n",
    "#f.close()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.93988537788391"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "public_users_ind=dict()\n",
    "for k , i in enumerate(rows_public):\n",
    "    if str(i[0]) not in public_filter:\n",
    "        if i[0] in public_users_ind:\n",
    "            public_users_ind[i[0]].append(k)\n",
    "        else:\n",
    "            public_users_ind[i[0]]=[k]\n",
    "time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31622"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(public_users_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3426"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(public_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "dic_ent_1=dict()\n",
    "for tweet in ent_tweets_rows:\n",
    "    words=tweet.split()\n",
    "    words=np.unique(words)\n",
    "    for word in words:\n",
    "        if word not in dic_ent_1:\n",
    "            dic_ent_1[word]=1\n",
    "        else:\n",
    "            dic_ent_1[word] +=1    \n",
    "            \n",
    "dic_ent_1=dict(sorted(dic_ent_1.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print(time.time()-t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.022189378738403\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ijk=0\n",
    "dic_ent_1 = dict()\n",
    "for k in ent_users:\n",
    "    #print(k)\n",
    "    if k in ent_users_ind:\n",
    "        #bb=np.where(ent_users_rows_np==k)\n",
    "        #ind=list(bb[0])\n",
    "        ind=ent_users_ind[k]\n",
    "        \n",
    "        words_k=set()\n",
    "\n",
    "        for kk in ind:\n",
    "            if kk in ent_year_ind_set:\n",
    "                ijk +=1\n",
    "                \n",
    "                tweet_words=ent_tweets_rows[kk].split()\n",
    "                tweet_words=list(set(tweet_words))\n",
    "                for word in tweet_words:\n",
    "                    if word not in words_k:\n",
    "                        words_k.add(word)\n",
    "                        if word not in dic_ent_1:\n",
    "                            dic_ent_1[word] =1\n",
    "                        else:\n",
    "                            dic_ent_1[word] +=1\n",
    "                            \n",
    "dic_ent_1=dict(sorted(dic_ent_1.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ent_tweets=ijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171.266361951828\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ijk=0\n",
    "dic_ent_2 = dict()\n",
    "for k in ent_users:\n",
    "    if k in ent_users_ind:\n",
    "        #bb=np.where(ent_users_rows_np==k)\n",
    "        #ind=list(bb[0])\n",
    "        ind=ent_users_ind[k]\n",
    "        \n",
    "        combinations_k=set()\n",
    "    \n",
    "        for kk in ind:\n",
    "            \n",
    "            if kk in ent_year_ind_set:\n",
    "                ijk +=1\n",
    "                \n",
    "                tweet_words=ent_tweets_rows[kk].split()\n",
    "\n",
    "                tweet_combinations_2=set()\n",
    "                \n",
    "                for combi in itertools.combinations(np.sort(list(set(tweet_words))), 2):\n",
    "                    tweet_combinations_2.add(combi)       \n",
    "\n",
    "                for combi in tweet_combinations_2:\n",
    "                    if combi not in combinations_k:\n",
    "                        combinations_k.add(combi)\n",
    "                        if combi not in dic_ent_2:\n",
    "                            dic_ent_2[combi] =1\n",
    "                        else:\n",
    "                            dic_ent_2[combi] +=1\n",
    "                    \n",
    "dic_ent_2=dict(sorted(dic_ent_2.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "dic_ent_3 = dict()\n",
    "\n",
    "for k in ent_users:\n",
    "    bb=np.where(ent_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=ent_tweets_rows[kk].split()\n",
    "        \n",
    "        tweet_combinations_3=set()\n",
    "#        ccc=np.sort(list(set(tweet_words)))\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 3):\n",
    "            tweet_combinations_3.update([combi])       \n",
    "        \n",
    "        for combi in tweet_combinations_3:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.update([combi])\n",
    "                if combi not in dic_ent_3:\n",
    "                    dic_ent_3[combi] =1\n",
    "                else:\n",
    "                    dic_ent_3[combi] +=1\n",
    "                    \n",
    "dic_ent_3=dict(sorted(dic_ent_3.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.619404792785645\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ijk=0\n",
    "dic_public_1 = dict()\n",
    "for k in public_users:\n",
    "\n",
    "    if k  in public_users_ind:\n",
    "        #bb=np.where(public_users_rows_np==k)\n",
    "        #ind=list(bb[0])\n",
    "        ind=public_users_ind[k]\n",
    "        \n",
    "        words_k=set()\n",
    "\n",
    "        for kk in ind:\n",
    "            if kk in public_year_ind_set:\n",
    "                ijk +=1\n",
    "                \n",
    "                tweet_words=public_tweets_rows[kk].split()\n",
    "                for word in tweet_words:\n",
    "                    if word not in words_k:\n",
    "                        words_k.add(word)\n",
    "                        if word not in dic_public_1:\n",
    "                            dic_public_1[word] =1\n",
    "                        else:\n",
    "                            dic_public_1[word] +=1\n",
    "\n",
    "dic_public_1=dict(sorted(dic_public_1.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_public_tweets=ijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.71860146522522\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ijk=0\n",
    "dic_public_2 = dict()\n",
    "for k in public_users:\n",
    "\n",
    "    if k  in public_users_ind:\n",
    "        #bb=np.where(public_users_rows_np==k)\n",
    "        #ind=list(bb[0])\n",
    "        ind=public_users_ind[k]\n",
    "        \n",
    "        \n",
    "        combinations_k=set()\n",
    "    \n",
    "        for kk in ind:\n",
    "            if kk in public_year_ind_set:\n",
    "                ijk +=1\n",
    "                \n",
    "                tweet_words=public_tweets_rows[kk].split()\n",
    "        \n",
    "                tweet_combinations_2=set()\n",
    "                for combi in itertools.combinations(sorted(set(tweet_words)), 2):\n",
    "                    tweet_combinations_2.add(combi)      \n",
    "        \n",
    "                for combi in tweet_combinations_2:\n",
    "                    if combi not in combinations_k:\n",
    "                        combinations_k.add(combi)\n",
    "                        if combi not in dic_public_2:\n",
    "                            dic_public_2[combi] =1\n",
    "                        else:\n",
    "                            dic_public_2[combi] +=1\n",
    "                    \n",
    "dic_public_2=dict(sorted(dic_public_2.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "dic_public_3 = dict()\n",
    "\n",
    "for k in public_users:\n",
    "    bb=np.where(public_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=public_tweets_rows[kk].split()\n",
    "        \n",
    "        tweet_combinations_3=set()\n",
    "#        ccc=np.sort(list(set(tweet_words)))\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 3):\n",
    "            tweet_combinations_3.add(combi)       \n",
    "        \n",
    "        for combi in tweet_combinations_3:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.add(combi)\n",
    "                if combi not in dic_public_3:\n",
    "                    dic_public_3[combi] =1\n",
    "                else:\n",
    "                    dic_public_3[combi] +=1\n",
    "                    \n",
    "dic_public_3=dict(sorted(dic_public_3.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1779, 1403)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_ent_1['apple'],dic_public_1['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"ent1.txt\", \"rb\") as fp:   \n",
    "  dic_ent_1 = pickle.load(fp)\n",
    "\n",
    "with open(\"ent2.txt\", \"rb\") as fp:   \n",
    "  dic_ent_2 = pickle.load(fp)\n",
    "\n",
    "with open(\"mng1.txt\", \"rb\") as fp:   \n",
    "  dic_mng_1 = pickle.load(fp)\n",
    "\n",
    "with open(\"mng2.txt\", \"rb\") as fp:   \n",
    "  dic_mng_2 = pickle.load(fp)\n",
    "\n",
    "with open(\"public1.txt\", \"rb\") as fp:   \n",
    "  dic_public_1 = pickle.load(fp)\n",
    "\n",
    "with open(\"public2.txt\", \"rb\") as fp:   \n",
    "  dic_public_2 = pickle.load(fp)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_ent1_1.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(dic_ent_1, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"./years/myfile.txt\",\"w\")\n",
    "#f.write(\"Now the file has more content!\")\n",
    "#f.write(dic_ent_1)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle('my_file.pkl')\n",
    "#df = pd.read_pickle('my_file.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_e2=pd.DataFrame([dic_ent_2])\n",
    "#df_p2=pd.DataFrame([dic_public_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_ent2_1.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(dic_ent_2, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"mng1.txt\", \"wb\") as fp:   \n",
    "#  pickle.dump(dic_mng_1, fp , protocol=4)\n",
    "\n",
    "#with open(\"mng2.txt\", \"wb\") as fp:   \n",
    "#  pickle.dump(dic_mng_2, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_public1_1.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(dic_public_1, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_public2_1.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(dic_public_2, fp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dics:\n",
      "20971616\n",
      "671088744\n",
      "10485864\n",
      "671088744\n"
     ]
    }
   ],
   "source": [
    "print('size of dics:')\n",
    "print(sys.getsizeof(dic_ent_1))\n",
    "print(sys.getsizeof(dic_ent_2))\n",
    "\n",
    "print(sys.getsizeof(dic_public_1))\n",
    "print(sys.getsizeof(dic_public_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the_remove_low=0\n",
    "for word,freq in dic_ent_1.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_ent_1[word]=freq\n",
    "    else:\n",
    "        dic_ent_1[word]=0 #break\n",
    "\n",
    "\n",
    "\n",
    "for word,freq in dic_public_1.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_public_1[word]=freq\n",
    "    else:\n",
    "        dic_public_1[word]=0\n",
    "\n",
    "for word,freq in dic_ent_2.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_ent_2[word]=freq\n",
    "    else:\n",
    "        dic_ent_2[word]=0\n",
    "\n",
    "\n",
    "\n",
    "for word,freq in dic_public_2.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_public_2[word]=freq\n",
    "    else:\n",
    "        dic_public_2[word]=0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ent users:     23902\n",
      "number of public users:  34769\n"
     ]
    }
   ],
   "source": [
    "print('number of ent users:    ',len(ent_users))\n",
    "print('number of public users: ',len(public_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct words in ent:    470620\n",
      "number of distinct words in public: 302492\n"
     ]
    }
   ],
   "source": [
    "print('number of distinct words in ent:   ',len(dic_ent_1)) \n",
    "print('number of distinct words in public:', len(dic_public_1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten words in ent:\n",
      "[('time', 8787), ('like', 8760), ('people', 8540), ('need', 8435), ('know', 8405), ('new', 8331), ('work', 8138), ('want', 8035), ('love', 7890), ('look', 7783)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "f=open(\"ent_words_freqs.txt\",\"w\")\n",
    "for item in dic_ent_1.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "'''\n",
    "\n",
    "first_ten = list(dic_ent_1.items())[:10]\n",
    "print('first ten words in ent:')\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_ent_1_values=list(dic_ent_1.values())\n",
    "dic_ent_1_keys=list(dic_ent_1.keys())\n",
    "\n",
    "#print('length of dic_ent_1:             ',len(dic_ent_1_values))\n",
    "#print('mean of dic_ent_1:               ',mean(dic_ent_1_values))\n",
    "#print('median of dic_ent_1:             ',median(dic_ent_1_values))\n",
    "#print('mode of dic_ent_1:               ',mode(dic_ent_1_values))\n",
    "#var=variance(dic_ent_1_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_ent_1:           ',var)\n",
    "#print('standard deviation of dic_ent_1: ',std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten words in public\n",
      "[('like', 6118), ('love', 5880), ('time', 5867), ('know', 5777), ('people', 5665), ('want', 5622), ('need', 5527), ('look', 5458), ('new', 5388), ('life', 5368)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "f=open(\"public_words_freqs.txt\",\"w\")\n",
    "for item in dic_public_1.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "'''\n",
    "\n",
    "first_ten = list(dic_public_1.items())[:10]\n",
    "print('first ten words in public')\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_public_1_values=list(dic_public_1.values())\n",
    "dic_public_1_keys=list(dic_public_1.keys())\n",
    "\n",
    "#print('length of dic_public_1:             ',len(dic_public_1_values))\n",
    "#print('mean of dic_public_1:               ',mean(dic_public_1_values))\n",
    "#print('median of dic_public_1:             ',median(dic_public_1_values))\n",
    "#print('mode of dic_public_1:               ',mode(dic_public_1_values))\n",
    "#var=variance(dic_public_1_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_public_1:           ',var)\n",
    "#print('standard deviation of dic_public_1: ',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique words in ent (without public):  337553\n",
      "length of unique words in public (without ent):  169425\n",
      "length of common words in ent and public:        133067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ent_1_unique=set(dic_ent_1_keys)-set(dic_public_1_keys)\n",
    "public_1_unique_exclude_ent=set(dic_public_1_keys)-set(dic_ent_1_keys)\n",
    "ent_public_1_common=set(dic_ent_1_keys) & set(dic_public_1_keys)\n",
    "\n",
    "print('length of unique words in ent (without public): ',len(ent_1_unique))\n",
    "print('length of unique words in public (without ent): ',len(public_1_unique_exclude_ent))\n",
    "print('length of common words in ent and public:       ',len(ent_public_1_common))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dic_ent_1_unique_frequent=dict()\n",
    "for i in list(ent_1_unique):\n",
    "    temp=dic_ent_1[i] \n",
    "    if temp>=2:\n",
    "        dic_ent_1_unique_frequent[i]=temp\n",
    "        \n",
    "dic_ent_1_unique_frequent=dict(sorted(dic_ent_1_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f1=open(\"ent unique words and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_ent_1_unique_frequent.items():\n",
    "    f1.write(str(item)+'\\n')\n",
    "f1.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_mng_1_unique_frequent=dict()\n",
    "for i in list(mng_1_unique):\n",
    "    temp=dic_mng_1[i] \n",
    "    if temp>=2:\n",
    "        dic_mng_1_unique_frequent[i]=temp\n",
    "        \n",
    "dic_mng_1_unique_frequent=dict(sorted(dic_mng_1_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f2=open(\"frequent mng unique words and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_mng_1_unique_frequent.items():\n",
    "    f2.write(str(item)+'\\n')\n",
    "f2.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_public_1_unique_exclude_ent_frequent=dict()\n",
    "for i in list(public_1_unique_exclude_ent):\n",
    "    temp=dic_public_1[i] \n",
    "    if temp>=2:\n",
    "        dic_public_1_unique_exclude_ent_frequent[i]=temp\n",
    "        \n",
    "dic_public_1_unique_exclude_ent_frequent=dict(sorted(dic_public_1_unique_exclude_ent_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f3=open(\"frequent public unique words (excluding ent words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_1_unique_exclude_ent_frequent.items():\n",
    "    f3.write(str(item)+'\\n')\n",
    "f3.close  \n",
    "pass\n",
    "\n",
    "\n",
    "dic_public_1_unique_exclude_mng_frequent=dict()\n",
    "for i in list(public_1_unique_exclude_mng):\n",
    "    temp=dic_public_1[i] \n",
    "    if temp>=2:\n",
    "        dic_public_1_unique_exclude_mng_frequent[i]=temp\n",
    "        \n",
    "dic_public_1_unique_exclude_mng_frequent=dict(sorted(dic_public_1_unique_exclude_mng_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f4=open(\"frequent public unique words (excluding mng words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_1_unique_exclude_mng_frequent.items():\n",
    "    f4.write(str(item)+'\\n')\n",
    "f4.close  \n",
    "pass\n",
    "\n",
    "\n",
    "dic_ent_public_1_frequent=dict()\n",
    "for word in ent_public_1_common:\n",
    "    e=dic_ent_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    dic_ent_public_1_frequent[word]=[e,p]\n",
    "    \n",
    "dic_ent_public_1_frequent=dict(sorted(dic_ent_public_1_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f5=open(\"frequent ent words (common with public) and thier frequencies.txt\",\"w\")\n",
    "f5.write('first number shows frequency of the combination in ent and second number shows that of in public')\n",
    "for item in dic_ent_public_1_frequent.items():\n",
    "    e=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (e>50 and p<10):\n",
    "#        print(item[0], e,p)\n",
    "        f5.write(str(item)+'\\n')\n",
    "f5.close  \n",
    "pass    \n",
    "\n",
    "\n",
    "dic_mng_public_1_frequent=dict()\n",
    "for word in mng_public_1_common:\n",
    "    m=dic_mng_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    dic_mng_public_1_frequent[word]=[m,p]\n",
    "    \n",
    "dic_mng_public_1_frequent=dict(sorted(dic_mng_public_1_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f6=open(\"frequent mng words (common with public) and thier frequencies.txt\",\"w\")\n",
    "f6.write('first number shows frequency of the combination in mng and second number shows that of in public')\n",
    "for item in dic_mng_public_1_frequent.items():\n",
    "    m=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (m>50 and p<10):\n",
    "#        print(item[0], m,p)\n",
    "        f6.write(str(item)+'\\n')\n",
    "f6.close \n",
    "'''\n",
    "pass   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mng_public_1_freq=[]\n",
    "for word in dic_mng_1:\n",
    "    m=dic_mng_1[word]\n",
    "    try:\n",
    "        p=dic_public_1[word]\n",
    "    except:\n",
    "        p=0\n",
    "    if m>5 or p>5:    \n",
    "        mng_public_1_freq.append([m,p])\n",
    "    \n",
    "with open(\"mng_public_1_freq.txt\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(mng_public_1_freq, fp)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct combinations of two-words in ent:     22050342\n",
      "number of distinct combinations of two-words in public:  12705204\n"
     ]
    }
   ],
   "source": [
    "print('number of distinct combinations of two-words in ent:    ', len(dic_ent_2))\n",
    "print('number of distinct combinations of two-words in public: ',len(dic_public_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('like', 'look'), 3932), (('like', 'people'), 3105), (('feel', 'like'), 2854), (('medium', 'social'), 2852), (('know', 'people'), 2817), (('hard', 'work'), 2789), (('know', 'want'), 2694), (('know', 'need'), 2691), (('people', 'want'), 2574), (('people', 'time'), 2520)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "f=open(\"ent_combinations_freqs.txt\",\"w\")\n",
    "for item in dic_ent_2.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "'''\n",
    "first_ten = list(dic_ent_2.items())[:10]\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_ent_2_values=list(dic_ent_2.values())\n",
    "dic_ent_2_keys=list(dic_ent_2.keys())\n",
    "\n",
    "#print('length of dic_ent_2:             ',len(dic_ent_2_values))\n",
    "#print('mean of dic_ent_2:               ',mean(dic_ent_2_values))\n",
    "#print('median of dic_ent_2:             ',median(dic_ent_2_values))\n",
    "#print('mode of dic_ent_2:               ',mode(dic_ent_2_values))\n",
    "#var=variance(dic_ent_2_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_ent_2:           ',var)\n",
    "#print('standard deviation of dic_ent_2: ',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('like', 'look'), 3626), (('feel', 'like'), 3405), (('like', 'people'), 3029), (('know', 'like'), 2737), (('like', 'time'), 2482), (('like', 'love'), 2399), (('like', 'want'), 2357), (('know', 'people'), 2341), (('know', 'want'), 2178), (('like', 'need'), 2117)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "f=open(\"public_combinations_freqs.txt\",\"w\")\n",
    "for item in dic_public_2.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "'''\n",
    "first_ten = list(dic_public_2.items())[:10]\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_public_2_values=list(dic_public_2.values())\n",
    "dic_public_2_keys=list(dic_public_2.keys())\n",
    "\n",
    "#print('length of dic_public_2:             ',len(dic_public_2_values))\n",
    "#print('mean of dic_public_2:               ',mean(dic_public_2_values))\n",
    "#print('median of dic_public_2:             ',median(dic_public_2_values))\n",
    "#print('mode of dic_public_2:               ',mode(dic_public_2_values))\n",
    "#var=variance(dic_public_2_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_public_2:           ',var)\n",
    "#print('standard deviation of dic_public_2: ',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique combinations_2 in ent (without public):  17546635\n",
      "length of unique combinations_2 in public (without ent):  8201497\n",
      "length of common combinations_2 in ent and public:        4503707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ent_2_unique=set(dic_ent_2_keys)-set(dic_public_2_keys)\n",
    "public_2_unique_exclude_ent=set(dic_public_2_keys)-set(dic_ent_2_keys)\n",
    "ent_public_2_common=set(dic_ent_2_keys) & set(dic_public_2_keys)\n",
    "\n",
    "print('length of unique combinations_2 in ent (without public): ',len(ent_2_unique))\n",
    "print('length of unique combinations_2 in public (without ent): ',len(public_2_unique_exclude_ent))\n",
    "print('length of common combinations_2 in ent and public:       ',len(ent_public_2_common))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dic_ent_2_unique_frequent=dict()\n",
    "for i in list(ent_2_unique):\n",
    "    temp=dic_ent_2[i] \n",
    "    if temp>=2:\n",
    "        dic_ent_2_unique_frequent[i]=temp\n",
    "        \n",
    "dic_ent_2_unique_frequent=dict(sorted(dic_ent_2_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f1=open(\"ent unique combinations and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_ent_2_unique_frequent.items():\n",
    "    f1.write(str(item)+'\\n')\n",
    "f1.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_mng_2_unique_frequent=dict()\n",
    "for i in list(mng_2_unique):\n",
    "    temp=dic_mng_2[i] \n",
    "    if temp>=2:\n",
    "        dic_mng_2_unique_frequent[i]=temp\n",
    "        \n",
    "dic_mng_2_unique_frequent=dict(sorted(dic_mng_2_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f2=open(\"frequent mng unique 2-words combinations and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_mng_2_unique_frequent.items():\n",
    "    f2.write(str(item)+'\\n')\n",
    "f2.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_public_2_unique_exclude_ent_frequent=dict()\n",
    "for i in list(public_2_unique_exclude_ent):\n",
    "    temp=dic_public_2[i] \n",
    "    if temp>=2:\n",
    "        dic_public_2_unique_exclude_ent_frequent[i]=temp\n",
    "        \n",
    "dic_public_2_unique_exclude_ent_frequent=dict(sorted(dic_public_2_unique_exclude_ent_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f3=open(\"frequent public unique 2-words combinations (excluding ent words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_2_unique_exclude_ent_frequent.items():\n",
    "    f3.write(str(item)+'\\n')\n",
    "f3.close  \n",
    "pass\n",
    "\n",
    "dic_public_2_unique_exclude_mng_frequent=dict()\n",
    "for i in list(public_2_unique_exclude_mng):\n",
    "    temp=dic_public_2[i] \n",
    "    if temp>=2:\n",
    "        dic_public_2_unique_exclude_mng_frequent[i]=temp\n",
    "        \n",
    "dic_public_2_unique_exclude_mng_frequent=dict(sorted(dic_public_2_unique_exclude_mng_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f4=open(\"frequent public unique 2-words combinations (excluding mng words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_2_unique_exclude_mng_frequent.items():\n",
    "    f4.write(str(item)+'\\n')\n",
    "f4.close  \n",
    "pass\n",
    "\n",
    "dic_ent_public_2_frequent=dict()\n",
    "for word in ent_public_2_common:\n",
    "    e=dic_ent_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    dic_ent_public_2_frequent[word]=[e,p]\n",
    "    \n",
    "dic_ent_public_2_frequent=dict(sorted(dic_ent_public_2_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f5=open(\"frequent ent 2-words combinations (common with public) and thier frequencies.txt\",\"w\")\n",
    "f5.write('first number shows frequency of the combination in ent and second number shows that of in public')\n",
    "for item in dic_ent_public_2_frequent.items():\n",
    "    e=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (e>50 and p<10):\n",
    "#        print(item[0], e,p)\n",
    "        f5.write(str(item)+'\\n')\n",
    "f5.close  \n",
    "pass    \n",
    "\n",
    "dic_mng_public_2_frequent=dict()\n",
    "for word in mng_public_2_common:\n",
    "    m=dic_mng_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    dic_mng_public_2_frequent[word]=[m,p]\n",
    "    \n",
    "dic_mng_public_2_frequent=dict(sorted(dic_mng_public_2_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f6=open(\"frequent mng 2-words combinations (common with public) and thier frequencies.txt\",\"w\")\n",
    "f6.write('first number shows frequency of the combination in mng and second number shows that of in public')\n",
    "for item in dic_mng_public_2_frequent.items():\n",
    "    m=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (m>50 and p<10):\n",
    "#        print(item[0], m,p)\n",
    "        f6.write(str(item)+'\\n')\n",
    "f6.close \n",
    "'''\n",
    "pass      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rank_temp=1\n",
    "freq=dic_ent_1_values[0]\n",
    "rank_ent_1=dict()\n",
    "for i,j in dic_ent_1.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_ent_1[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_ent_1[i]=rank_temp\n",
    "\n",
    "#list(rank_ent_1.items())[0:10]\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_ent_2_values[0]\n",
    "rank_ent_2=dict()\n",
    "for i,j in dic_ent_2.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_ent_2[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_ent_2[i]=rank_temp\n",
    "\n",
    "#list(rank_ent_2.items())[0:10]\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_mng_1_values[0]\n",
    "rank_mng_1=dict()\n",
    "for i,j in dic_mng_1.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_mng_1[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_mng_1[i]=rank_temp\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_mng_2_values[0]\n",
    "rank_mng_2=dict()\n",
    "for i,j in dic_mng_2.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_mng_2[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_mng_2[i]=rank_temp\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_public_1_values[0]\n",
    "rank_public_1=dict()\n",
    "for i,j in dic_public_1.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_public_1[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_public_1[i]=rank_temp\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_public_2_values[0]\n",
    "rank_public_2=dict()\n",
    "for i,j in dic_public_2.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_public_2[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_public_2[i]=rank_temp\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pdf = fpdf.FPDF(format='letter')\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', size=12)\n",
    "temp='word,    ' + 'frequency in ent,    '+'rank in ent,   '+'frequency  in public,  '+'rank in public'\n",
    "pdf.write(5,temp)\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "for word in dic_ent_1_keys:\n",
    "    if word in ent_public_1_common:\n",
    "        if dic_ent_1[word] !=1:\n",
    "            e=dic_ent_1[word]\n",
    "            p=dic_public_1[word]\n",
    "            re=rank_ent_1[word]\n",
    "            rp=rank_public_1[word]\n",
    "            temp=(word,e,re,p,rp)\n",
    "            pdf.write(5,str(temp))\n",
    "            pdf.ln()\n",
    "\n",
    "pdf.output(\"ent_public_word_ranks.pdf\")   \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pdf = fpdf.FPDF(format='letter')\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', size=12)\n",
    "temp='combination,    ' + 'frequency in ent,    '+'rank in ent,   '+'frequency  in public,  '+'rank in public'\n",
    "pdf.write(5,temp)\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "for combi in dic_ent_2_keys:\n",
    "    if dic_ent_2[combi] !=1:\n",
    "        if combi in ent_public_2_common:\n",
    "            e=dic_ent_2[combi]\n",
    "            p=dic_public_2[combi]\n",
    "            re=rank_ent_2[combi]\n",
    "            rp=rank_public_2[combi]\n",
    "            temp=(combi,e,re,p,rp)\n",
    "            pdf.write(5,str(temp))\n",
    "            pdf.ln()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pdf.output(\"ent_public_combinations_rank.pdf\")   \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "frequency_ent_public_1=dict()\n",
    "for word in ent_public_1_common:\n",
    "    e=dic_ent_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    frequency_ent_public_1[word]=[e,p]\n",
    "\n",
    "frequency_mng_public_1=dict()\n",
    "for word in mng_public_1_common:\n",
    "    m=dic_mng_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    frequency_mng_public_1[word]=[m,p]\n",
    "\n",
    "frequency_ent_public_2=dict()\n",
    "for word in ent_public_2_common:\n",
    "    e=dic_ent_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    frequency_ent_public_2[word]=[e,p]\n",
    "\n",
    "frequency_mng_public_2=dict()\n",
    "for word in mng_public_2_common:\n",
    "    m=dic_mng_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    frequency_mng_public_2[word]=[m,p]\n",
    "\n",
    "frequency_ent_1_unique=dict()\n",
    "for word in ent_1_unique:\n",
    "    e=dic_ent_1[word]\n",
    "    frequency_ent_1_unique[word]=e\n",
    "\n",
    "frequency_mng_1_unique=dict()\n",
    "for word in mng_1_unique:\n",
    "    m=dic_mng_1[word]\n",
    "    frequency_mng_1_unique[word]=m\n",
    "\n",
    "frequency_public_1_unique_exclude_ent=dict()\n",
    "for word in public_1_unique_exclude_ent:\n",
    "    p=dic_public_1[word]\n",
    "    frequency_public_1_unique_exclude_ent[word]=p\n",
    "\n",
    "frequency_public_1_unique_exclude_mng=dict()\n",
    "for word in public_1_unique_exclude_mng:\n",
    "    p=dic_public_1[word]\n",
    "    frequency_public_1_unique_exclude_mng[word]=p\n",
    "\n",
    "frequency_ent_2_unique=dict()\n",
    "for word in ent_2_unique:\n",
    "    e=dic_ent_2[word]\n",
    "    frequency_ent_2_unique[word]=e\n",
    "\n",
    "frequency_mng_2_unique=dict()\n",
    "for word in mng_2_unique:\n",
    "    m=dic_mng_2[word]\n",
    "    frequency_mng_2_unique[word]=m\n",
    "\n",
    "frequency_public_2_unique_exclude_ent=dict()\n",
    "for word in public_2_unique_exclude_ent:\n",
    "    p=dic_public_2[word]\n",
    "    frequency_public_2_unique_exclude_ent[word]=p\n",
    "\n",
    "frequency_public_2_unique_exclude_mng=dict()\n",
    "for word in public_2_unique_exclude_mng:\n",
    "    p=dic_public_2[word]\n",
    "    frequency_public_2_unique_exclude_mng[word]=p\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "th_h_public=50\n",
    "th_l_public=5\n",
    "th_h_ent=50\n",
    "th_l_ent=5\n",
    "th_h_mng=50\n",
    "th_l_mng=5\n",
    "th_h=50\n",
    "th_l=5\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_ent_2=dict()\n",
    "for combi_2 in dic_ent_2:\n",
    "    \n",
    "    if combi_2 in ent_2_unique:\n",
    "        if dic_ent_2[combi_2]>th_h:\n",
    "            #print(combi_2)\n",
    "            weight_ent_2[combi_2]=2\n",
    "        else:\n",
    "            weight_ent_2[combi_2]=0\n",
    "                \n",
    "    else:\n",
    "        [e,p]=frequency_ent_public_2[combi_2]\n",
    "        if e>th_h and p<th_l:\n",
    "            #print(e,p,combi_2)\n",
    "            weight_ent_2[combi_2]=2\n",
    "        elif e<th_l and p>th_h:\n",
    "            weight_ent_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_ent_2[combi_2]=0\n",
    "'''       \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21842"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ent_user=len(ent_users_ind)\n",
    "len_ent_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31622"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_public_user=len(public_users_ind)\n",
    "len_public_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 2000000)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ent_tweets,num_public_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4477611940298507"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale= (num_ent_tweets*len_public_user)/(num_public_tweets*len_ent_user)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1779, 1403)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_ent_1['apple'],dic_public_1['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11440"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ent_user=len(ent_users_year)\n",
    "len_ent_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7724"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_public_user=len(public_users_year)\n",
    "len_public_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th=0.03\n",
    "weight_ent_2=dict()\n",
    "len_ent_user=len(ent_users_year)\n",
    "#len_ent_user=num_ent_tweets\n",
    "\n",
    "##len_ent_user=len(ent_users)\n",
    "##len_public_user=len(public_users)\n",
    "len_public_user=len(public_users_year)\n",
    "#len_public_user=num_public_tweets\n",
    "\n",
    "for combi_2 in dic_ent_2:\n",
    "    \n",
    "    if combi_2 in ent_2_unique:\n",
    "#        value= dic_ent_2[combi_2]/len_ent_user\n",
    "#        if   value >= th:\n",
    "#            weight_ent_2[combi_2]=2\n",
    "#            print(combi_2)\n",
    "#        else:\n",
    "#            weight_ent_2[combi_2]=0\n",
    "        weight_ent_2[combi_2]=2*dic_ent_2[combi_2]/len_ent_user\n",
    "\n",
    "    else:\n",
    "#        [e,p]=frequency_ent_public_2[combi_2]\n",
    "#        value=((e/len_ent_user)-(p/len_public_user) )\n",
    "#        if  value >= th:\n",
    "#        if (e/len_ent_user)>=th and (p/len_public_user)<=th :\n",
    "#            weight_ent_2[combi_2]=2\n",
    "#        elif  value <=-th:\n",
    "#        elif (e/len_ent_user)<=th and (p/len_public_user)>=th :\n",
    "#            weight_ent_2[combi_2]=-2\n",
    "#        else:\n",
    "#            weight_ent_2[combi_2]=0\n",
    "#        weight_ent_2[combi_2]=2*((e/len_ent_user)-(p/len_public_user) )\n",
    "        weight_ent_2[combi_2]=2*((dic_ent_2[combi_2]/len_ent_user)-(dic_public_2[combi_2]/len_public_user) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11440, 7724)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ent_user,len_public_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_mng_2=dict()\n",
    "for combi_2 in dic_mng_2:\n",
    "    \n",
    "    if combi_2 in mng_2_unique:\n",
    "        if dic_mng_2[combi_2]>th_h:\n",
    "            #print(combi_2)\n",
    "            weight_mng_2[combi_2]=2\n",
    "        else:\n",
    "            weight_mng_2[combi_2]=0\n",
    "                \n",
    "    else:\n",
    "        [m,p]=frequency_mng_public_2[combi_2]\n",
    "        if m>th_h and p<th_l:\n",
    "            #print(m,p,combi_2)\n",
    "            weight_mng_2[combi_2]=2\n",
    "        elif m<th_l and p>th_h:\n",
    "            weight_mng_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_mng_2[combi_2]=0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_public_2=dict()\n",
    "for combi_2 in dic_public_2:\n",
    "    \n",
    "    if (combi_2 in public_2_unique_exclude_ent) and (combi_2 in public_2_unique_exclude_mng):\n",
    "        if dic_public_2[combi_2]>50:\n",
    "            #print(combi_2)\n",
    "            weight_public_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_public_2[combi_2]=0\n",
    "                \n",
    "    else:\n",
    "#        p=dic_public_2[combi_2]\n",
    "        if combi_2 in mng_public_2:\n",
    "            [m,p]=frequency_mng_public_2[combi_2]\n",
    "        else:\n",
    "            m=0\n",
    "        if combi_2 in ent_public_2:  \n",
    "            [e,p]=frequency_ent_public_2[combi_2]\n",
    "        else:\n",
    "            e=0\n",
    "\n",
    "        if p>50 and m<5 and e<5:\n",
    "            #print(m,p,combi_2)\n",
    "            weight_public_2[combi_2]=-2\n",
    "        elif p<5 and (e>50:\n",
    "            weight_mng_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_mng_2[combi_2]=0\n",
    "''' \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# collecting words from relevant and irrelevant combination so that we can find the relevant and irrelevant single words based on a relation\n",
    "agg_2_words_rel_ent=set()\n",
    "agg_2_words_irrel_ent=set()\n",
    "for combi_2,weight in weight_ent_2.items():\n",
    "    if weight==2:\n",
    "        agg_2_words_rel_ent.update(combi_2)\n",
    "    elif weight==-2:\n",
    "        agg_2_words_irrel_ent.update(combi_2)\n",
    "\n",
    "\n",
    "# collecting words from relevant and irrelevant combination so that we can find the relevant and irrelevant single words based on a relation\n",
    "agg_2_words_rel_mng=set()\n",
    "agg_2_words_irrel_mng=set()\n",
    "for combi_2,weight in weight_mng_2.items():\n",
    "    if weight==2:\n",
    "        agg_2_words_rel_mng.update(combi_2)\n",
    "    elif weight==-2:\n",
    "        agg_2_words_irrel_mng.update(combi_2)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_ent_1=dict()\n",
    "for word in dic_ent_1:\n",
    "    \n",
    "    if (word in ent_1_unique):\n",
    "        if (dic_ent_1[word]>th_h): # and (word in agg_2_words_rel_ent):  \n",
    "            weight_ent_1[word]=1\n",
    "        else:\n",
    "            weight_ent_1[word]=0\n",
    "                \n",
    "    else:\n",
    "        [e,p]=frequency_ent_public_1[word]\n",
    "        if (e>th_h and p<th_l): # and (word in agg_2_words_rel_ent):\n",
    "            #print(e,p,word)\n",
    "            weight_ent_1[word]=1\n",
    "        elif (e<th_l and p>th_h):# and (word in agg_2_words_irrel_ent):\n",
    "            weight_ent_1[word]=-1\n",
    "        else:\n",
    "            weight_ent_1[word]=0\n",
    "'''    \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th=0.03\n",
    "weight_ent_1=dict()\n",
    "len_ent_user=len(ent_users_year)\n",
    "#len_ent_user=num_ent_tweets\n",
    "\n",
    "##len_ent_user=len(ent_users)\n",
    "##len_public_user=len(public_users)\n",
    "len_public_user=len(public_users_year)\n",
    "#len_public_user=num_public_tweets\n",
    "for word in dic_ent_1:\n",
    "    \n",
    "    \n",
    "    if (word in ent_1_unique):\n",
    "#        value= dic_ent_1[word]/len_ent_user\n",
    "#        if value >=th:\n",
    "#            weight_ent_1[word]=1\n",
    "#            print(word)\n",
    "#        else:\n",
    "#            weight_ent_1[word]=0  \n",
    "        weight_ent_1[word]=dic_ent_1[word]/len_ent_user\n",
    "    else:\n",
    "#        [e,p]=frequency_ent_public_1[word]\n",
    "#        value= (e/len_ent_user)-(p/len_public_user)\n",
    "#        if  value >=th:\n",
    "#        if (e/len_ent_user)>=th and (p/len_public_user)<=th: \n",
    "#            weight_ent_1[word]=1\n",
    "#        elif   value <= -th:\n",
    "#        elif  (e/len_ent_user)<=th and (p/len_public_user)>=th: \n",
    "#            weight_ent_1[word]=-1\n",
    "#        else:\n",
    "#            weight_ent_1[word]=0\n",
    "#        weight_ent_1[word]=(e/len_ent_user)-(p/len_public_user)\n",
    "        weight_ent_1[word]=(dic_ent_1[word]/len_ent_user)-(dic_public_1[word]/len_public_user)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.026134643450800882"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_ent_1['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_mng_1=dict()\n",
    "for word in dic_mng_1:\n",
    "    \n",
    "    if (word in mng_1_unique):\n",
    "        if (dic_mng_1[word]>th_h): # and (word in agg_2_words_rel_mng):  \n",
    "            weight_mng_1[word]=1\n",
    "        else:\n",
    "            weight_mng_1[word]=0\n",
    "                \n",
    "    else:\n",
    "        [m,p]=frequency_mng_public_1[word]\n",
    "        if (m>th_h and p<th_l): # and (word in agg_2_words_rel_mng):\n",
    "            #print(m,p,word)\n",
    "            weight_mng_1[word]=1\n",
    "        elif (m<th_l and p>th_h):# and (word in agg_2_words_irrel_mng):\n",
    "            weight_mng_1[word]=-1\n",
    "        else:\n",
    "            weight_mng_1[word]=0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ttt={('a','b'):25,('b','c'):40,('a','d'):30}\n",
    "\n",
    "for t in ttt:\n",
    "    x=set(t)-set('a')\n",
    "    print(x)\n",
    "\n",
    "strange=['a','b']\n",
    "\n",
    "add_strange=dict()\n",
    "for word in strange:\n",
    "#    print(word)\n",
    "    add=0\n",
    "    for k in ttt:\n",
    "#        print(k)\n",
    "        if word in k:\n",
    "            xxx=set(k)-set(word)\n",
    "            print(xxx)\n",
    "            if xxx in agg_2_words_rel:\n",
    "                print(k)\n",
    "                add += ttt[k]\n",
    "#                print(dic_ent_2[k])\n",
    "    add_strange[word]=add  \n",
    "'''  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_year_ind_list=sorted(ent_year_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.026134643450800882"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_ent_1['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ent_tweets_scores=dict()\n",
    "ijk\n",
    "#for k,tweet in enumerate(ent_tweets_rows):\n",
    "#    if k in ent_year_ind:\n",
    "for  index in ent_year_ind_list:\n",
    "        tweet=ent_tweets_rows[index]\n",
    "        words=tweet.split()\n",
    "        score_words=0\n",
    "        if len(words) != 0:\n",
    "            for word in words:\n",
    "                score_words += weight_ent_1[word]       \n",
    "\n",
    "            score_words_norm=score_words/len(words)\n",
    "            len_words=len(words)\n",
    "        \n",
    "        else:\n",
    "            len_words=0\n",
    "            score_words_norm=0\n",
    "\n",
    "\n",
    "        score_combi_2=0\n",
    "        if len(set(words)) >= 2:\n",
    "            combinations_2=[]\n",
    "            for combi in itertools.combinations(sorted(set(words)), 2):\n",
    "                combinations_2.append(combi)\n",
    "    \n",
    "            for combi in combinations_2:\n",
    "                score_combi_2 += weight_ent_2[combi]\n",
    "   \n",
    "\n",
    "            score_combinations2_norm=score_combi_2/len(combinations_2)\n",
    "            len_combination_2=len(combinations_2)           \n",
    "        else:\n",
    "            len_combination_2=0\n",
    "            score_combinations2_norm=0        \n",
    "        \n",
    "\n",
    "##    if len_words != 0:\n",
    "##        dic_ent_tweets_scores[k]= (score_words+score_combi_2)/(len_combination_2+len_words)\n",
    "##    else:\n",
    "##        dic_ent_tweets_scores[k]= 0\n",
    "\n",
    "\n",
    "#        dic_ent_tweets_scores[k]=  (score_combinations2_norm +  score_words_norm)\n",
    "        dic_ent_tweets_scores[index]=  (score_combinations2_norm +  score_words_norm)\n",
    "#        if len_combination_2<10:\n",
    "#            dic_ent_tweets_scores[k]=0\n",
    "    \n",
    "#    else:\n",
    "#        dic_ent_tweets_scores[k]=  0     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "dic_ent_tweets_scores=dict(sorted(dic_ent_tweets_scores.items(),key=operator.itemgetter(1),reverse=True))\n",
    "print(len(dic_ent_tweets_scores))\n",
    "\n",
    "ent_tweets_score = list(dic_ent_tweets_scores.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8428607, 0.4447795084252878), (35842959, -0.587412610046608))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_tweets_score[0], ent_tweets_score[-1], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_year_ind_list[ent_tweets_score[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows_ent[ent_year_ind_list[ent_tweets_score[0][0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17888284, 0.34436344985930695), 'start business business')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_tweets_score[1], ent_tweets_rows[ent_tweets_score[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets with positive score:  963673\n",
      "0.517493724822998\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "id_ent_rel=[]\n",
    "for score_id in ent_tweets_score:\n",
    "    if score_id[1]>0:\n",
    "        id_ent_rel.append(score_id[0])\n",
    "    else:\n",
    "        break\n",
    "print('number of tweets with positive score: ', len(id_ent_rel))\n",
    "tweets_rel_ent=[]  \n",
    "i_id=0\n",
    "\n",
    "\n",
    "dist_len=dict()\n",
    "#seed_tweets_ent=[]\n",
    "\n",
    "#for id_tweet in id_ent_rel:   \n",
    "#    temp = rows_ent[id_tweet][1]\n",
    "#    temp = strip_tags(temp)\n",
    "#    temp = re.sub(\"“|”\", '''\"''', temp)  \n",
    "#    temp = re.sub(\"’|′|‘|`\", \"'\", temp)  \n",
    "#    temp = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', temp)  \n",
    "#    temp = temp.lower() \n",
    "#    temp = re.sub(r'(http://|https://)\\S+', '', temp) \n",
    "#    temp = re.sub(r'www\\.\\S+', '', temp)\n",
    "#    temp = re.sub('( |^)@\\S+', '', temp)  \n",
    "#    temp= re.sub(\"[\\\"\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘'_;•«»,@:~!\\=%&]+\", ' ',temp) \n",
    "#    temp = temp.encode('utf8').decode('ISO-8859-1') # decode('ISO-8859-1') or decode('latin-1') or decode('windows-1252')\n",
    " \n",
    "#    len_tw=len(temp.split())\n",
    "#    if len_tw in dist_len:\n",
    "#        dist_len[len_tw] +=1\n",
    "#    else:\n",
    "#        dist_len[len_tw] =1\n",
    "      \n",
    "    \n",
    "    \n",
    "#    if temp not in tweets_rel_ent:\n",
    "#        tweets_rel_ent.append(temp)\n",
    "#        seed_tweets_ent.append(rows_ent[id_tweet][1])\n",
    "#        seed_tweets_ent.append(id_tweet)\n",
    "#        i_id +=1\n",
    "#    if i_id==200000:\n",
    "#        break\n",
    "        \n",
    "print(time.time()-t0)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('seed ent lenght:',len(seed_tweets_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dist_len=dict(sorted(dist_len.items(),key=operator.itemgetter(0),reverse=True))\n",
    "#dist_len=dict(sorted(dist_len.items()))\n",
    "\n",
    "aa=list(dist_len.values())\n",
    "cumulative = np.cumsum(aa)/len(id_ent_rel)\n",
    "cumulative\n",
    "\n",
    "cum_len=dict()\n",
    "cum_len[10]=0\n",
    "for i,k in dist_len.items():\n",
    "    if i in range(1,10):\n",
    "        cum_len[i]=k\n",
    "    else:\n",
    "        cum_len[10] +=k\n",
    "        \n",
    "\n",
    "cum_len\n",
    "\n",
    "\n",
    "bb=list(cum_len.values())\n",
    "cumulative1 = np.cumsum(bb)/len(id_ent_rel)\n",
    "cumulative1\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "plt.bar(list(cum_len.keys()), cumulative1)\n",
    "plt.xlabel('years')\n",
    "plt.ylabel('cumulative distribution')\n",
    "plt.xticks(rotation=90)\n",
    "#plt.savefig(\"cumulative.jpg\")\n",
    "#plt.savefig('cumulative.pdf')\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dic_ent_tweets_scores_reverse=dict(sorted(dic_ent_tweets_scores.items(),key=operator.itemgetter(1),reverse=False))\n",
    "print(len(dic_ent_tweets_scores_reverse))\n",
    "\n",
    "ent_tweets_score_reverse = list(dic_ent_tweets_scores_reverse.items())\n",
    "ent_tweets_score_reverse[0]\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets with negative score:  988183\n",
      "0.47269415855407715\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "id_ent_irrel=[]\n",
    "for score_id in ent_tweets_score_reverse:\n",
    "    if score_id[1]<0:\n",
    "        id_ent_irrel.append(score_id[0])\n",
    "    else:\n",
    "        break\n",
    "print('number of tweets with negative score: ', len(id_ent_irrel))\n",
    "tweets_irrel_ent=[]  \n",
    "i_id=0\n",
    "\n",
    "dist_len_neg=dict()\n",
    "#seed_tweets_non_ent=[]\n",
    "\n",
    "#for id_tweet in id_ent_irrel:\n",
    "#    temp = rows_ent[id_tweet][1]\n",
    "#    temp = strip_tags(temp)\n",
    "#    temp = re.sub(\"“|”\", '''\"''', temp)  \n",
    "#    temp = re.sub(\"’|′|‘|`\", \"'\", temp)  \n",
    "#    temp = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', temp)  \n",
    "#    temp = temp.lower() \n",
    "#    temp = re.sub(r'(http://|https://)\\S+', '', temp) \n",
    "#    temp = re.sub(r'www\\.\\S+', '', temp)\n",
    "#    temp = re.sub('( |^)@\\S+', '', temp)  \n",
    "#    temp= re.sub(\"[\\\"\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘'_;•«»,@:~!\\=%&]+\", ' ',temp) \n",
    "#    temp = temp.encode('utf8').decode('ISO-8859-1') # decode('ISO-8859-1') or decode('latin-1') or decode('windows-1252')\n",
    "    \n",
    "#    len_tw=len(temp.split())\n",
    "#    if len_tw in dist_len_neg:\n",
    "#        dist_len_neg[len_tw] +=1\n",
    "#    else:\n",
    "#        dist_len_neg[len_tw] =1\n",
    "\n",
    "#    if temp not in tweets_irrel_ent:\n",
    "#        tweets_rel_ent.append(temp)\n",
    "#        seed_tweets_non_ent.append(rows_ent[id_tweet][1])\n",
    "#        seed_tweets_ent.append(id_tweet)\n",
    "#        i_id +=1\n",
    "#    if i_id==200000:\n",
    "#        break\n",
    "print(time.time()-t0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dic_mng_tweets_scores_reverse=dict(sorted(dic_mng_tweets_scores.items(),key=operator.itemgetter(1),reverse=False))\n",
    "print(len(dic_mng_tweets_scores_reverse))\n",
    "\n",
    "mng_tweets_score_reverse = list(dic_mng_tweets_scores_reverse.items())[0:20000]\n",
    "mng_tweets_score_reverse[0]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./years/\"+year+\"_seed_ent_pos_1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(id_ent_rel, fp, protocol=4)\n",
    "\n",
    "'''\n",
    "with open(\"seed_irrel_tweets_mng.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(seed_irrel_tweets_mng, fp , protocol=4)\n",
    "'''\n",
    "\n",
    "with open(\"./years/\"+year+\"_seed_ent_neg_1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(id_ent_irrel, fp , protocol=4)\n",
    " \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_tweets_ent_scores_1.txt\", \"wb\") as fp:  \n",
    "    pickle.dump(dic_ent_tweets_scores, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
