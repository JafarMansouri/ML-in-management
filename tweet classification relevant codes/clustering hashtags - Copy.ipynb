{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cluster import OPTICS\n",
    "import time \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import sys\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing to tweets \n",
    "with open(\"tweet_ent.txt\", \"rb\") as fp:   # Unpickling\n",
    "  tweet_ent = pickle.load(fp)\n",
    "  \n",
    "with open(\"tweet_mng.txt\", \"rb\") as fp:   # Unpickling\n",
    "  tweet_mng = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text)   \n",
    "    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)\n",
    "    \n",
    "    #removing hashtag sign\n",
    "    text=re.sub('#', '', text) \n",
    "    \n",
    "    text=''.join(text)\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_hash_num=0\n",
    "ent_hash_tweet=[]\n",
    "for tweet in tweet_ent:\n",
    "    ans=[cleaning(i)  for i in tweet.split() if i.startswith(\"#\") ]\n",
    "    if ans!=[]:\n",
    "#        ans=' '.join(ans)\n",
    "#        ent_hash_tweet.append(ans)\n",
    "        ent_hash_tweet.extend(ans)\n",
    "        ent_hash_num +=1\n",
    "print(len(ent_hash_tweet))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_hash_tweet[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_hash_num=0\n",
    "mng_hash_tweet=[]\n",
    "for tweet in tweet_mng:\n",
    "    ans=[cleaning(i)  for i in tweet.split() if i.startswith(\"#\") ]\n",
    "    if ans!=[]:\n",
    "#        ans=' '.join(ans)\n",
    "#        mng_hash_tweet.append(ans)\n",
    "        mng_hash_tweet.extend(ans)\n",
    "        mng_hash_num +=1\n",
    "print(len(mng_hash_tweet))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_hash_tweet[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each tweet create a list of hashtags\n",
    "#for each tweet, find the embedding vector for each hashtag(without #) in the tweet. and store all of a them in a list  \n",
    "#Finally we have to lists: one for hashtags of tweets and one for thier embedding vectors. \n",
    "#e.g. tweet_ent_hash=[ [ hashtags of tweet 1], [hashtags of tweet 4,], ......]\n",
    "#e.g. embedding_ent_hash=[[vecotos for hashtags of tweet 1], [vecotos for hashtags of tweet 4], ......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "ent_embedding=[]\n",
    "ent_tokenized=[]\n",
    "\n",
    "for text in ent_hash_tweet:\n",
    "#    print(text)\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#    print(tokenized_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        out = outputs[0]\n",
    "  \n",
    "    token_embeddings = torch.squeeze(out, dim=0)  \n",
    "    \n",
    "    token_embeddings_list=token_embeddings.tolist()\n",
    "    \n",
    "    tokenized_text.pop(0)\n",
    "    tokenized_text.pop(-1)\n",
    "    token_embeddings_list.pop(0)\n",
    "    token_embeddings_list.pop(-1)\n",
    "    for ii in range(len(tokenized_text)-1 ,0 ,-1):\n",
    "#        print(ii)\n",
    "#        print(tokenized_text[ii][0:2])\n",
    "        if tokenized_text[ii][0:2]==\"##\":\n",
    "#            print(tokenized_text[ii])\n",
    "            token_embeddings_list[ii-1]=token_embeddings_list[ii-1]+token_embeddings_list[ii]\n",
    "            token_embeddings_list.pop(ii) # or del token_vecs_sum[ii]\n",
    "            tokenized_text[ii-1]=tokenized_text[ii-1]+tokenized_text[ii][2:]\n",
    "            tokenized_text.pop(ii) # or del tokenized_text[ii]\n",
    "\n",
    "    ent_embedding.append(token_embeddings_list)\n",
    "    ent_tokenized.append(tokenized_text)\n",
    "#    print(ent_tokenized)\n",
    "#    print()\n",
    "    \n",
    "    #for i, token_str in enumerate(tokenized_text):\n",
    "    #    print (i, token_str)\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ent_full_clustering = OPTICS(min_samples=10).fit(ent_full_embedding)\n",
    "#ent_full_clustering = DBSCAN(eps=3, min_samples=2).fit(ent_full_embedding)\n",
    "#kmeans = KMeans(n_clusters=1000) #760s for 100 clusters , 5699 s for 1000 clusters\n",
    "#ent_full_clustering = kmeans.fit(ent_full_embedding)\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
