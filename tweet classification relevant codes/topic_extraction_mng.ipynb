{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import html\n",
    "import xml.sax.saxutils as saxutils\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "import random\n",
    "import operator\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "#tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import fasttext.util\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1134"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(cachedStopWords))\n",
    "#print(len(cachedStopWords))\n",
    "#print(type(cachedStopWords))\n",
    "\n",
    "nltk_stopwords=set(cachedStopWords)\n",
    "\n",
    "english_alghabet=['b','c','e','f','g','h','j','k','l','n','p','q','r','u','v','w','x','z']\n",
    "\n",
    "numbers_remove=['one','two','three','four','five','six','seven','eight','nine','ten','tens','twenty',\n",
    "                'fourty','fifty','sixty','seventy','eighty','ninety','hundred','hundreds','million','billion','trillion',\n",
    "                'millions','thousand','thousands','second','third','forth','tenth','billions','trillions'] \n",
    "\n",
    "miscellaneous_remove=['absolutely', 'actually', 'adieu', 'ain', \"ain't\", 'aint', 'almost',\n",
    "                       'awesome','awfully','amazing','interesting',\n",
    "                       'alright','alrighty', 'amoungst', 'anybody', 'anymore', 'anyways', 'apart', 'apparently', 'anytime',\n",
    "                       'appropriate',  'approximately', 'arent', 'behold', 'better', 'bravo','briefly','bad','best','brilliant',\n",
    "                       'bye', 'cant', 'certainly', 'chrissakes', 'clearly', 'completely',\n",
    "                       'congrat', 'congrats','congratulation', 'congratulations', 'consequently', 'cool', 'couldnt',\n",
    "                       'darnit', 'de','dear', 'definitely','disappointing', 'didn', 'doesn', 'don', 'downwards',\n",
    "                       'disgusting','dude','down','eg',\"e.g.\",'i.e.',\n",
    "                       'encore','entirely', 'especially', 'et', 'etc', 'everybody', 'ex', 'exactly', 'excellent',\n",
    "                       'fantastic','far', 'farewell','funny',\n",
    "                       'felicitation', 'felicitations','finally', 'fully','furthermore', 'gadzooks', \n",
    "                       'good', 'goodby','goodness', 'gracious', 'great', \n",
    "                       'greetings', 'hallo', 'hardly', 'hasnt', 'haven', 'hello', 'here','hi', 'hither','higher','hopefully',\n",
    "                       'here','there','including',\n",
    "                       'howbeit', 'ie', 'immediately', 'inasmuch', 'inner', 'insofar', 'instead', 'inward', 'important',\n",
    "                       'indeed','just', \"it'd\", \"it'll\", 'inside','kertyschoo', 'kg', 'km', 'lackaday', \n",
    "                       'largely', 'lately', 'later','lovely',\n",
    "                       'lest', 'let', 'lets', 'likely', 'little', 'ltd', 'lower','magnificent', 'mainly', 'marvelous',\n",
    "                       'myself','yourself','yourselves','himself','herself','hisself','ourselves','themsleves',\n",
    "                       'maybe', 'meantime', 'merely', 'minus', 'near', 'nearly', 'necessary', 'never', \n",
    "                       'non', 'normally', 'obviously', 'ok', 'okay', 'ones', 'outside', 'over','other','others','only',\n",
    "                       'overall', 'particular', 'particularly', 'please', 'plus', 'poorly', 'possible','up',\n",
    "                       'possibly', 'potentially', 'predominantly', 'presumably', 'previously','primarily', 'probably',\n",
    "                       'promising',\n",
    "                       'promptly', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref',\n",
    "                       'refs', 'regardless', 'related', 'relatively', 'respectively', 'resulting', 'right', 'sec', \n",
    "                       'secondly','self', 'selves', 'seriously', 'shall', 'shucks','somebody', 'somethan','sorry',\n",
    "                       'somewhat', 'soon', 'late' , 'sorry', 'stupid', 'sub', 'substantially', 'successfully', 'sufficiently',\n",
    "                       \n",
    "                       'super', 'sure', \"t's\", 'th', 'thank', 'thanks', 'thanx', \"that've\", 'thats', 'there', \"there'll\",\n",
    "                       \"there've\", 'thered', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyre', 'thorough',\n",
    "                       'then','thankfully','too','today','yesterday','tomorrow','night',\"morning\",'afternoon','noon','tonight',\n",
    "                       'evening','day','everyday', 'everynight','todays','nights','mornings','noons','afternoons','days',\n",
    "                       'evenings','week','month','year',\n",
    "                       'thoroughly', 'tnx', 'too','truly', 'twice', 'undoubtedly','unfortunately', 'unlike','unlikely',\n",
    "                       'unto',  'usually', 'vs', 'welcome', 'well', 'went', 'werent', 'what', 'whatever', 'wheres', 'widely',\n",
    "                       'wonderful', 'wont', 'wouldnt', 'wrong', 'worst','worse','www', 'yes', 'youd', 'youre', 'yummy', \n",
    "                       'zoinks','shit','literally','literal','pleasure','effective','fabulous','delighted',\n",
    "                       'saturday','sunday','monday','tuesday','wednesday','thursday', 'friday','past','future','suitable',\n",
    "                       'much','many','less','least','few','lots','lot','fewer','fewset','therefore','pm',\n",
    "                       'afaik', 'br', 'idk','smh','qotd', 'ftw','bfn','yw', 'icymi','fomo','smdh', 'b4','imho',\n",
    "                       'urdddd','fab' ,'delightful','absolute','pleasure','huge','latest','nowadays',\n",
    "                       'january','february','april','june','july','august','september','october',\n",
    "                       'november','december', 'autumn' ,'spring','winter','summer',\n",
    "                       'mr','madam','sir','mrs','easy', 'difficult','dr',\n",
    "                       'weekend','south','north','west','east','asia','africa','europe','america','totally',\n",
    "                       'come', 'comes', 'coming', 'came', 'seems', 'gives', 'gave', 'makes', 'made', 'keeps', 'kept', \n",
    "                       'calls', 'called', 'says', 'saying', 'said', 'goes', 'went', 'gone', 'got', 'saw', 'seen', 'shows',\n",
    "                       'shown', 'took', 'taken', 'uses', 'moved', 'moves', 'puts',\n",
    "                       'using','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "                       'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use',\n",
    "                       'find', 'finds', 'finding','aka',\n",
    "                       'lol' , 'brb', 'lmk', 'ama', 'tbh', 'irl', \"tl;dr\", 'fml', 'bfn' ,' br', 'ht', \"hth\",'j/k', 'lmao' ] #cool\n",
    "                        # small big large useful \n",
    "\n",
    "interjection_remove=['aaaahh', 'aaah', 'aaargh', 'aaay', 'aagh', 'aah',\n",
    "                   'aargh', 'achoo', 'adios', 'ah', 'aha', 'ahem', 'ahh', 'ahhh',\n",
    "                   'ahoy', 'alas', 'allo', 'amen', 'areet', 'argh', 'arrggh',\n",
    "                   'arrividerci', 'asap', 'attaboy', 'avaunt', 'aw', 'aw', 'aww',\n",
    "                   'awww', 'ay', 'ay', 'aye', 'ayeaugh', 'bada', 'badum', 'bah',\n",
    "                   'bahaha', 'bam', 'bazinga', 'behold', 'bingce', 'bingo', 'blah',\n",
    "                   'blech', 'bleh', 'blimey', 'bonjour', 'boo', 'booh', 'boohoo',\n",
    "                   'booyah', 'bravo', 'brr', 'brrrr', 'btw', 'bwahaha', 'capeesh',\n",
    "                   'capisce', 'cheerio', 'cheers', 'ciao', 'cor', 'cowabunga',\n",
    "                   'crikey', 'cripes', 'da', 'dabba', 'dah', 'dammit', 'damn', 'dang',\n",
    "                   'darn', 'de', 'dee', 'di', 'dizamn', 'doh', 'doo', 'drat', 'duh',\n",
    "                   'dum', 'eeeek', 'eek', 'eep', 'egad', 'egads', 'eh', 'ehem', 'em',\n",
    "                   'er', 'eureka', 'eww', 'ewww', 'eyh', 'fiddledeedee', 'fie',\n",
    "                   'fore', 'foul', 'fuff', 'gah', 'gak', 'gee', 'geez', 'gesundheit',\n",
    "                   'giddyap', 'golly', 'gosh', 'grr', 'grrrr', 'ha', 'hah', 'haha',\n",
    "                   'hahaha', 'hallelujah', 'halloa', 'harrumph', 'harumph', 'haw',\n",
    "                   'heck', 'heck', 'heeey', 'heh', 'hehe', 'hey', 'hhh', 'hic', 'hm',\n",
    "                   'hmm', 'hmmm', 'hmmmm', 'hmmph', 'hmpf', 'ho', 'hola', 'hoo',\n",
    "                   'hooray', 'howdy', 'hrmm', 'hrmph', 'hrmph', 'hrrmph', 'hu', 'huh',\n",
    "                   'hullo', 'humph', 'hurrah', 'huzza', 'huzzah', 'ich', 'ick',\n",
    "                   'ixnay', 'jeepers', 'jeez', 'kaboom', 'kapow', 'kerwham', 'la',\n",
    "                   'lala', 'lo', 'lordy', 'meh', 'mhm', 'ml', 'mm', 'mmh', 'mmhm',\n",
    "                   'mmm', 'muahaha', 'mwah', 'mwahaha', 'na','nay','nah', 'nanu', 'nooo', 'nope',\n",
    "                   'nuh', 'oh', 'ohh', 'oho', 'oi', 'okeydoke', 'om', 'oof', 'ooh',\n",
    "                   'oomph', 'oooh', 'ooooh', 'oops', 'ouch', 'ow', 'oww', 'oy',\n",
    "                   'oyez', 'oyh', 'pew', 'pff', 'pffh', 'pfft', 'phew', 'phut',\n",
    "                   'phweep', 'phwoar', 'phwoarr', 'poof', 'poogh', 'prethee',\n",
    "                   'prithee', 'prosit', 'pssh', 'psst', 'queep', 'roger', 'salaam',\n",
    "                   'salam', 'sheesh', 'shh', 'shhh', 'shitfire', 'shoo', 'shoop',\n",
    "                   'shush', 'sigh', 'sssh', 'strewth', 'ta', 'tarnations', 'tchah',\n",
    "                   'teehee', 'tish', 'touché', 'tsk', 'tss', 'tut', 'uggh', 'ugh',\n",
    "                   'uh', 'uhh', 'uhm', 'um', 'umm', 'ummm', 'umph', 'unh', 'upadaisy',\n",
    "                   'upsadaisy', 'ur', 'urgh', 'vay', 'vayf', 'viva', 'voila', 'waa',\n",
    "                   'waaaaah', 'waah', 'wah', 'wahey', 'wassup', 'weee', 'welp',\n",
    "                   'wham', 'whamo', 'whee', 'whew', 'whizz', 'whoa',\n",
    "                   'whoo', 'whoopee','whoop', 'whoops', 'whoopsy', 'whoosh', 'woah', 'woo',\n",
    "                   'woohoo', 'wotcha', 'wotcher', 'wow', 'wowsers', 'wowsers',\n",
    "                   'wuzzup', 'wuzzup', 'wuzzup', 'ya', 'yabba', 'yada', 'yadda',\n",
    "                   'yak', 'yarooh', 'yay', 'yea', 'yeah', 'yech', 'yee', 'yeeeeaah',\n",
    "                   'yeehaw', 'yeow', 'yes', 'yessiree', 'yew', 'yikes', 'yippee',\n",
    "                   'yo', 'yoo', 'yoohoo', 'yow', 'yowza', 'yuck', 'yuh', 'zing',\n",
    "                   'zoiks', 'zomfg', 'zomg', 'zounds', 'zut']\n",
    "             \n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "type(spacy_stopwords)\n",
    "#spacy_exclude=['using','name','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "#               'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use']# serious\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words1 = get_stop_words('english')\n",
    "#print(type(stop_words1))\n",
    "#print()\n",
    "#print(stop_words1)\n",
    "lib_stopwords=set(stop_words1)\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#print(type(ENGLISH_STOP_WORDS))\n",
    "#print()\n",
    "#print(set(ENGLISH_STOP_WORDS))\n",
    "#sklearn_exclude=['find','get','found','go','see','seem','seems','give','seemed','take','keep','show','put','made'] # system  cry\n",
    "sklearn_stopwords=set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "#spacy_stopwords.difference_update(set(spacy_exclude))\n",
    "#sklearn_stopwords.difference_update(set(sklearn_stopwords))\n",
    "#for removing \"just\" one item, use \"remove\"\n",
    "temp_1=set([])\n",
    "#######################\n",
    "temp_1.update(nltk_stopwords)\n",
    "temp_1.update(lib_stopwords)\n",
    "temp_1.update(sklearn_stopwords)\n",
    "temp_1.update(spacy_stopwords)\n",
    "temp_1.update(set(english_alghabet))\n",
    "temp_1.update(set(numbers_remove)) \n",
    "temp_1.update(set(miscellaneous_remove))\n",
    "temp_1.update(set(interjection_remove))\n",
    "temp_1.update(['rt','be','will','was','were','is','am','are','have','has','had','do','does','done'])\n",
    "###              ,'thing','want','getting', 'looking','way'])\n",
    "###temp_1.update(['rt','like','look','get','take'])\n",
    "cachedStopWords=temp_1\n",
    "len(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "#print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "#print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "#print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)rt ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "    text=' '.join(text)\n",
    "    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningA (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    #text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    #text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    #text=re.sub('Dollar|Dollars|Yen|Yens|Euros', ' money ', text)   # not euro \n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    text = re.sub(r'(HTTP://|HTTPS://)\\S+', '', text)\n",
    "\n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    text=re.sub(r'WWW\\.\\S+', '', text)\n",
    "\n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"United States\",'USA', text)\n",
    "    text=re.sub(\"United Kingdom\",'UK', text)\n",
    "    text=re.sub(\" the US \",' USA ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"U\\.S\\.A\", 'USA', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text)  \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)RT | RT ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_punc(text):\n",
    "    '''\n",
    "    text=re.sub(\"looking forward|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", ' ',text) #except  \\- _\n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    '''\n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    text= re.sub(\">\", ' ',text)\n",
    "    text= re.sub(\"<\", ' ',text)\n",
    "    text= re.sub(\" - \", ' ',text)\n",
    "    text= re.sub(\" --\", ' ',text)\n",
    "    '''\n",
    "    \n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "\n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46502302, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "Number of tweets in mng: 46502302\n"
     ]
    }
   ],
   "source": [
    "#df_mng = pd.read_csv('/archives1/Datasets/TweetsWorld/mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_mng0 = pd.read_csv('mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_mng0.shape)\n",
    "print(df_mng0.columns)\n",
    "#print(df_mng0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_mng=list(df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_mng= list(zip(df_mng0.user_id, df_mng.tweet))\n",
    "#rows_mng=df_mng0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_mng = df_mng0[['tweet','tweet_created_at']]   \n",
    "\n",
    "\n",
    "del df_mng0\n",
    "\n",
    "print(\"Number of tweets in mng:\",len(df_mng))  #rows_mng0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Memory size of mng:',sys.getsizeof(df_mng)) #rows_mng0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_mng0 = pd.read_csv('mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#df_mng0 = pd.read_csv('/archives1/Datasets/TweetsWorld/mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "print(df_mng0.shape)\n",
    "print(df_mng0.columns)\n",
    "#print(df_mng0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_mng=list(df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_mng0\n",
    "#rows_mng= list(zip(df_mng0.user_id, df_mng.tweet))\n",
    "#rows_mng=df_mng0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_mng = df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']]\n",
    "\n",
    "del df_mng0\n",
    "\n",
    "print(len(df_mng)) #rows_mng0\n",
    "print('memry size of mng:', sys.getsizeof(df_mng)) #rows_mng0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/mng_cleaning_classification.txt\", \"rb\") as fp:  \n",
    "#    mng_tweets_rows=pickle.load(fp)\n",
    "#len(mng_tweets_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"_mng_year_ind.txt\", \"rb\") as fp:   \n",
    "    mng_year_ind_2019=pickle.load(fp)\n",
    "\n",
    "len(mng_year_ind_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"_mng_year_ind.txt\", \"rb\") as fp:   \n",
    "    mng_year_ind_2017=pickle.load(fp)\n",
    "\n",
    "len(mng_year_ind_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"result_mng_vote.txt\", \"rb\") as fp:  \n",
    "    vote_19=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_mng_label.txt\", \"rb\") as fp:  \n",
    "    label_19=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_2019_rel=[]\n",
    "mng_2019_irrel=[]\n",
    "for i , k in enumerate(mng_year_ind_2019):\n",
    "\n",
    "    if '2019'  not in df_mng['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_19[i]==1:\n",
    "        mng_2019_rel.append(k)\n",
    "    else:\n",
    "        mng_2019_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600143, 1399857)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mng_2019_rel), len(mng_2019_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"result_mng_vote.txt\", \"rb\") as fp:  \n",
    "    vote_17=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_mng_label.txt\", \"rb\") as fp:  \n",
    "    label_17=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_2017_rel=[]\n",
    "mng_2017_irrel=[]\n",
    "for i , k in enumerate(mng_year_ind_2017):\n",
    "    if '2017' not in df_mng['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_17[i]==1:\n",
    "        mng_2017_rel.append(k)\n",
    "    else:\n",
    "        mng_2017_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756871, 1243129)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mng_2017_rel), len(mng_2017_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mng['tweet_created_at'][mng_2017_rel[0]], df_mng['tweet'][mng_2017_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378.6259415149689"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0=time.time()\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "#X_train_test=[]\n",
    "\n",
    "tweet_vectors_19=np.zeros([len(mng_2019_rel),embedding_dim],dtype='float32')\n",
    "\n",
    "for i,index in enumerate(mng_2019_rel):\n",
    "    tweet=df_mng['tweet'][index]\n",
    "    \n",
    "    #tweet='I like it.'\n",
    "    text=cleaning(tweet) # the result for cleaning and cleaningA is different \n",
    "    #text=cleaning_punc(text)\n",
    "    #text=cleaning_numbers(text)\n",
    "#train_texts_tokens=[]\n",
    "#for text in train_texts:\n",
    "#    tokens=text.split()\n",
    "    #tokens=nltk.word_tokenize(text)\n",
    "    tokens=tknzr.tokenize(text)\n",
    "    #train_texts_tokens.append(tokens)\n",
    "    \n",
    "#print(len(train_texts_tokens))\n",
    "\n",
    "#for kk,tokens in enumerate(train_texts_tokens):\n",
    "    if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "        a1=[]\n",
    "        num_tok=0\n",
    "        for token in tokens:\n",
    "            num_tok +=1\n",
    "            a1.append( ft.get_word_vector(token) )\n",
    "                    \n",
    "        #print(a1)\n",
    "        #print()\n",
    "        #temp1=np.sum(a1,axis=0)/num_tok\n",
    "        #print(temp1)\n",
    "        #print()\n",
    "        temp=np.mean(a1,axis=0)\n",
    "        #print(abc-temp1)\n",
    "        \n",
    "        \n",
    "        #if len(tokens)>max_tokens:\n",
    "        #    temp=a1[0:max_tokens]\n",
    "        #elif len(tokens)==max_tokens:\n",
    "        #    temp=a1\n",
    "        #else: # if len(tokens)<max_tokens:\n",
    "        #    temp[0:len(tokens)]=a1\n",
    "        #X_train_test.append(temp) \n",
    "        tweet_vectors_19[i]=temp\n",
    "\n",
    "    else:\n",
    "        #print(index)\n",
    "        #print(tweet)\n",
    "        #print('strange')\n",
    "        temp=np.zeros([1,embedding_dim])\n",
    "#        X_train_test.append(temp)\n",
    "        tweet_vectors_19[i]=temp\n",
    "        \n",
    "time.time()-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600143, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_umap = umap.UMAP().fit_transform(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.709344387054443\n",
      "4.710359573364258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "t0=time.time()\n",
    "\n",
    "#kpca = KernelPCA(n_components=2, kernel='linear')\n",
    "#X_kpca = kpca.fit_transform(tweet_vectors_19)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2019 = pca.fit_transform(tweet_vectors_19)\n",
    "\n",
    "print(time.time()-t0)\n",
    "\n",
    "#tsne = TSNE()\n",
    "#X_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600143, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = LocallyLinearEmbedding(n_components=2)\n",
    "#X_transformed = embedding.fit_transform(tweet_vectors_19)\n",
    "#X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "x_svd=svd.fit_transform(tweet_vectors_19)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "x_svd.shape\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = Isomap(n_components=2)\n",
    "#X_transformed_iso = embedding.fit_transform(tweet_vectors_19)\n",
    "#X_transformed_iso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.manifold import MDS\n",
    "embedding = MDS(n_components=2)\n",
    "X_transformed = embedding.fit_transform(tweet_vectors_19)\n",
    "X_transformed.shape\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer_19 = hdbscan.HDBSCAN(min_cluster_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.42037844657898\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "clusterer_19.fit(X_pca2019)\n",
    "#clusterer_19.fit(x_svd)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer_19.labels_\n",
    "clusterer_19.labels_[0:10]\n",
    "clusterer_19.labels_.max()\n",
    "np.unique(clusterer_19.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of tweets in each cluster for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408281\n",
      "1598\n",
      "939\n",
      "629\n",
      "1167\n",
      "16034\n",
      "1898\n",
      "743\n",
      "1758\n",
      "1435\n",
      "1032\n",
      "1229\n",
      "1006\n",
      "5820\n",
      "511\n",
      "2546\n",
      "2100\n",
      "1001\n",
      "772\n",
      "849\n",
      "1264\n",
      "588\n",
      "131714\n",
      "1115\n",
      "544\n",
      "972\n",
      "3089\n",
      "9509\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "cluster_index=[None]*(clusterer_19.labels_.max()+1)\n",
    "#display(cluster_index)\n",
    "\n",
    "for i in range(-1,clusterer_19.labels_.max()+1):\n",
    "    aa1=np.where(clusterer_19.labels_ == i)\n",
    "    if i!=-1:\n",
    "        cluster_index[i]=aa1[0]\n",
    "    print(len(aa1[0]))\n",
    "\n",
    "print('----')\n",
    "#len(cluster_index[4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_index[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=cachedStopWords\n",
    "stop.update(set(english_alghabet))\n",
    "stop.update(set(numbers_remove))\n",
    "stop.update(set(['rt','u']))\n",
    "\n",
    "exclude_punc = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "def clean(doc):\n",
    "    \n",
    "    doc=cleaning(doc)\n",
    "    doc=cleaning_punc(doc)\n",
    "    doc=cleaning_numbers(doc)\n",
    "    doc= re.sub(\" 00am | 00pm | pm | am \", '',doc)\n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() re.sub(\"(\\d+)\", '',i))\n",
    "\n",
    "    stop_free_=[]                      \n",
    "    for i in stop_free.lower().split():\n",
    "                          \n",
    "        if not i.isdigit():\n",
    "            stop_free_.append(i)\n",
    "                          \n",
    "    stop_free = \" \".join(stop_free_)\n",
    "\n",
    "    #text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude_punc)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    cleaned = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Topics and thier words for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for ii in range(0,clusterer_19.labels_.max()+1):\n",
    "    print(\"cluster index : \",ii)\n",
    "\n",
    "    set_x=[]\n",
    "    for i in cluster_index[ii]:\n",
    "        text=df_mng.tweet[mng_2019_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_mng['tweet'][mng_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 1, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't miss the new Quotable articles! Subscribe to our bi-weekly newsletter now! https://t.co/53zxj4NU7F #sales https://t.co/4BDfV2cmYg\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mng.tweet[mng_2019_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241692"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "241692"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['photo' 'trump' 'post' 'president' 'usa' 'report' 'new' 'state' 'climate'\n",
      " 'news' 'country' 'work' 'support' 'government' 'india' 'time' 'world'\n",
      " 'vote' 'need' 'impeachment']\n",
      "----------\n",
      "Topic 2 ['news' 'check' 'new' 'dm' 'support' 'email' 'list' 'incredible' 'fun'\n",
      " 'podcast' 'work' 'mate' 'luck' 'team' 'run' 'xx' 'episode' 'interview'\n",
      " 'send' 'wooglobe']\n",
      "----------\n",
      "Topic 3 ['course' 'appreciate' 'job' 'fair' 'glad' 'look' 'hire' 'achievement'\n",
      " 'market' 'invite' 'realestate' 'team' 'work' 'list' 'check' 'vacancy'\n",
      " 'open' 'option' 'deserve' 'register']\n",
      "----------\n",
      "Topic 4 ['money' 'vote' 'brexit' 'uk' 'market' 'deal' 'price' 'new' 'pay' 'eu'\n",
      " 'election' 'tax' 'time' 'trade' 'labour' 'work' 'party' 'need' 'think'\n",
      " 'fund']\n",
      "----------\n",
      "Topic 5 ['read' 'join' 'ai' 'business' 'learn' 'technology' 'cloud' 'data'\n",
      " 'security' 'digital' 'new' 'cybersecurity' 'innovation' 'solution'\n",
      " 'proud' 'customer' 'service' 'tech' 'market' 'project']\n",
      "----------\n",
      "Topic 6 ['new' 'content' 'google' 'update' 'market' 'work' 'post' 'facebook'\n",
      " 'website' 'data' 'design' 'seo' 'app' 'write' 'business' 'page' 'need'\n",
      " 'feature' 'create' 'video']\n",
      "----------\n",
      "Topic 7 ['work' 'people' 'need' 'help' 'team' 'learn' 'support' 'new' 'job'\n",
      " 'think' 'health' 'way' 'time' 'experience' 'opportunity' 'share' 'know'\n",
      " 'look' 'leader' 'leadership']\n",
      "----------\n",
      "Topic 8 ['follow' 'new' 'book' 'enjoy' 'share' 'time' 'event' 'team' 'win' 'hope'\n",
      " 'happy' 'look' 'start' 'join' 'meet' 'work' 'ticket' 'christmas' 'london'\n",
      " 'award']\n",
      "----------\n",
      "Topic 9 ['look' 'agree' 'forward' 'excite' 'link' 'new' 'plan' 'bring' 'team'\n",
      " 'free' 'meet' 'visit' 'work' 'event' 'office' 'session' 'sound' 'join'\n",
      " 'read' 'like']\n",
      "----------\n",
      "Topic 10 ['work' 'team' 'idea' 'help' 'job' 'nice' 'hard' 'love' 'time' 'new'\n",
      " 'service' 'goal' 'money' 'need' 'long' 'look' 'think' 'home' 'support'\n",
      " 'join']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    set_x=[]\n",
    "    for i in range(0,len(mng_2019_rel)):\n",
    "        #print(mng_2019_rel[i])\n",
    "        text=df_mng.tweet[mng_2019_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_mng['tweet'][mng_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 10, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusterer_optics = OPTICS(min_samples=5).fit(X_pca)\n",
    "#clusterer_dbscan = DBSCAN(eps=5, min_samples=2).fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#kmeans = KMeans(n_clusters=3) \n",
    "#mng_kmeans_clustering = kmeans.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469.74988746643066"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0=time.time()\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "#X_train_test=[]\n",
    "\n",
    "tweet_vectors_17=np.zeros([len(mng_2017_rel),embedding_dim],dtype='float32')\n",
    "\n",
    "for i,index in enumerate(mng_2017_rel):\n",
    "    tweet=df_mng['tweet'][index]\n",
    "    \n",
    "    #tweet='I like it.'\n",
    "    text=cleaning(tweet) # the result for cleaning and cleaningA is different \n",
    "    #text=cleaning_punc(text)\n",
    "    #text=cleaning_numbers(text)\n",
    "#train_texts_tokens=[]\n",
    "#for text in train_texts:\n",
    "#    tokens=text.split()\n",
    "    #tokens=nltk.word_tokenize(text)\n",
    "    tokens=tknzr.tokenize(text)\n",
    "    #train_texts_tokens.append(tokens)\n",
    "    \n",
    "#print(len(train_texts_tokens))\n",
    "\n",
    "#for kk,tokens in enumerate(train_texts_tokens):\n",
    "    if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "        a1=[]\n",
    "        num_tok=0\n",
    "        for token in tokens:\n",
    "            num_tok +=1\n",
    "            a1.append( ft.get_word_vector(token) )\n",
    "                    \n",
    "        #print(a1)\n",
    "        #print()\n",
    "        #temp1=np.sum(a1,axis=0)/num_tok\n",
    "        #print(temp1)\n",
    "        #print()\n",
    "        temp=np.mean(a1,axis=0)\n",
    "        #print(abc-temp1)\n",
    "        \n",
    "        \n",
    "        #if len(tokens)>max_tokens:\n",
    "        #    temp=a1[0:max_tokens]\n",
    "        #elif len(tokens)==max_tokens:\n",
    "        #    temp=a1\n",
    "        #else: # if len(tokens)<max_tokens:\n",
    "        #    temp[0:len(tokens)]=a1\n",
    "        #X_train_test.append(temp) \n",
    "        tweet_vectors_17[i]=temp\n",
    "\n",
    "    else:\n",
    "        #print(index)\n",
    "        #print(tweet)\n",
    "        #print('strange')\n",
    "        temp=np.zeros([1,embedding_dim])\n",
    "#        X_train_test.append(temp)\n",
    "        tweet_vectors_17[i]=temp\n",
    "        \n",
    "time.time()-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756871, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tweet_vectors_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0490217208862305\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2017 = pca.fit_transform(tweet_vectors_17)\n",
    "\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer_17 = hdbscan.HDBSCAN(min_cluster_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.34524917602539\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "clusterer_17.fit(X_pca2017)\n",
    "#clusterer.fit(x_svd)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "       33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer_17.labels_\n",
    "\n",
    "clusterer_17.labels_[0:10]\n",
    "\n",
    "clusterer_17.labels_.max()\n",
    "\n",
    "np.unique(clusterer_17.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of tweets in each cluster for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445302\n",
      "676\n",
      "629\n",
      "980\n",
      "1060\n",
      "944\n",
      "2366\n",
      "974\n",
      "2330\n",
      "10501\n",
      "704\n",
      "576\n",
      "3808\n",
      "911\n",
      "3554\n",
      "1446\n",
      "548\n",
      "3054\n",
      "4356\n",
      "2218\n",
      "4841\n",
      "1360\n",
      "2514\n",
      "564\n",
      "796\n",
      "1155\n",
      "4787\n",
      "992\n",
      "1836\n",
      "845\n",
      "741\n",
      "620\n",
      "661\n",
      "580\n",
      "933\n",
      "1101\n",
      "528\n",
      "1823\n",
      "6186\n",
      "2063\n",
      "220241\n",
      "2001\n",
      "1304\n",
      "1087\n",
      "651\n",
      "1202\n",
      "8522\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "cluster_index=[None]*(clusterer_17.labels_.max()+1)\n",
    "#display(cluster_index)\n",
    "\n",
    "for i in range(-1,clusterer_17.labels_.max()+1):\n",
    "    aa1=np.where(clusterer_17.labels_ == i)\n",
    "    if i!=-1:\n",
    "        cluster_index[i]=aa1[0]\n",
    "    print(len(aa1[0]))\n",
    "\n",
    "print('----')\n",
    "#len(cluster_index[4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics and thier words for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for ii in range(0,clusterer_17.labels_.max()+1):\n",
    "    print(\"cluster index : \",ii)\n",
    "\n",
    "    set_x=[]\n",
    "    for i in cluster_index[ii]:\n",
    "        text=df_mng.tweet[mng_2017_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_mng['tweet'][mng_2017_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 1, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255764"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "255764"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['money' 'support' 'work' 'need' 'people' 'help' 'uk' 'new' 'health'\n",
      " 'share' 'http' 'vote' 'report' 'trump' 'change' 'usa' 'plan' 'fund'\n",
      " 'time' 'tax']\n",
      "----------\n",
      "Topic 2 ['photo' 'post' 'excite' 'new' 'facebook' 'help' 'meet' 'blog' 'cancer'\n",
      " 'scorpio' 'goal' 'sanitation' 'work' 'look' 'incredible' 'news' 'set'\n",
      " 'event' 'need' 'share']\n",
      "----------\n",
      "Topic 3 ['market' 'new' 'business' 'social' 'learn' 'content' 'data' 'medium'\n",
      " 'email' 'work' 'way' 'need' 'update' 'cloud' 'tip' 'service' 'customer'\n",
      " 'help' 'know' 'digital']\n",
      "----------\n",
      "Topic 4 ['follower' 'new' 'enjoy' 'stats' 'hope' 'team' 'track' 'work' 'glad'\n",
      " 'look' 'xx' 'support' 'join' 'san' 'world' 'time' 'help' 'session' 'free'\n",
      " 'interview']\n",
      "----------\n",
      "Topic 5 ['new' 'check' 'join' 'happy' 'team' 'work' 'london' 'nice' 'train'\n",
      " 'event' 'city' 'look' 'time' 'job' 'meet' 'york' 'volunteer' 'excite'\n",
      " 'http' 'connect']\n",
      "----------\n",
      "Topic 6 ['look' 'new' 'forward' 'win' 'news' 'chance' 'video' 'follow' 'facebook'\n",
      " 'post' 'sign' 'giveaway' 'work' 'daily' 'enter' 'team' 'time' 'digital'\n",
      " 'market' 'pop']\n",
      "----------\n",
      "Topic 7 ['agree' 'new' 'join' 'review' 'book' 'write' 'cruise' 'work' 'recommend'\n",
      " 'team' 'event' 'contact' 'guestlist' 'restaurant' 'highly' 'quote'\n",
      " 'eventprofs' 'time' 'http' 'offer']\n",
      "----------\n",
      "Topic 8 ['work' 'team' 'award' 'luck' 'proud' 'hard' 'new' 'support' 'winner'\n",
      " 'job' 'step' 'success' 'time' 'look' 'love' 'help' 'meet' 'christmas'\n",
      " 'join' 'travel']\n",
      "----------\n",
      "Topic 9 ['follow' 'read' 'book' 'fun' 'ticket' 'idea' 'event' 'time' 'open' 'join'\n",
      " 'new' 'money' 'work' 'look' 'free' 'start' 'visit' 'http' 'holiday'\n",
      " 'place']\n",
      "----------\n",
      "Topic 10 ['job' 'look' 'course' 'manager' 'new' 'work' 'opportunity' 'team' 'apply'\n",
      " 'check' 'project' 'office' 'hire' 'run' 'news' 'daktronics' 'view' 'golf'\n",
      " 'career' 'sale']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    set_x=[]\n",
    "    for i in range(0,len(mng_2017_rel)):\n",
    "        #print(mng_2017_rel[i])\n",
    "        text=df_mng.tweet[mng_2017_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_mng['tweet'][mng_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 10, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment codes (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def noun_phrases(text):\n",
    "    # noun phrase by nltk\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "    grammar = \"\"\"NP: {<DT>?<JJ>*<NN.*>+}\"\"\"\n",
    "\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    #for i in sentences:\n",
    "    #    result = cp.parse(i)\n",
    "    #    print(result)\n",
    "    #    print(type(result))\n",
    "        #result.draw() \n",
    "   # noun_phrases_list = [[' '.join(leaf[0].lower() for leaf in tree.leaves()) \n",
    "\n",
    "    noun_phrases_list = [[' '.join(leaf[0] for leaf in tree.leaves()) \n",
    "                          for tree in cp.parse(sent).subtrees() \n",
    "                          if tree.label()=='NP'] \n",
    "                          for sent in sentences]\n",
    "\n",
    "\n",
    "    phrases_all=[]\n",
    "    for i in noun_phrases_list:\n",
    "        phrases_all.extend(i)\n",
    "    \n",
    "    return phrases_all\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def noun_phrases1(text):\n",
    "    # noun phrase by Spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    phrases_all=[]\n",
    "    for phrase in doc.noun_chunks:\n",
    "        phrases_all.append(phrase.text)\n",
    "        #phrases_all.append(phrase.text.lower())\n",
    "\n",
    "    return phrases_all\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./mng_results300_np.txt\", \"rb\") as fp:   \n",
    "    mng_results = pickle.load(fp)\n",
    "\n",
    "num_mng_positive=0\n",
    "for i in mng_results:\n",
    "    if i>=0.5:\n",
    "        num_mng_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to mng:\",num_mng_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./mng_results300.txt\", \"rb\") as fp:   \n",
    "    mng_results = pickle.load(fp)\n",
    "\n",
    "num_mng_positive=0\n",
    "for i in mng_results:\n",
    "    if i>=0.5:\n",
    "        num_mng_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to mng:\", num_mng_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequnent hashtags in MNG (with thier frequencies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "mng_hash_2017=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        mng_hash_2017.extend(hashtags)\n",
    "        \n",
    "mng_Hashtags=dict()\n",
    "\n",
    "for ph in mng_hash:\n",
    "    if ph !='':\n",
    "        if ph not in mng_Hashtags:\n",
    "            mng_Hashtags[ph]=1\n",
    "        else:\n",
    "            mng_Hashtags[ph]+=1  \n",
    "\n",
    "        \n",
    "mng_Hashtags = sorted(mng_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.773009061813354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('job', 2902),\n",
       " ('marketing', 2393),\n",
       " ('leadership', 2099),\n",
       " ('business', 1741),\n",
       " ('seo', 1715),\n",
       " ('socialmedia', 1707),\n",
       " ('ai', 1537),\n",
       " ('cloud', 1458),\n",
       " ('management', 1254),\n",
       " ('iot', 1246),\n",
       " ('digitalmarketing', 1232),\n",
       " ('quote', 1120),\n",
       " ('sesocial', 1063),\n",
       " ('contentmarketing', 1049),\n",
       " ('jobs', 1046),\n",
       " ('daktronics', 964),\n",
       " ('cybersecurity', 922),\n",
       " ('innovation', 896),\n",
       " ('tech', 891),\n",
       " ('win', 861),\n",
       " ('hr', 816),\n",
       " ('brexit', 803),\n",
       " ('data', 774),\n",
       " ('freelance', 743),\n",
       " ('sagesummit', 717),\n",
       " ('security', 699),\n",
       " ('1', 697),\n",
       " ('gdpr', 683),\n",
       " ('london', 669),\n",
       " ('realestate', 668),\n",
       " ('giveaway', 650),\n",
       " ('startup', 627),\n",
       " ('healthcare', 616),\n",
       " ('blockchain', 613),\n",
       " ('teamo2', 607),\n",
       " ('digital', 605),\n",
       " ('nhs', 601),\n",
       " ('travel', 598),\n",
       " ('eventprofs', 579),\n",
       " ('verticalmeasures', 565),\n",
       " ('fintech', 549),\n",
       " ('mentalhealth', 547),\n",
       " ('bigdata', 543),\n",
       " ('climatechange', 539),\n",
       " ('remote', 529),\n",
       " ('resort', 525),\n",
       " ('ge2017', 522),\n",
       " ('timeshare', 521),\n",
       " ('charity', 517),\n",
       " ('startups', 505),\n",
       " ('work', 504),\n",
       " ('sales', 500),\n",
       " ('brookings', 490),\n",
       " ('retail', 489),\n",
       " ('blog', 484),\n",
       " ('analytics', 483),\n",
       " ('machinelearning', 481),\n",
       " ('christmas', 478),\n",
       " ('onlinecareer', 470),\n",
       " ('news', 462),\n",
       " ('networking', 459),\n",
       " ('vr', 456),\n",
       " ('azure', 455),\n",
       " ('health', 453),\n",
       " ('digitaltransformation', 452),\n",
       " ('technology', 447),\n",
       " ('theatre', 433),\n",
       " ('globalpayroll', 425),\n",
       " ('water', 422),\n",
       " ('education', 421),\n",
       " ('photography', 416),\n",
       " ('manchester', 408),\n",
       " ('sanitation', 405),\n",
       " ('fitstats_en_gb', 403),\n",
       " ('writing', 397),\n",
       " ('edtech', 393),\n",
       " ('design', 390),\n",
       " ('facebook', 389),\n",
       " ('success', 388),\n",
       " ('africa', 382),\n",
       " ('entrepreneur', 381),\n",
       " ('socialmediatips', 380),\n",
       " ('sccm', 376),\n",
       " ('art', 370),\n",
       " ('mondaymotivation', 367),\n",
       " ('pr', 364),\n",
       " ('ya', 362),\n",
       " ('career', 360),\n",
       " ('sem', 358),\n",
       " ('privateequity', 358),\n",
       " ('configmgr', 357),\n",
       " ('hiring', 350),\n",
       " ('wedding', 349),\n",
       " ('ecommerce', 339),\n",
       " ('golf', 335),\n",
       " ('microsoft', 332),\n",
       " ('sap', 331),\n",
       " ('ux', 330),\n",
       " ('oracle', 330),\n",
       " ('competition', 328)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "mng_hash_2017=[]\n",
    "\n",
    "for i in range(0,len(mng_2017_rel)):\n",
    "    #print(mng_2017_rel[i])\n",
    "    text=df_mng.tweet[mng_2017_rel[i]]\n",
    "    \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "    text=re.sub(\"…\", '' ,text)\n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.)+\", ' ',text)  \n",
    "    \n",
    "    hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "    mng_hash_2017.extend(hashtags)\n",
    "    \n",
    "'''\n",
    "for i,value in enumerate(mng_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_mng['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        mng_hash_2017.extend(hashtags)\n",
    "'''\n",
    "        \n",
    "        \n",
    "mng_Hashtags_2017=dict()\n",
    "\n",
    "for ph in mng_hash_2017:\n",
    "    if ph !='':\n",
    "        if ph not in mng_Hashtags_2017:\n",
    "            mng_Hashtags_2017[ph]=1\n",
    "        else:\n",
    "            mng_Hashtags_2017[ph]+=1  \n",
    "\n",
    "        \n",
    "mng_Hashtags_2017 = sorted(mng_Hashtags_2017.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "mng_Hashtags_2017[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_mng=pd.DataFrame(mng_Hashtags_2017)\n",
    "df_hash_mng.columns =['Hashtags in mng','frequency']\n",
    "df_hash_mng=df_hash_mng.head(100)\n",
    "df_hash_mng.to_csv('hashtags_mng.csv')\n",
    "df_hash_mng_styled = df_hash_mng.style \n",
    "dfi.export(df_hash_mng_styled,\"hashtags_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.440707683563232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('job', 2243),\n",
       " ('ai', 2216),\n",
       " ('marketing', 1769),\n",
       " ('socialmedia', 1633),\n",
       " ('business', 1426),\n",
       " ('leadership', 1398),\n",
       " ('sportlomo', 1389),\n",
       " ('cybersecurity', 1253),\n",
       " ('innovation', 1054),\n",
       " ('tech', 939),\n",
       " ('seo', 933),\n",
       " ('iot', 892),\n",
       " ('digitalmarketing', 871),\n",
       " ('digitaltransformation', 832),\n",
       " ('brexit', 799),\n",
       " ('technology', 744),\n",
       " ('jobs', 693),\n",
       " ('cloud', 657),\n",
       " ('ux', 648),\n",
       " ('realestate', 642),\n",
       " ('startup', 620),\n",
       " ('data', 601),\n",
       " ('sales', 595),\n",
       " ('security', 595),\n",
       " ('healthcare', 575),\n",
       " ('management', 570),\n",
       " ('hiring', 559),\n",
       " ('contentmarketing', 556),\n",
       " ('dental', 534),\n",
       " ('dentalpractice', 532),\n",
       " ('blockchain', 528),\n",
       " ('1', 507),\n",
       " ('win', 503),\n",
       " ('dentalpracticemanager', 488),\n",
       " ('digital', 485),\n",
       " ('machinelearning', 483),\n",
       " ('socialmediamarketing', 468),\n",
       " ('climatechange', 467),\n",
       " ('artificialintelligence', 467),\n",
       " ('infosec', 466),\n",
       " ('hr', 464),\n",
       " ('feedly', 440),\n",
       " ('giveaway', 424),\n",
       " ('cx', 422),\n",
       " ('london', 420),\n",
       " ('fintech', 420),\n",
       " ('podcast', 416),\n",
       " ('blog', 411),\n",
       " ('success', 409),\n",
       " ('bitcoin', 391),\n",
       " ('microsoft', 386),\n",
       " ('entrepreneur', 380),\n",
       " ('disney', 380),\n",
       " ('nhs', 375),\n",
       " ('community', 371),\n",
       " ('mentalhealth', 369),\n",
       " ('bigdata', 369),\n",
       " ('smm', 353),\n",
       " ('prodmgmt', 350),\n",
       " ('travel', 349),\n",
       " ('google', 345),\n",
       " ('instagram', 344),\n",
       " ('teammicrofocus', 344),\n",
       " ('disneyjobs', 341),\n",
       " ('property', 339),\n",
       " ('retail', 337),\n",
       " ('smallbusiness', 334),\n",
       " ('facebook', 327),\n",
       " ('design', 325),\n",
       " ('investment', 322),\n",
       " ('projectmanagement', 318),\n",
       " ('analytics', 316),\n",
       " ('sustainability', 315),\n",
       " ('azure', 313),\n",
       " ('5g', 309),\n",
       " ('wordpress', 308),\n",
       " ('productivity', 295),\n",
       " ('investing', 295),\n",
       " ('recruiting', 295),\n",
       " ('networking', 291),\n",
       " ('startups', 289),\n",
       " ('wellbeing', 288),\n",
       " ('training', 287),\n",
       " ('writingcommunity', 287),\n",
       " ('mondaymotivation', 284),\n",
       " ('vacancy', 284),\n",
       " ('accounting', 284),\n",
       " ('art', 283),\n",
       " ('banking', 280),\n",
       " ('education', 279),\n",
       " ('health', 272),\n",
       " ('energy', 268),\n",
       " ('recruitment', 268),\n",
       " ('news', 267),\n",
       " ('microfocus', 263),\n",
       " ('freelance', 263),\n",
       " ('klbk', 261),\n",
       " ('christmas', 258),\n",
       " ('measure', 256),\n",
       " ('photography', 250)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "mng_hash_2019=[]\n",
    "\n",
    "for i in range(0,len(mng_2019_rel)):\n",
    "    #print(mng_2019_rel[i])\n",
    "    text=df_mng.tweet[mng_2019_rel[i]]\n",
    "    \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "    text=re.sub(\"…\", '' ,text)\n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.)+\", ' ',text)  \n",
    "    \n",
    "    hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "    mng_hash_2019.extend(hashtags)\n",
    "\n",
    "\n",
    "'''\n",
    "for i,value in enumerate(mng_year_ind_2019):\n",
    "    if label_19[i]==1:\n",
    "        text=cleaningA(df_mng['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        mng_hash_2019.extend(hashtags)\n",
    "'''        \n",
    "        \n",
    "        \n",
    "mng_Hashtags_2019=dict()\n",
    "\n",
    "for ph in mng_hash_2019:\n",
    "    if ph !='':\n",
    "        if ph not in mng_Hashtags_2019:\n",
    "            mng_Hashtags_2019[ph]=1\n",
    "        else:\n",
    "            mng_Hashtags_2019[ph]+=1  \n",
    "\n",
    "        \n",
    "mng_Hashtags_2019 = sorted(mng_Hashtags_2019.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "mng_Hashtags_2019[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequent hashtags in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "mng_hash=[]\n",
    "\n",
    "for i in range(0,len(mng_2017_rel)):\n",
    "    #print(mng_2017_rel[i])\n",
    "    text=df_mng.tweet[mng_2017_rel[i]]\n",
    "    #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "    hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "    mng_hash.extend(hashtags)\n",
    "    \n",
    "    \n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "\n",
    "        mng_hash.extend(hashtags)\n",
    "    \n",
    "mng_Hashtags=dict()\n",
    "\n",
    "for ph in mng_hash:\n",
    "    if ph != '':\n",
    "        if ph not in mng_Hashtags:\n",
    "            mng_Hashtags[ph]=1\n",
    "        else:\n",
    "            mng_Hashtags[ph]+=1  \n",
    "\n",
    "mng_Hashtags = sorted(mng_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_Hashtags[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_mng=pd.DataFrame(mng_Hashtags)\n",
    "df_hash_mng.columns =['Hashtags in mng','frequency']\n",
    "df_hash_mng=df_hash_mng.head(100)\n",
    "\n",
    "df_hash_mng.to_csv('hashtags_mng.csv')\n",
    "\n",
    "df_hash_mng_styled = df_hash_mng.style \n",
    "dfi.export(df_hash_mng_styled,\"hashtags_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignore below lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "mng_spacy=[]\n",
    "for i,value in enumerate(mng_year_ind_2017):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        mng_spacy.extend(concepts)\n",
    "    \n",
    "\n",
    "mng_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in mng_spacy:\n",
    "    if ph not in mng_noun_phrase_spacy:\n",
    "        mng_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        mng_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "mng_noun_phrase_spacy = sorted(mng_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "mng_spacy=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "        \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        mng_spacy.extend(concepts)\n",
    "    \n",
    "    \n",
    "mng_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in mng_spacy:\n",
    "    if ph not in mng_noun_phrase_spacy:\n",
    "        mng_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        mng_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "mng_noun_phrase_spacy = sorted(mng_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in MNG found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "## finding concepts by nltk\n",
    "#mng_2017_nltk=[]\n",
    "\n",
    "#mng_noun_phrase_2017_nltk=dict()\n",
    "\n",
    "#for i,value in enumerate(mng_year_ind_2017):\n",
    "#    if label_17[i]==1:\n",
    "#        text=cleaningA(df_mng['tweet'][value])\n",
    "##        print(text)\n",
    "##        print('----')\n",
    "#        #text=re.sub('#', ' ', text)\n",
    "#        concepts_x=noun_phrases(text)\n",
    "    \n",
    "#        concepts=[]\n",
    "#        for concept in concepts_x:\n",
    "#            if concept not in cachedStopWords:\n",
    "#                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "#                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "#                if concept !='':\n",
    "#                    if concept not in mng_noun_phrase_2017_nltk:\n",
    "#                        mng_noun_phrase_2017_nltk[concept]=1\n",
    "#                    else:\n",
    "#                        mng_noun_phrase_2017_nltk[concept]+=1\n",
    "                \n",
    "#                #concepts.append(concept)\n",
    "                \n",
    "##        print(concepts)\n",
    "##        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "#        #mng_2017_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in mng_2017_nltk:\n",
    "    if word !='':\n",
    "        if word not in mng_noun_phrase_2017_nltk:\n",
    "            mng_noun_phrase_2017_nltk[word]=1\n",
    "        else:\n",
    "            mng_noun_phrase_2017_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "#mng_noun_phrase_2017_nltk = sorted(mng_noun_phrase_2017_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "#print( time.time() - t0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_noun_phrase_2017_nltk[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i,value in enumerate(mng_year_ind_2017[0:1000000]):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_mng['tweet'][value])\n",
    "        #text= re.sub(\"#\", '',text0)\n",
    "        if 'Great ' in text:\n",
    "            if 'Great' in noun_phrases(text):\n",
    "                \n",
    "                print(text)\n",
    "                print(noun_phrases(text))\n",
    "                print('-----')\n",
    "                print(noun_phrases1(text))\n",
    "                print()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_noun_phrase_2017_nltk_non[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "yes_17=[]\n",
    "for i in mng_noun_phrase_2017_nltk[0:50]:\n",
    "    yes_17.append(i[0])\n",
    "\n",
    "non_17=[]\n",
    "for i in mng_noun_phrase_2017_nltk_non[0:50]:\n",
    "    non_17.append(i[0])\n",
    "A\n",
    "aaa=set(yes_17)-set(non_17)\n",
    "aaa\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())\n",
    "\n",
    "aaa=list(set(yes)-set(non))\n",
    "aaa=list(set(yes))\n",
    "concept_vec=np.zeros((len(aaa),300))\n",
    "\n",
    "for i,concept in enumerate(aaa):\n",
    "    concept_vec[i]=ft.get_word_vector(concept)\n",
    "\n",
    "\n",
    "np.shape(concept_vec)\n",
    "\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustering_dbscan = DBSCAN(eps=5, min_samples=2).fit(concept_vec)\n",
    "print(clustering_dbscan.labels_)\n",
    "\n",
    "clustering_optics = OPTICS(min_samples=3).fit(concept_vec)\n",
    "print(clustering_optics.labels_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2) \n",
    "mng_kmeans_clustering = kmeans.fit(concept_vec)\n",
    "print(mng_kmeans_clustering.labels_)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_mng=pd.DataFrame(mng_noun_phrase_nltk)\n",
    "df_ph_mng.columns =['noun_phrase in mng by NLTK','frequency']\n",
    "df_ph_mng=df_ph_mng.head(100)\n",
    "df_ph_mng.to_csv('noun_phrase_mng.csv')\n",
    "df_ph_mng_styled = df_ph_mng.style \n",
    "dfi.export(df_ph_mng_styled,\"noun_phrase_mng_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "yes_19=[]\n",
    "for i in mng_noun_phrase_2019_nltk[0:50]:\n",
    "    yes_19.append(i[0])\n",
    "\n",
    "non_19=[]\n",
    "for i in mng_noun_phrase_2019_nltk_non[0:50]:\n",
    "    non_19.append(i[0])\n",
    "\n",
    "bbb=set(yes_19)-set(non_19)\n",
    "bbb\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in MNG found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "#finding concepts by nltk \n",
    "mng_nltk=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                #concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                #concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                concepts.append(concept)\n",
    "        \n",
    "#        print(concepts)       \n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "        mng_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "mng_noun_phrase_nltk=dict()\n",
    "\n",
    "for word in mng_nltk:\n",
    "    if word !='':\n",
    "        if word not in mng_noun_phrase_nltk:\n",
    "            mng_noun_phrase_nltk[word]=1\n",
    "        else:\n",
    "            mng_noun_phrase_nltk[word]+=1   \n",
    "            \n",
    "mng_noun_phrase_nltk = sorted(mng_noun_phrase_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_noun_phrase_nltk[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_mng=pd.DataFrame(mng_noun_phrase_nltk)\n",
    "df_ph_mng.columns =['noun_phrase in mng by NLTK','frequency']\n",
    "df_ph_mng=df_ph_mng.head(100)\n",
    "\n",
    "df_ph_mng.to_csv('noun_phrase_mng.csv')\n",
    "\n",
    "df_ph_mng_styled = df_ph_mng.style \n",
    "dfi.export(df_ph_mng_styled,\"noun_phrase_mng_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "words_mng=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(mng_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_mng['tweet'][value])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                word= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',word)\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "    \n",
    "        for word in words_nostop:\n",
    "            #word= lemm(wordx)\n",
    "            if word not in words_mng:\n",
    "                words_mng[word]=1\n",
    "            else:\n",
    "                words_mng[word] +=1\n",
    "            \n",
    "words_mng = sorted(words_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_mng))\n",
    "\n",
    "print('First 100 frequent words in mng:')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_mng=pd.DataFrame(words_mng)\n",
    "df_word_mng.columns =['word in mng','frequency']\n",
    "df_word_mng=df_word_mng.head(100)\n",
    "df_word_mng.to_csv('word_mng.csv')\n",
    "df_word_mng_styled = df_word_mng.style \n",
    "dfi.export(df_word_mng_styled,\"words_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "words_mng=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                    \n",
    "        for word in words_nostop:\n",
    "            #word=lemm(wordx)\n",
    "            if word not in words_mng:\n",
    "                words_mng[word]=1\n",
    "            else:\n",
    "                words_mng[word] +=1\n",
    "                \n",
    "words_mng = sorted(words_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('First 100 frequent words in mng:')\n",
    "#words_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_mng=pd.DataFrame(words_mng)\n",
    "df_word_mng.columns =['word in mng','frequency']\n",
    "df_word_mng=df_word_mng.head(100)\n",
    "\n",
    "df_word_mng.to_csv('word_mng.csv')\n",
    "\n",
    "df_word_mng_styled = df_word_mng.style \n",
    "dfi.export(df_word_mng_styled,\"words_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_mng:\n",
    "                bigrams_mng[combi]=1\n",
    "            else:\n",
    "                bigrams_mng[combi] +=1\n",
    "                \n",
    "bigrams_mng = sorted(bigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_mng))\n",
    "\n",
    "print('first 100 frequent bigrams in mng:')\n",
    "\n",
    "bigrams_mng[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_mng=pd.DataFrame(bigrams_mng)\n",
    "df_bigram_mng.columns =['bigram in mng','frequency']\n",
    "df_bigram_mng=df_bigram_mng.head(100)\n",
    "df_bigram_mng.to_csv('bigram_mng.csv')\n",
    "df_bigram_mng_styled = df_bigram_mng.style \n",
    "dfi.export(df_bigram_mng_styled,\"bigrams_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_mng:\n",
    "                bigrams_mng[combi]=1\n",
    "            else:\n",
    "                bigrams_mng[combi] +=1\n",
    "                \n",
    "bigrams_mng = sorted(bigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent bigrams in mng:')\n",
    "#bigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_mng=pd.DataFrame(bigrams_mng)\n",
    "df_bigram_mng.columns =['bigram in mng','frequency']\n",
    "df_bigram_mng=df_bigram_mng.head(100)\n",
    "\n",
    "df_bigram_mng.to_csv('bigram_mng.csv')\n",
    "\n",
    "df_bigram_mng_styled = df_bigram_mng.style \n",
    "dfi.export(df_bigram_mng_styled,\"bigrams_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bigram_mng_styled.to_excel('jjdsfhj.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)  \n",
    "            if combi not in trigrams_mng:\n",
    "                trigrams_mng[combi]=1\n",
    "            else:\n",
    "                trigrams_mng[combi] +=1\n",
    "                \n",
    "trigrams_mng = sorted(trigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('Number of trigrams:',len(trigrams_mng))\n",
    "\n",
    "print('first 100 frequent trigrams in mng:')\n",
    "trigrams_mng[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)\n",
    "            if combi not in trigrams_mng:\n",
    "                trigrams_mng[combi]=1\n",
    "            else:\n",
    "                trigrams_mng[combi] +=1\n",
    "                \n",
    "trigrams_mng = sorted(trigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of trigrams:',len(trigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent trigrams in mng:')\n",
    "#trigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
