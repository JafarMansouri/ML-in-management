{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "### export PYTHONIOENCODING=utf-8  # at cmd of linux\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import html\n",
    "import xml.sax.saxutils as saxutils\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import fasttext.util\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_bert = DistilBertModel.from_pretrained('distilbert-base-uncased',\n",
    "                                       output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                       )  \n",
    "'''\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                       output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                       )\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(cachedStopWords))\n",
    "#print(len(cachedStopWords))\n",
    "#print(type(cachedStopWords))\n",
    "\n",
    "nltk_stopwords=set(cachedStopWords)\n",
    "\n",
    "english_alghabet=['b','c','e','f','g','h','j','k','l','n','p','q','r','u','v','w','x','z']\n",
    "\n",
    "numbers_remove=['one','two','three','four','five','six','seven','eight','nine','ten','tens','twenty',\n",
    "                'fourty','fifty','sixty','seventy','eighty','ninety','hundred','hundreds','million','billion','trillion',\n",
    "                'millions','thousand','thousands','second','third','forth','tenth','billions','trillions'] \n",
    "\n",
    "miscellaneous_remove=['absolutely', 'actually', 'adieu', 'ain', \"ain't\", 'aint', 'almost',\n",
    "                       'awesome','awfully','amazing','interesting',\n",
    "                       'alright','alrighty', 'amoungst', 'anybody', 'anymore', 'anyways', 'apart', 'apparently', 'anytime',\n",
    "                       'appropriate',  'approximately', 'arent', 'behold', 'better', 'bravo','briefly','bad','best','brilliant',\n",
    "                       'bye', 'cant', 'certainly', 'chrissakes', 'clearly', 'completely',\n",
    "                       'congrat', 'congrats','congratulation', 'congratulations', 'consequently', 'cool', 'couldnt',\n",
    "                       'darnit', 'de','dear', 'definitely','disappointing', 'didn', 'doesn', 'don', 'downwards',\n",
    "                       'disgusting','dude','down','eg',\"e.g.\",'i.e.',\n",
    "                       'encore','entirely', 'especially', 'et', 'etc', 'everybody', 'ex', 'exactly', 'excellent',\n",
    "                       'fantastic','far', 'farewell','funny',\n",
    "                       'felicitation', 'felicitations','finally', 'fully','furthermore', 'gadzooks', \n",
    "                       'good', 'goodby','goodness', 'gracious', 'great', \n",
    "                       'greetings', 'hallo', 'hardly', 'hasnt', 'haven', 'hello', 'here','hi', 'hither','higher','hopefully',\n",
    "                       'here','there','including',\n",
    "                       'howbeit', 'ie', 'immediately', 'inasmuch', 'inner', 'insofar', 'instead', 'inward', 'important',\n",
    "                       'indeed','just', \"it'd\", \"it'll\", 'inside','kertyschoo', 'kg', 'km', 'lackaday', \n",
    "                       'largely', 'lately', 'later','lovely','large','big','small',\n",
    "                       'lest', 'let', 'lets', 'likely', 'little', 'ltd', 'lower','magnificent', 'mainly', 'marvelous',\n",
    "                       'myself','yourself','yourselves','himself','herself','hisself','ourselves','themsleves',\n",
    "                       'maybe', 'meantime', 'merely', 'minus', 'near', 'nearly', 'necessary', 'never', \n",
    "                       'non', 'normally', 'obviously', 'ok', 'okay', 'ones', 'outside', 'over','other','others','only',\n",
    "                       'overall', 'particular', 'particularly', 'please', 'plus', 'poorly', 'possible','up',\n",
    "                       'possibly', 'potentially', 'predominantly', 'presumably', 'previously','primarily', 'probably',\n",
    "                       'promising',\n",
    "                       'promptly', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref',\n",
    "                       'refs', 'regardless', 'related', 'relatively', 'respectively', 'resulting', 'right', 'sec', \n",
    "                       'secondly','self', 'selves', 'seriously', 'shall', 'shucks','somebody', 'somethan','sorry',\n",
    "                       'somewhat', 'soon', 'late' , 'sorry', 'stupid', 'sub', 'substantially', 'successfully', 'sufficiently',\n",
    "                       'useful',\n",
    "                       'super', 'sure', \"t's\", 'th', 'thank', 'thanks', 'thanx', \"that've\", 'thats', 'there', \"there'll\",\n",
    "                       \"there've\", 'thered', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyre', 'thorough',\n",
    "                       'then','thankfully','too','today','yesterday','tomorrow','night',\"morning\",'afternoon','noon','tonight',\n",
    "                       'evening','day','everyday', 'everynight','todays','nights','mornings','noons','afternoons','days',\n",
    "                       'evenings','week','month','year',\n",
    "                       'thoroughly', 'tnx', 'too','truly', 'twice', 'undoubtedly','unfortunately', 'unlike','unlikely',\n",
    "                       'unto',  'usually', 'vs', 'welcome', 'well', 'went', 'werent', 'what', 'whatever', 'wheres', 'widely',\n",
    "                       'wonderful', 'wont', 'wouldnt', 'wrong', 'worst','worse','www', 'yes', 'youd', 'youre', 'yummy', \n",
    "                       'zoinks','shit','literally','literal','pleasure','effective','fabulous','delighted',\n",
    "                       'saturday','sunday','monday','tuesday','wednesday','thursday', 'friday','past','future','suitable',\n",
    "                       'much','many','less','least','few','lots','lot','fewer','fewset','therefore','pm',\n",
    "                       'afaik', 'br', 'idk','smh','qotd', 'ftw','bfn','yw', 'icymi','fomo','smdh', 'b4','imho',\n",
    "                       'urdddd','fab' ,'delightful','absolute','pleasure','huge','latest','nowadays',\n",
    "                       'january','february','april','june','july','august','september','october',\n",
    "                       'november','december', 'autumn' ,'spring','winter','summer',\n",
    "                       'mr','madam','sir','mrs','easy', 'difficult',\n",
    "                       'weekend','south','north','west','east','asia','africa','europe','america','totally',\n",
    "                       'come', 'comes', 'coming', 'came', 'seems', 'gives', 'gave', 'makes', 'made', 'keeps', 'kept', \n",
    "                       'calls', 'called', 'says', 'saying', 'said', 'goes', 'went', 'gone', 'got', 'saw', 'seen', 'shows',\n",
    "                       'shown', 'took', 'taken', 'uses', 'moved', 'moves', 'puts',\n",
    "                       'using','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "                       'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use',\n",
    "                       'find', 'finds', 'finding','aka',\n",
    "                       'lol' , 'brb', 'lmk', 'ama', 'tbh', 'irl', \"tl;dr\", 'fml', 'bfn' ,' br', 'ht', \"hth\",'j/k', 'lmao' ] #cool\n",
    "\n",
    "interjection_remove=['aaaahh', 'aaah', 'aaargh', 'aaay', 'aagh', 'aah',\n",
    "                   'aargh', 'achoo', 'adios', 'ah', 'aha', 'ahem', 'ahh', 'ahhh',\n",
    "                   'ahoy', 'alas', 'allo', 'amen', 'areet', 'argh', 'arrggh',\n",
    "                   'arrividerci', 'asap', 'attaboy', 'avaunt', 'aw', 'aw', 'aww',\n",
    "                   'awww', 'ay', 'ay', 'aye', 'ayeaugh', 'bada', 'badum', 'bah',\n",
    "                   'bahaha', 'bam', 'bazinga', 'behold', 'bingce', 'bingo', 'blah',\n",
    "                   'blech', 'bleh', 'blimey', 'bonjour', 'boo', 'booh', 'boohoo',\n",
    "                   'booyah', 'bravo', 'brr', 'brrrr', 'btw', 'bwahaha', 'capeesh',\n",
    "                   'capisce', 'cheerio', 'cheers', 'ciao', 'cor', 'cowabunga',\n",
    "                   'crikey', 'cripes', 'da', 'dabba', 'dah', 'dammit', 'damn', 'dang',\n",
    "                   'darn', 'de', 'dee', 'di', 'dizamn', 'doh', 'doo', 'drat', 'duh',\n",
    "                   'dum', 'eeeek', 'eek', 'eep', 'egad', 'egads', 'eh', 'ehem', 'em',\n",
    "                   'er', 'eureka', 'eww', 'ewww', 'eyh', 'fiddledeedee', 'fie',\n",
    "                   'fore', 'foul', 'fuff', 'gah', 'gak', 'gee', 'geez', 'gesundheit',\n",
    "                   'giddyap', 'golly', 'gosh', 'grr', 'grrrr', 'ha', 'hah', 'haha',\n",
    "                   'hahaha', 'hallelujah', 'halloa', 'harrumph', 'harumph', 'haw',\n",
    "                   'heck', 'heck', 'heeey', 'heh', 'hehe', 'hey', 'hhh', 'hic', 'hm',\n",
    "                   'hmm', 'hmmm', 'hmmmm', 'hmmph', 'hmpf', 'ho', 'hola', 'hoo',\n",
    "                   'hooray', 'howdy', 'hrmm', 'hrmph', 'hrmph', 'hrrmph', 'hu', 'huh',\n",
    "                   'hullo', 'humph', 'hurrah', 'huzza', 'huzzah', 'ich', 'ick',\n",
    "                   'ixnay', 'jeepers', 'jeez', 'kaboom', 'kapow', 'kerwham', 'la',\n",
    "                   'lala', 'lo', 'lordy', 'meh', 'mhm', 'ml', 'mm', 'mmh', 'mmhm',\n",
    "                   'mmm', 'muahaha', 'mwah', 'mwahaha', 'na','nay','nah', 'nanu', 'nooo', 'nope',\n",
    "                   'nuh', 'oh', 'ohh', 'oho', 'oi', 'okeydoke', 'om', 'oof', 'ooh',\n",
    "                   'oomph', 'oooh', 'ooooh', 'oops', 'ouch', 'ow', 'oww', 'oy',\n",
    "                   'oyez', 'oyh', 'pew', 'pff', 'pffh', 'pfft', 'phew', 'phut',\n",
    "                   'phweep', 'phwoar', 'phwoarr', 'poof', 'poogh', 'prethee',\n",
    "                   'prithee', 'prosit', 'pssh', 'psst', 'queep', 'roger', 'salaam',\n",
    "                   'salam', 'sheesh', 'shh', 'shhh', 'shitfire', 'shoo', 'shoop',\n",
    "                   'shush', 'sigh', 'sssh', 'strewth', 'ta', 'tarnations', 'tchah',\n",
    "                   'teehee', 'tish', 'touché', 'tsk', 'tss', 'tut', 'uggh', 'ugh',\n",
    "                   'uh', 'uhh', 'uhm', 'um', 'umm', 'ummm', 'umph', 'unh', 'upadaisy',\n",
    "                   'upsadaisy', 'ur', 'urgh', 'vay', 'vayf', 'viva', 'voila', 'waa',\n",
    "                   'waaaaah', 'waah', 'wah', 'wahey', 'wassup', 'weee', 'welp',\n",
    "                   'wham', 'whamo', 'whee', 'whew', 'whizz', 'whoa',\n",
    "                   'whoo', 'whoopee','whoop', 'whoops', 'whoopsy', 'whoosh', 'woah', 'woo',\n",
    "                   'woohoo', 'wotcha', 'wotcher', 'wow', 'wowsers', 'wowsers',\n",
    "                   'wuzzup', 'wuzzup', 'wuzzup', 'ya', 'yabba', 'yada', 'yadda',\n",
    "                   'yak', 'yarooh', 'yay', 'yea', 'yeah', 'yech', 'yee', 'yeeeeaah',\n",
    "                   'yeehaw', 'yeow', 'yes', 'yessiree', 'yew', 'yikes', 'yippee',\n",
    "                   'yo', 'yoo', 'yoohoo', 'yow', 'yowza', 'yuck', 'yuh', 'zing',\n",
    "                   'zoiks', 'zomfg', 'zomg', 'zounds', 'zut']\n",
    "             \n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "type(spacy_stopwords)\n",
    "#spacy_exclude=['using','name','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "#               'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use']# serious\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words1 = get_stop_words('english')\n",
    "#print(type(stop_words1))\n",
    "#print()\n",
    "#print(stop_words1)\n",
    "lib_stopwords=set(stop_words1)\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#print(type(ENGLISH_STOP_WORDS))\n",
    "#print()\n",
    "#print(set(ENGLISH_STOP_WORDS))\n",
    "#sklearn_exclude=['find','get','found','go','see','seem','seems','give','seemed','take','keep','show','put','made'] # system  cry\n",
    "sklearn_stopwords=set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "#spacy_stopwords.difference_update(set(spacy_exclude))\n",
    "#sklearn_stopwords.difference_update(set(sklearn_stopwords))\n",
    "#for removing \"just\" one item, use \"remove\"\n",
    "temp_1=set([])\n",
    "#temp_1.update(nltk_stopwords)\n",
    "#temp_1.update(lib_stopwords)\n",
    "#temp_1.update(sklearn_stopwords)\n",
    "#temp_1.update(spacy_stopwords)\n",
    "#temp_1.update(set(english_alghabet))\n",
    "#temp_1.update(set(numbers_remove)) \n",
    "#temp_1.update(set(miscellaneous_remove))\n",
    "#temp_1.update(set(interjection_remove))\n",
    "#temp_1.update(['rt','be','will','was','were','is','am','are','have','has','had','do','does','done'])\n",
    "#temp_1.update(['rt'])\n",
    "cachedStopWords=temp_1\n",
    "#len(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "#print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "#print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "#print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text) \n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower\n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", ''' \" ''', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \" ' \", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "#    text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "\n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    '''\n",
    "    #removing common expressions\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "#    text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "#    text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "#    text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"\\+\",' + ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    #text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘'_;•«»,@:~!\\=%&]+\", ' ',text) \n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)    \n",
    "    \n",
    "    text=re.sub('(^)rt ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text) # necessary for removing stopwords\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "##cur.execute(\"SELECT user_id, tweet from ent_2019_100K limit 100000 \")\n",
    "cur.execute(\"SELECT user_id, tweet from ent_2019_1000k \")\n",
    "rows_ent = cur.fetchall()\n",
    "con.close()\n",
    "\n",
    "print(len(rows_ent))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#df_ent = pd.read_csv('/archives1/Datasets/TweetsWorld/ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_ent = pd.read_csv('ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_ent.shape)\n",
    "print(df_ent.columns)\n",
    "#print(df_ent.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_ent=list(df_ent[['user_id', 'tweet','tweet_created_at']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_ent= list(zip(df_ent.user_id, df_ent.tweet))\n",
    "#rows_ent=df_ent[['user_id','tweet']].apply(tuple, axis=1) \n",
    "del df_ent\n",
    "\n",
    "print(\"Number of tweets in ent:\",len(rows_ent))  #rows_ent0\n",
    "print('Memory size of ent:',sys.getsizeof(rows_ent)) #rows_ent0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"ent_filtering.txt\", \"rb\") as fp:   \n",
    "#    ent_filter = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"_rand_2M_e.txt\", \"rb\") as fp:   \n",
    "#    rand_2M_e=pickle.load(fp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"_seed_ent_pos_1.txt\", \"rb\") as fp:   \n",
    "#    id_ent_rel = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"_seed_ent_neg_1.txt\", \"rb\") as fp:   \n",
    "#    id_ent_irrel = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"_tweets_ent_scores_1.txt\", \"rb\") as fp:   \n",
    "#    dic_ent_tweets_scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(id_ent_rel), len(id_ent_irrel), len(id_ent_rel)+len(id_ent_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_year_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "ent_year_ind_all=set()\n",
    "#ent_tweets_rows=[]\n",
    "\n",
    "for k, i in enumerate(rows_ent):\n",
    "    #ent_tweets_rows.append(cleaning(i[1]))\n",
    "    if year in i[2]:\n",
    "        if str(i[0]) not in ent_filter:\n",
    "            ent_year_ind_all.add(k)\n",
    "\n",
    "print( time.time() - t0)\n",
    "\n",
    "len(ent_year_ind_all)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind=pickle.load(fp)\n",
    "\n",
    "len(ent_year_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "#ent_users_rows=[]\n",
    "ent_tweets_rows=[]\n",
    "for i in rows_ent:\n",
    "    #ent_users_rows.append(i[0])\n",
    "    ent_tweets_rows.append(cleaning(i[1]))\n",
    "    \n",
    "print( time.time() - t0)\n",
    "\n",
    "#ent_users_rows_np=np.array(ent_users_rows)  \n",
    "#ent_users=np.unique(ent_users_rows_np)\n",
    "#print(len(ent_users))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/ent_cleaning_classification.txt\", \"wb\") as fp:  \n",
    "#    pickle.dump(ent_tweets_rows, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/ent_cleaning_classification.txt\", \"rb\") as fp:  \n",
    "    ent_tweets_rows=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "mng_users_rows=[]\n",
    "mng_tweets_rows=[]\n",
    "for i in rows_mng:\n",
    "    mng_users_rows.append(i[0])\n",
    "    mng_tweets_rows.append(cleaning(i[1]))\n",
    "\n",
    "print( time.time() - t0)\n",
    "\n",
    "mng_users_rows_np=np.array(mng_users_rows)  \n",
    "mng_users=np.unique(mng_users_rows_np)\n",
    "print(len(mng_users))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedder (tweets):\n",
    "    # padding after {SEP}\n",
    "    embedding_dim=768\n",
    "    t0 = time.time()\n",
    "    max_len_tokens=20\n",
    "    num_tweets=np.shape(tweets)[0]\n",
    "    tweets_embedded= np.zeros([num_tweets,max_len_tokens,embedding_dim],dtype=\"float32\")\n",
    "    #mng_full_embedding_word=[]\n",
    "#    mng_full_tokenized=[]\n",
    "    \n",
    "    for i, text in enumerate(tweets):\n",
    "\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#        print('a1',tokenized_text)\n",
    "        \n",
    "        # truncate if size of tekens is more than max_len_tokens\n",
    "        if len(tokenized_text)>max_len_tokens+2:\n",
    "#            print('a2',max_len_tokens+2)\n",
    "            tokenized_text= tokenized_text[0:max_len_tokens+2]\n",
    "#            print('a3',tokenized_text)\n",
    "            tokenized_text[max_len_tokens+2-1]=\"[SEP]\"\n",
    "#            print('a4',tokenized_text)\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#        print('a5',indexed_tokens)\n",
    "        indexed_tokens = indexed_tokens + [0] * (max_len_tokens+2 - len(indexed_tokens))\n",
    "#        print('a6',indexed_tokens)\n",
    "\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        temp=len(tokenized_text)\n",
    "        segments_ids= segments_ids + [0] * (max_len_tokens+2 - len(segments_ids)) \n",
    "#        print('a7',segments_ids) \n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(tokens_tensor, segments_tensors)\n",
    "            out = outputs[0]\n",
    "\n",
    "        token_embeddings = torch.squeeze(out, dim=0)  \n",
    "#        print('a8',token_embeddings)\n",
    "        token_embeddings=np.delete(token_embeddings, (0,temp-1), axis = 0)\n",
    "\n",
    "        tweets_embedded[i]=token_embeddings\n",
    "#        mng_full_tokenized.append(tokenized_text)\n",
    "\n",
    "#    print(np.shape(tweets_embedded))\n",
    "#    print( time.time() - t0)\n",
    "    return tweets_embedded    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedder2 (tweets):\n",
    "    #padding before {SEP}\n",
    "    embedding_dim=768\n",
    "    t0 = time.time()\n",
    "    max_len_tokens=20\n",
    "    num_tweets=np.shape(tweets)[0]\n",
    "    tweets_embedded= np.zeros([num_tweets,max_len_tokens,embedding_dim],dtype=\"float32\")\n",
    "    #mng_full_embedding_word=[]\n",
    "#    mng_full_tokenized=[]\n",
    "    \n",
    "    for i, text in enumerate(tweets):\n",
    "\n",
    "        marked_text = '[CLS]'+ text \n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#        print('a1',tokenized_text)\n",
    "        temp=len(tokenized_text)\n",
    "#        print(temp)\n",
    "        # truncate if size of tekens is more than max_len_tokens\n",
    "        if temp>=max_len_tokens+2:\n",
    "#            print('a2',max_len_tokens+2)\n",
    "            tokenized_text= tokenized_text[0:max_len_tokens+2]\n",
    "#            print('a3',tokenized_text)\n",
    "            tokenized_text[max_len_tokens+2-1]=\"[SEP]\"\n",
    "#            print('a4',tokenized_text)\n",
    "        else:\n",
    "#            print(type(tokenized_text))\n",
    "            tokenized_text = tokenized_text + ['[PAD]'] * (max_len_tokens+1 - temp)+ ['[SEP]']\n",
    "#            print('a44',tokenized_text)\n",
    "\n",
    "#            tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#                sequences, maxlen=10, dtype='int32', padding='post',\n",
    "#                truncating='post', value=0.0)\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#        print('a5',indexed_tokens)\n",
    "#        indexed_tokens = indexed_tokens + [0] * (max_len_tokens+2 - len(indexed_tokens))\n",
    "#        print('a6',indexed_tokens)\n",
    "\n",
    "#        segments_ids = [1] * len(tokenized_text)\n",
    "        segments_ids= [1]* min(temp,max_len_tokens+1) + [0] * (max_len_tokens+1 - temp)  + [1]\n",
    "#        print('a7',segments_ids) \n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(tokens_tensor, segments_tensors)\n",
    "            out = outputs[0]\n",
    "\n",
    "        token_embeddings = torch.squeeze(out, dim=0)  \n",
    "#        print('a8',token_embeddings)\n",
    "        token_embeddings=np.delete(token_embeddings, (0,-1), axis = 0)\n",
    "\n",
    "        tweets_embedded[i]=token_embeddings\n",
    "#        mng_full_tokenized.append(tokenized_text)\n",
    "\n",
    "#    print(np.shape(tweets_embedded))\n",
    "#    print( time.time() - t0)\n",
    "    return tweets_embedded    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedder0 (tweets):\n",
    "    #padding before {SEP} forcing zero\n",
    "    embedding_dim=768\n",
    "    t0 = time.time()\n",
    "    max_len_tokens=30\n",
    "    num_tweets=np.shape(tweets)[0]\n",
    "    tweets_embedded= np.zeros([num_tweets,max_len_tokens,embedding_dim],dtype=\"float32\")\n",
    "#    print('a11',np.shape(vector_temp))\n",
    "\n",
    "    #mng_full_embedding_word=[]\n",
    "#    mng_full_tokenized=[]\n",
    "    \n",
    "    for i, text in enumerate(tweets):\n",
    "        \n",
    "        \n",
    "                \n",
    "        marked_text = '[CLS]'+ text + '[SEP]'\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "#        print('a1',tokenized_text)\n",
    "        temp=len(tokenized_text)-2\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(tokens_tensor, segments_tensors)\n",
    "            out = outputs[0]\n",
    "        token_embeddings = torch.squeeze(out, dim=0)  \n",
    "#        print('a8',token_embeddings)\n",
    "        token_embeddings=np.delete(token_embeddings, (0,-1), axis = 0)\n",
    "#        print('a88',len(token_embeddings))\n",
    "        if len(token_embeddings)>=max_len_tokens:\n",
    "            vector_temp= token_embeddings[0:max_len_tokens]\n",
    "#            print('a9',vector_temp)\n",
    "        else:\n",
    "            vector_temp = np.zeros([max_len_tokens,embedding_dim],dtype=\"float32\")\n",
    "            vector_temp[0:temp] = token_embeddings\n",
    "#            print('a99',vector_temp)\n",
    "        tweets_embedded[i]=vector_temp\n",
    "#        mng_full_tokenized.append(tokenized_text)\n",
    "\n",
    "#    print(np.shape(tweets_embedded))\n",
    "#    print( time.time() - t0)\n",
    "    return tweets_embedded    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa=embedder0(['That is very good and joy '])\n",
    "#aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand_inds1e=random.sample(range(0, 1000000), 300000) #200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"rand_inds1e.txt\", \"wb\") as fp:   \n",
    "#    pickle.dump(rand_inds1e, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "temp_set=set()\n",
    "seed_public0=[]\n",
    "for i in rand_inds1e:\n",
    "    temp=public_tweets_rows[i]\n",
    "    if temp !='' and temp !=' ':\n",
    "        if temp not in temp_set:\n",
    "            seed_public0.append(public_tweets_rows[i])\n",
    "            temp_set.add(temp) # in order that we do not have repetative tweets\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# be careful: it has been changed \n",
    "seed_negative_all=[]\n",
    "for index in id_ent_irrel:\n",
    "    #print(index)\n",
    "    #print(ent_tweets_rows[index])\n",
    "    #print(rows_ent[index])\n",
    "    seed_negative_all.append(ent_tweets_rows[index])  \n",
    "\n",
    "len(seed_negative_all)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "seed_ent_all=[]\n",
    "for index in id_ent_rel:\n",
    "    #print(index)\n",
    "    #print(ent_tweets_rows[index])\n",
    "    #print(rows_ent[index]) \n",
    "    seed_ent_all.append(ent_tweets_rows[index])    \n",
    "\n",
    "len(seed_ent_all)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices=np.arange(len(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_texts, test_texts, y_train, y_test,idx1,idx2 = train_test_split(x_data, y_data, indices, test_size=.2, shuffle=True, stratify= y_data)\n",
    "print(\"train set size \" + str(len(train_texts)))\n",
    "print(\"test set size \" + str(len(test_texts)))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "#tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "train_texts_tokens=[]\n",
    "for text in train_texts:\n",
    "#    tokens=text.split()\n",
    "    #tokens=nltk.word_tokenize(text)\n",
    "    tokens=tknzr.tokenize(text)\n",
    "    train_texts_tokens.append(tokens)\n",
    "    \n",
    "print(len(train_texts_tokens))\n",
    "time.time()-t0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext.util.reduce_model(ft, 100)\n",
    "#ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "#X_train_test=[]\n",
    "x_train=np.zeros([len(train_texts_tokens),max_tokens,embedding_dim],dtype='float32')\n",
    "\n",
    "for kk,tokens in enumerate(train_texts_tokens):\n",
    "    if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "        a1=[]\n",
    "        for token in tokens:\n",
    "            a1.append( ft.get_word_vector(token) )\n",
    "                    \n",
    "        temp=np.zeros([max_tokens,embedding_dim])\n",
    "        if len(tokens)>max_tokens:\n",
    "            temp=a1[0:max_tokens]\n",
    "        elif len(tokens)==max_tokens:\n",
    "            temp=a1\n",
    "        else: # if len(tokens)<max_tokens:\n",
    "            temp[0:len(tokens)]=a1\n",
    "#        X_train_test.append(temp) \n",
    "        x_train[kk]=temp\n",
    "\n",
    "    else:\n",
    "        temp=np.zeros([max_tokens,embedding_dim])\n",
    "#        X_train_test.append(temp)\n",
    "        x_train[kk]=temp\n",
    "        \n",
    "time.time()-t0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = tf.cast(x_train, tf.float32)\n",
    "#y_train = tf.cast(y_train, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train1, x_test1, y_train, y_test = train_test_split(data, y_data, test_size=0.2, shuffle=True, stratify= y_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=50\n",
    "num_filters=100\n",
    "embedding_dim=300   #768 for bert\n",
    "dropout_rate=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(sequence_length,embedding_dim,), dtype='float32')\n",
    "#inputs = Input(shape=(sequence_length,), dtype='float32')\n",
    "\n",
    "#embedded_inputs = embedding_layer(inputs)\n",
    "#embedding_layer = Embedding(input_dim=20000, output_dim=embedding_dim, input_length=sequence_length, weights=[inputs])(inputs)\n",
    "\n",
    "#inputs_reshaped = Reshape((sequence_length, embedding_dim, 1))(embedded_inputs)\n",
    "inputs_reshaped = Reshape((sequence_length, embedding_dim, 1))(inputs)\n",
    "\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(1, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_1 = LeakyReLU(alpha=0.2)(conv_1) #without activation at Conv2D\n",
    "conv_1 = BatchNormalization()(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(2, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_2 = LeakyReLU(alpha=0.2)(conv_2)\n",
    "conv_2 = BatchNormalization()(conv_2)\n",
    "\n",
    "conv_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_3 = LeakyReLU(alpha=0.2)(conv_3)\n",
    "conv_3 = BatchNormalization()(conv_3)\n",
    "\n",
    "conv_4 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_4 = LeakyReLU(alpha=0.2)(conv_4)\n",
    "conv_4 = BatchNormalization()(conv_4)\n",
    "\n",
    "conv_5 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_5 = LeakyReLU(alpha=0.2)(conv_5)\n",
    "conv_5 = BatchNormalization()(conv_5)\n",
    "\n",
    "conv_6 = Conv2D(num_filters, kernel_size=(6, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_6 = LeakyReLU(alpha=0.2)(conv_6)\n",
    "conv_6 = BatchNormalization()(conv_6)\n",
    "\n",
    "conv_7 = Conv2D(num_filters, kernel_size=(7, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_7 = LeakyReLU(alpha=0.2)(conv_7)\n",
    "conv_7 = BatchNormalization()(conv_7)\n",
    "\n",
    "conv_8 = Conv2D(num_filters, kernel_size=(8, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(inputs_reshaped)\n",
    "#conv_8 = LeakyReLU(alpha=0.2)(conv_8)\n",
    "#conv_8 = BatchNormalization()(conv_8)\n",
    "\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 1 + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 2 + 1, 1), strides=(1,1))(conv_2)\n",
    "maxpool_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1))(conv_3)\n",
    "maxpool_4 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1))(conv_4)\n",
    "maxpool_5 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1))(conv_5)\n",
    "maxpool_6 = MaxPool2D(pool_size=(sequence_length - 6 + 1, 1), strides=(1,1))(conv_6)\n",
    "maxpool_7 = MaxPool2D(pool_size=(sequence_length - 7 + 1, 1), strides=(1,1))(conv_7)\n",
    "maxpool_8 = MaxPool2D(pool_size=(sequence_length - 8 + 1, 1), strides=(1,1))(conv_8)\n",
    "\n",
    "#concatenated_e1 = Concatenate(axis=3)([maxpool_1, maxpool_2, maxpool_3, maxpool_4, maxpool_5])\n",
    "concatenated_e1 = Concatenate(axis=3)([ maxpool_1, maxpool_2, maxpool_3, maxpool_4, maxpool_5, maxpool_6, maxpool_7 ])\n",
    "\n",
    "X = Flatten()(concatenated_e1)\n",
    "\n",
    "X = Dense(units=512, activation='linear')(X) \n",
    "X = Dropout(dropout_rate)(X)\n",
    "\n",
    "X = Dense(units=256, activation='linear')(X) \n",
    "X = Dropout(dropout_rate)(X)\n",
    "\n",
    "X = Dense(units=128, activation='linear')(X) \n",
    "X = Dropout(dropout_rate)(X)\n",
    "\n",
    "X = Dense(units=64, activation='linear')(X) \n",
    "X = Dropout(dropout_rate)(X)\n",
    "\n",
    "X = Dense(units=32, activation='linear')(X) \n",
    "X = Dropout(dropout_rate)(X)\n",
    "\n",
    "X = Dense(units=16, activation='linear')(X)\n",
    "X = Dropout(dropout_rate)(X)\n",
    "\n",
    "X = Dense(units=8, activation='linear')(X)\n",
    "X = Dropout(dropout_rate)(X)\n",
    "\n",
    "#outputs = Dense(units=5, activation='softmax')(X) \n",
    "outputs = Dense(units=1, activation='sigmoid')(X) \n",
    "\n",
    "classifier = keras.Model(inputs, outputs)\n",
    "classifier.summary()\n",
    "#classifier.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3) , loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.compile(optimizer=keras.optimizers.Adamax() , loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#optim= adamax, adam\n",
    "#loss:categorical_crossentropy, KLDivergence,\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# not good results ##type 2( small kernels like images)\n",
    "#sequence_length=16 # or 24 or 32 or 64 \n",
    "##encoder\n",
    "print(sequence_length,embedding_dim)\n",
    "inputs = Input(shape=(sequence_length,embedding_dim,), dtype='float32')\n",
    "\n",
    "x = Reshape((sequence_length, embedding_dim, 1))(inputs)\n",
    "print(x.shape)\n",
    "x = Conv2D(8, (4,4), activation='relu', padding='same')(x)\n",
    "print(x.shape)\n",
    "x = MaxPooling2D((2, 3), padding='same')(x)\n",
    "print(x.shape)\n",
    "x = Conv2D(8, (4,4), activation='relu', padding='same')(x)\n",
    "print(x.shape)\n",
    "x = MaxPooling2D((2, 3), padding='same')(x)\n",
    "print(x.shape)\n",
    "x = Conv2D(8, (2, 2), activation='relu', padding='same')(x)\n",
    "print(x.shape)\n",
    "x = MaxPooling2D((2, 3), padding='same')(x) \n",
    "print(x.shape)\n",
    "\n",
    "#print(x.shape)\n",
    "\n",
    "x = Flatten()(x)\n",
    "print(np.shape(x))\n",
    "#x = Dense(1764, activation=\"relu\")(x)\n",
    "\n",
    "#x = Dense(units=8, activation='linear')(x)\n",
    "#x = Dropout(dropout_rate)(x)\n",
    "\n",
    "outputs = Dense(units=5, activation='softmax')(x) \n",
    "\n",
    "classifier = keras.Model(inputs, outputs)\n",
    "#optim = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#classifier.compile(optimizer=optim)\n",
    "classifier.compile(optimizer=keras.optimizers.Adam() , loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.summary()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "#history= classifier.fit(x_train,x_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=1,batch_size=32,epochs=1000)\n",
    "print(np.shape(x_train))\n",
    "\n",
    "history=classifier.fit(x_train,y_train,\n",
    "                        epochs=10,\n",
    "                        #callbacks=[monitor],\n",
    "                        batch_size=512, # or 64\n",
    "                        shuffle= True,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.125) #0.25\n",
    "\n",
    "#history = classifier.fit(x_train, y_train, epochs=8, batch_size=512, verbose=1, validation_split=0.25, shuffle=True)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.save('./all_neg_first_pos/2018/case2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.models.load_model('./all_neg_first_pos/2018/case3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# for bert\n",
    "t0=time.time()\n",
    "ent_ind_detected=[]\n",
    "b=ent_tweets_rows[0:200000]\n",
    "ent_test_embedded=embedder0(b)\n",
    "y_predicted=classifier.predict(ent_test_embedded)\n",
    "for ind, value in enumerate(y_predicted):\n",
    "    if value >= 0.5:\n",
    "        ent_ind_detected.append(ind)\n",
    "print(time.time()-t0) \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "max_tokens=50\n",
    "embedding_dim=300\n",
    "batch_size=1000\n",
    "batch=0\n",
    "#len_ent=len(ent_tweets_rows)\n",
    "len_ent=len(test_texts)\n",
    "\n",
    "results_ent=np.zeros([len_ent,1])\n",
    "\n",
    "while batch <len_ent:\n",
    "    test =  test_texts [batch : batch + batch_size]\n",
    "\n",
    "#    test =  ent_tweets_rows [batch : batch + batch_size]\n",
    "    test_data=[]\n",
    "    \n",
    "    for tweet in test:\n",
    "    #    tokens=text.split()\n",
    "        #tokens=nltk.word_tokenize(tweet)\n",
    "        tokens=tknzr.tokenize(tweet)\n",
    "        if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "            a1=[]\n",
    "            for token in tokens[0:max_tokens]:\n",
    "                a1.append( ft.get_word_vector(token) )\n",
    "            \n",
    "            a1=np.asarray(a1, dtype=np.float32)            \n",
    "            temp=np.zeros([max_tokens,embedding_dim])\n",
    "\n",
    "            if len(tokens)>max_tokens:\n",
    "                temp=a1[0:max_tokens]\n",
    "            elif len(tokens)==max_tokens:\n",
    "                temp=a1\n",
    "            else: #if len(tokens)<max_tokens:\n",
    "                temp[0:len(tokens)]=a1\n",
    "        \n",
    "            #x_test=temp.reshape((1,max_tokens,embedding_dim))  \n",
    "            #print(np.shape(temp))\n",
    "            #x_test = tf.cast(temp, tf.float32)\n",
    "            test_data.append(temp)\n",
    "            \n",
    "        else:\n",
    "            test_data.append(np.zeros([max_tokens,embedding_dim]))\n",
    "              \n",
    "#    test_data = tf.cast(test_data, tf.float32)\n",
    "    test_data = np.asarray(test_data, dtype=np.float32)\n",
    "    res_test=classifier.predict(test_data)\n",
    "    results_ent [ batch: batch +len (res_test)]= res_test\n",
    "    batch += batch_size\n",
    "     \n",
    "time.time()-t0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(results_ent),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_correct=0\n",
    "false_val=[]\n",
    "for k,i in enumerate(results_ent):\n",
    "    #print(k)\n",
    "    #print(i)\n",
    "    if i>0.5:\n",
    "        temp=1\n",
    "    else:\n",
    "        temp=0\n",
    "    \n",
    "    if temp == y_test[k]:\n",
    "        n_correct +=1\n",
    "    else:\n",
    "        false_val.append(i)\n",
    "        \n",
    "        \n",
    "accuracy=    n_correct/len(results_ent)\n",
    "accuracy\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_test=seed_ent_all[0:len(seed_ent_all)-len(seed_negative)]\n",
    "#len(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/\"+year+\"_tweets_ent_scores_1.txt\", \"rb\") as fp:  \n",
    "#    dic_ent_tweets_scores=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_unused=id_ent_rel+id_ent_irrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_unused=id_ent_rel[num:]+id_ent_irrel[num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rand_inds=np.sort(random.sample(range(0, len(id_unused)), 100000))\n",
    "len(rand_inds)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "id_new=[]\n",
    "for i in rand_inds:\n",
    "    id_new.append(id_unused[i])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_new=id_unused\n",
    "#len(id_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in id_new[0:0]:\n",
    "    print(i)\n",
    "    if i in id_unused:\n",
    "        print('yes')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test=[]\n",
    "for i in ent_year_ind:\n",
    "    new_test.append(ent_tweets_rows[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_test=seed_ent_all[len(seed_negative):]\n",
    "#len(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_test1=seed_ent_all[len(seed_ent):]\n",
    "#len(new_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_test2=seed_negative_all[len(seed_negative):]\n",
    "#len(new_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_test=new_test1+new_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices_unused=id_ent_rel[num:]+id_ent_irrel[num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic_ent_tweets_scores[id_ent_rel[num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic_ent_tweets_scores[id_ent_irrel[num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores=list(dic_ent_tweets_scores.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i , k in scores[0:5]:\n",
    "    print(i,k)\n",
    "    \n",
    "print()\n",
    "\n",
    "for i , k in scores[-5:-1]:\n",
    "    print(i,k)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5459.147403001785"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0=time.time()\n",
    "\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "batch_size=1000\n",
    "batch=0\n",
    "\n",
    "len_ent_test_new=len(new_test)\n",
    "\n",
    "results_ent_test_new=np.zeros([len_ent_test_new,1])\n",
    "\n",
    "while batch <len_ent_test_new:\n",
    "    test =  new_test [batch : batch + batch_size]\n",
    "    test_data=[]\n",
    "    \n",
    "    for tweet in test:\n",
    "    #    tokens=text.split()\n",
    "        #tokens=nltk.word_tokenize(tweet)\n",
    "        tokens=tknzr.tokenize(tweet)\n",
    "\n",
    "        if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "            a1=[]\n",
    "            for token in tokens[0:max_tokens]:\n",
    "                a1.append( ft.get_word_vector(token) )\n",
    "            \n",
    "            a1=np.asarray(a1, dtype=np.float32)            \n",
    "            temp=np.zeros([max_tokens,embedding_dim])\n",
    "\n",
    "            if len(tokens)>max_tokens:\n",
    "                temp=a1[0:max_tokens]\n",
    "            elif len(tokens)==max_tokens:\n",
    "                temp=a1\n",
    "            else: #if len(tokens)<max_tokens:\n",
    "                temp[0:len(tokens)]=a1\n",
    "        \n",
    "            #x_test=temp.reshape((1,max_tokens,embedding_dim))  \n",
    "            #print(np.shape(temp))\n",
    "            #x_test = tf.cast(temp, tf.float32)\n",
    "            test_data.append(temp)\n",
    "            \n",
    "        else:\n",
    "            test_data.append(np.zeros([max_tokens,embedding_dim]))\n",
    "              \n",
    "#    test_data = tf.cast(test_data, tf.float32)\n",
    "    test_data = np.asarray(test_data, dtype=np.float32)\n",
    "    res_test=classifier.predict(test_data)\n",
    "    results_ent_test_new [ batch: batch +len (res_test)]= res_test\n",
    "    batch += batch_size\n",
    "     \n",
    "time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./years/\"+year+\"result_ent_bin_3.txt\", \"wb\") as fp:  \n",
    "    pickle.dump(results_ent_test_new,fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_correct=0\n",
    "false_values=[]\n",
    "correct_values=[]\n",
    "\n",
    "for k,i in enumerate(results_ent_test_new):\n",
    "    #print(k)\n",
    "    #print(i)\n",
    "    if i>0.5:\n",
    "        temp=1\n",
    "    else:\n",
    "        temp=-1\n",
    "    \n",
    "    #print(k)\n",
    "    #temp1=k+len(seed_public)\n",
    "    #print(temp1)\n",
    "    #print(id_ent_rel[temp1])\n",
    "    #print(dic_ent_tweets_scores[id_ent_rel[temp1]])\n",
    "    #print(dic_ent_tweets_scores[id_ent_rel[len(seed_public)+k]])\n",
    "    if np.sign(temp) == np.sign(dic_ent_tweets_scores[id_new[k]]):\n",
    "        n_correct +=1\n",
    "        #correct_values.append(dic_ent_tweets_scores[id_ent_rel[len(seed_negative)+k]])\n",
    "        #correct_values.append(dic_ent_tweets_scores[id_ent_rel[k]])\n",
    "        correct_values.append(dic_ent_tweets_scores[id_new[k]])\n",
    "\n",
    "    else:\n",
    "        #false_values.append(dic_ent_tweets_scores[id_ent_rel[len(seed_negative)+k]])\n",
    "        #false_values.append(dic_ent_tweets_scores[id_ent_rel[k]])\n",
    "        false_values.append(dic_ent_tweets_scores[id_new[k]])\n",
    "\n",
    "\n",
    "        \n",
    "accuracy=    n_correct/len(results_ent_test_new)\n",
    "accuracy\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.min(correct_values), np.min(false_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.max(correct_values), np.max(false_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "positive_values=[]\n",
    "\n",
    "for i in id_ent_rel:\n",
    "    positive_values.append(dic_ent_tweets_scores[i])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_ent_rel[0],dic_ent_tweets_scores[id_ent_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_ent_irrel[0],dic_ent_tweets_scores[id_ent_irrel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#fig = plt.figure()\n",
    "fig = plt.figure(figsize=(8, 10))\n",
    "\n",
    "plt.title('false distribution')\n",
    "plt.xlabel(' score')\n",
    "plt.ylabel('number of samples')\n",
    "#n_max=dic_ent_tweets_scores[id_ent_rel[len(seed_public)]]\n",
    "#x = np.random.randn(1000)\n",
    "plt.hist(false_values, bins=100)\n",
    "fig.savefig('./all_neg_first_pos/'+year+'/false_distribution2_all.jpg')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from matplotlib.pyplot import figure\n",
    "#figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 10))\n",
    "\n",
    "\n",
    "plt.title('correct distribution')\n",
    "plt.xlabel(' score')\n",
    "plt.ylabel('number of samples')\n",
    "#n_max=dic_ent_tweets_scores[id_ent_rel[len(seed_public)]]\n",
    "#x = np.random.randn(1000)\n",
    "plt.hist(correct_values, bins=100)\n",
    "fig.savefig('./all_neg_first_pos/'+year+'/correct_distribution2_all.jpg')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from matplotlib.pyplot import figure\n",
    "#figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 10))\n",
    "plt.title('all positive distribution')\n",
    "plt.xlabel('positive score')\n",
    "plt.ylabel('number of samples')\n",
    "#n_max=dic_ent_tweets_scores[id_ent_rel[len(seed_public)]]\n",
    "#x = np.random.randn(1000)\n",
    "plt.hist(positive_values, bins=40)\n",
    "fig.savefig('./all_neg_first_pos/2017/all_distribution2.jpg')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(false_values), len(correct_values), len(false_values)+len(correct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(10*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"ent_results300.txt\", \"wb\") as fp:   \n",
    "#    pickle.dump(results_ent, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_ent_numpy=np.asarray(results_ent, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(results_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"ent_results300_np.txt\", \"wb\") as fp:   \n",
    "#    pickle.dump(results_ent_numpy, fp , protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
