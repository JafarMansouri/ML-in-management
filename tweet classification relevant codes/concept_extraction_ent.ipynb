{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import html\n",
    "import xml.sax.saxutils as saxutils\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "import random\n",
    "import operator\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "import dataframe_image as dfi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import fasttext.util\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(cachedStopWords))\n",
    "#print(len(cachedStopWords))\n",
    "#print(type(cachedStopWords))\n",
    "\n",
    "nltk_stopwords=set(cachedStopWords)\n",
    "\n",
    "english_alghabet=['b','c','e','f','g','h','j','k','l','n','p','q','r','u','v','w','x','z']\n",
    "\n",
    "numbers_remove=['one','two','three','four','five','six','seven','eight','nine','ten','tens','twenty',\n",
    "                'fourty','fifty','sixty','seventy','eighty','ninety','hundred','hundreds','million','billion','trillion',\n",
    "                'millions','thousand','thousands','second','third','forth','tenth','billions','trillions'] \n",
    "\n",
    "miscellaneous_remove=['absolutely', 'actually', 'adieu', 'ain', \"ain't\", 'aint', 'almost',\n",
    "                       'awesome','awfully','amazing','interesting',\n",
    "                       'alright','alrighty', 'amoungst', 'anybody', 'anymore', 'anyways', 'apart', 'apparently', 'anytime',\n",
    "                       'appropriate',  'approximately', 'arent', 'behold', 'better', 'bravo','briefly','bad','best','brilliant',\n",
    "                       'bye', 'cant', 'certainly', 'chrissakes', 'clearly', 'completely',\n",
    "                       'congrat', 'congrats','congratulation', 'congratulations', 'consequently', 'cool', 'couldnt',\n",
    "                       'darnit', 'de','dear', 'definitely','disappointing', 'didn', 'doesn', 'don', 'downwards',\n",
    "                       'disgusting','dude','down','eg',\"e.g.\",'i.e.',\n",
    "                       'encore','entirely', 'especially', 'et', 'etc', 'everybody', 'ex', 'exactly', 'excellent',\n",
    "                       'fantastic','far', 'farewell','funny',\n",
    "                       'felicitation', 'felicitations','finally', 'fully','furthermore', 'gadzooks', \n",
    "                       'good', 'goodby','goodness', 'gracious', 'great', \n",
    "                       'greetings', 'hallo', 'hardly', 'hasnt', 'haven', 'hello', 'here','hi', 'hither','higher','hopefully',\n",
    "                       'here','there','including',\n",
    "                       'howbeit', 'ie', 'immediately', 'inasmuch', 'inner', 'insofar', 'instead', 'inward', 'important',\n",
    "                       'indeed','just', \"it'd\", \"it'll\", 'inside','kertyschoo', 'kg', 'km', 'lackaday', \n",
    "                       'largely', 'lately', 'later','lovely',\n",
    "                       'lest', 'let', 'lets', 'likely', 'little', 'ltd', 'lower','magnificent', 'mainly', 'marvelous',\n",
    "                       'myself','yourself','yourselves','himself','herself','hisself','ourselves','themsleves',\n",
    "                       'maybe', 'meantime', 'merely', 'minus', 'near', 'nearly', 'necessary', 'never', \n",
    "                       'non', 'normally', 'obviously', 'ok', 'okay', 'ones', 'outside', 'over','other','others','only',\n",
    "                       'overall', 'particular', 'particularly', 'please', 'plus', 'poorly', 'possible','up',\n",
    "                       'possibly', 'potentially', 'predominantly', 'presumably', 'previously','primarily', 'probably',\n",
    "                       'promising',\n",
    "                       'promptly', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref',\n",
    "                       'refs', 'regardless', 'related', 'relatively', 'respectively', 'resulting', 'right', 'sec', \n",
    "                       'secondly','self', 'selves', 'seriously', 'shall', 'shucks','somebody', 'somethan','sorry',\n",
    "                       'somewhat', 'soon', 'late' , 'sorry', 'stupid', 'sub', 'substantially', 'successfully', 'sufficiently',\n",
    "                       \n",
    "                       'super', 'sure', \"t's\", 'th', 'thank', 'thanks', 'thanx', \"that've\", 'thats', 'there', \"there'll\",\n",
    "                       \"there've\", 'thered', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyre', 'thorough',\n",
    "                       'then','thankfully','too','today','yesterday','tomorrow','night',\"morning\",'afternoon','noon','tonight',\n",
    "                       'evening','day','everyday', 'everynight','todays','nights','mornings','noons','afternoons','days',\n",
    "                       'evenings','week','month','year',\n",
    "                       'thoroughly', 'tnx', 'too','truly', 'twice', 'undoubtedly','unfortunately', 'unlike','unlikely',\n",
    "                       'unto',  'usually', 'vs', 'welcome', 'well', 'went', 'werent', 'what', 'whatever', 'wheres', 'widely',\n",
    "                       'wonderful', 'wont', 'wouldnt', 'wrong', 'worst','worse','www', 'yes', 'youd', 'youre', 'yummy', \n",
    "                       'zoinks','shit','literally','literal','pleasure','effective','fabulous','delighted',\n",
    "                       'saturday','sunday','monday','tuesday','wednesday','thursday', 'friday','past','future','suitable',\n",
    "                       'much','many','less','least','few','lots','lot','fewer','fewset','therefore','pm',\n",
    "                       'afaik', 'br', 'idk','smh','qotd', 'ftw','bfn','yw', 'icymi','fomo','smdh', 'b4','imho',\n",
    "                       'urdddd','fab' ,'delightful','absolute','pleasure','huge','latest','nowadays',\n",
    "                       'january','february','april','june','july','august','september','october',\n",
    "                       'november','december', 'autumn' ,'spring','winter','summer',\n",
    "                       'mr','madam','sir','mrs','easy', 'difficult',\n",
    "                       'weekend','south','north','west','east','asia','africa','europe','america','totally',\n",
    "                       'come', 'comes', 'coming', 'came', 'seems', 'gives', 'gave', 'makes', 'made', 'keeps', 'kept', \n",
    "                       'calls', 'called', 'says', 'saying', 'said', 'goes', 'went', 'gone', 'got', 'saw', 'seen', 'shows',\n",
    "                       'shown', 'took', 'taken', 'uses', 'moved', 'moves', 'puts',\n",
    "                       'using','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "                       'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use',\n",
    "                       'find', 'finds', 'finding','aka',\n",
    "                       'lol' , 'brb', 'lmk', 'ama', 'tbh', 'irl', \"tl;dr\", 'fml', 'bfn' ,' br', 'ht', \"hth\",'j/k', 'lmao' ] #cool\n",
    "                        # small big large useful \n",
    "\n",
    "interjection_remove=['aaaahh', 'aaah', 'aaargh', 'aaay', 'aagh', 'aah',\n",
    "                   'aargh', 'achoo', 'adios', 'ah', 'aha', 'ahem', 'ahh', 'ahhh',\n",
    "                   'ahoy', 'alas', 'allo', 'amen', 'areet', 'argh', 'arrggh',\n",
    "                   'arrividerci', 'asap', 'attaboy', 'avaunt', 'aw', 'aw', 'aww',\n",
    "                   'awww', 'ay', 'ay', 'aye', 'ayeaugh', 'bada', 'badum', 'bah',\n",
    "                   'bahaha', 'bam', 'bazinga', 'behold', 'bingce', 'bingo', 'blah',\n",
    "                   'blech', 'bleh', 'blimey', 'bonjour', 'boo', 'booh', 'boohoo',\n",
    "                   'booyah', 'bravo', 'brr', 'brrrr', 'btw', 'bwahaha', 'capeesh',\n",
    "                   'capisce', 'cheerio', 'cheers', 'ciao', 'cor', 'cowabunga',\n",
    "                   'crikey', 'cripes', 'da', 'dabba', 'dah', 'dammit', 'damn', 'dang',\n",
    "                   'darn', 'de', 'dee', 'di', 'dizamn', 'doh', 'doo', 'drat', 'duh',\n",
    "                   'dum', 'eeeek', 'eek', 'eep', 'egad', 'egads', 'eh', 'ehem', 'em',\n",
    "                   'er', 'eureka', 'eww', 'ewww', 'eyh', 'fiddledeedee', 'fie',\n",
    "                   'fore', 'foul', 'fuff', 'gah', 'gak', 'gee', 'geez', 'gesundheit',\n",
    "                   'giddyap', 'golly', 'gosh', 'grr', 'grrrr', 'ha', 'hah', 'haha',\n",
    "                   'hahaha', 'hallelujah', 'halloa', 'harrumph', 'harumph', 'haw',\n",
    "                   'heck', 'heck', 'heeey', 'heh', 'hehe', 'hey', 'hhh', 'hic', 'hm',\n",
    "                   'hmm', 'hmmm', 'hmmmm', 'hmmph', 'hmpf', 'ho', 'hola', 'hoo',\n",
    "                   'hooray', 'howdy', 'hrmm', 'hrmph', 'hrmph', 'hrrmph', 'hu', 'huh',\n",
    "                   'hullo', 'humph', 'hurrah', 'huzza', 'huzzah', 'ich', 'ick',\n",
    "                   'ixnay', 'jeepers', 'jeez', 'kaboom', 'kapow', 'kerwham', 'la',\n",
    "                   'lala', 'lo', 'lordy', 'meh', 'mhm', 'ml', 'mm', 'mmh', 'mmhm',\n",
    "                   'mmm', 'muahaha', 'mwah', 'mwahaha', 'na','nay','nah', 'nanu', 'nooo', 'nope',\n",
    "                   'nuh', 'oh', 'ohh', 'oho', 'oi', 'okeydoke', 'om', 'oof', 'ooh',\n",
    "                   'oomph', 'oooh', 'ooooh', 'oops', 'ouch', 'ow', 'oww', 'oy',\n",
    "                   'oyez', 'oyh', 'pew', 'pff', 'pffh', 'pfft', 'phew', 'phut',\n",
    "                   'phweep', 'phwoar', 'phwoarr', 'poof', 'poogh', 'prethee',\n",
    "                   'prithee', 'prosit', 'pssh', 'psst', 'queep', 'roger', 'salaam',\n",
    "                   'salam', 'sheesh', 'shh', 'shhh', 'shitfire', 'shoo', 'shoop',\n",
    "                   'shush', 'sigh', 'sssh', 'strewth', 'ta', 'tarnations', 'tchah',\n",
    "                   'teehee', 'tish', 'touché', 'tsk', 'tss', 'tut', 'uggh', 'ugh',\n",
    "                   'uh', 'uhh', 'uhm', 'um', 'umm', 'ummm', 'umph', 'unh', 'upadaisy',\n",
    "                   'upsadaisy', 'ur', 'urgh', 'vay', 'vayf', 'viva', 'voila', 'waa',\n",
    "                   'waaaaah', 'waah', 'wah', 'wahey', 'wassup', 'weee', 'welp',\n",
    "                   'wham', 'whamo', 'whee', 'whew', 'whizz', 'whoa',\n",
    "                   'whoo', 'whoopee','whoop', 'whoops', 'whoopsy', 'whoosh', 'woah', 'woo',\n",
    "                   'woohoo', 'wotcha', 'wotcher', 'wow', 'wowsers', 'wowsers',\n",
    "                   'wuzzup', 'wuzzup', 'wuzzup', 'ya', 'yabba', 'yada', 'yadda',\n",
    "                   'yak', 'yarooh', 'yay', 'yea', 'yeah', 'yech', 'yee', 'yeeeeaah',\n",
    "                   'yeehaw', 'yeow', 'yes', 'yessiree', 'yew', 'yikes', 'yippee',\n",
    "                   'yo', 'yoo', 'yoohoo', 'yow', 'yowza', 'yuck', 'yuh', 'zing',\n",
    "                   'zoiks', 'zomfg', 'zomg', 'zounds', 'zut']\n",
    "             \n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "type(spacy_stopwords)\n",
    "#spacy_exclude=['using','name','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "#               'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use']# serious\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words1 = get_stop_words('english')\n",
    "#print(type(stop_words1))\n",
    "#print()\n",
    "#print(stop_words1)\n",
    "lib_stopwords=set(stop_words1)\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#print(type(ENGLISH_STOP_WORDS))\n",
    "#print()\n",
    "#print(set(ENGLISH_STOP_WORDS))\n",
    "#sklearn_exclude=['find','get','found','go','see','seem','seems','give','seemed','take','keep','show','put','made'] # system  cry\n",
    "sklearn_stopwords=set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "#spacy_stopwords.difference_update(set(spacy_exclude))\n",
    "#sklearn_stopwords.difference_update(set(sklearn_stopwords))\n",
    "#for removing \"just\" one item, use \"remove\"\n",
    "temp_1=set([])\n",
    "#######################\n",
    "temp_1.update(nltk_stopwords)\n",
    "#temp_1.update(lib_stopwords)\n",
    "temp_1.update(sklearn_stopwords)\n",
    "temp_1.update(spacy_stopwords)\n",
    "#temp_1.update(set(english_alghabet))\n",
    "#temp_1.update(set(numbers_remove)) \n",
    "#temp_1.update(set(miscellaneous_remove))\n",
    "#temp_1.update(set(interjection_remove))\n",
    "#temp_1.update(['rt','be','will','was','were','is','am','are','have','has','had','do','does','done','thing','want',\n",
    "#              'getting', 'looking','way'])\n",
    "#temp_1.update(['rt','like','look','get','take'])\n",
    "cachedStopWords=temp_1\n",
    "len(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "#print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "#print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "#print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)rt ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningA (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    #text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    #text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    #text=re.sub('Dollar|Dollars|Yen|Yens|Euros', ' money ', text)   # not euro \n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    text = re.sub(r'(HTTP://|HTTPS://)\\S+', '', text)\n",
    "\n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    text=re.sub(r'WWW\\.\\S+', '', text)\n",
    "\n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"United States\",'USA', text)\n",
    "    text=re.sub(\"United Kingdom\",'UK', text)\n",
    "    text=re.sub(\" the US \",' USA ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"U\\.S\\.A\", 'USA', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text)  \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)RT | RT ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_punc(text):\n",
    "    '''\n",
    "    text=re.sub(\"looking forward|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", ' ',text) #except  \\- _\n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    '''\n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    text= re.sub(\">\", ' ',text)\n",
    "    text= re.sub(\"<\", ' ',text)\n",
    "    text= re.sub(\" - \", ' ',text)\n",
    "    text= re.sub(\" --\", ' ',text)\n",
    "    '''\n",
    "    \n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47604376, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "Number of tweets in ent: 47604376\n",
      "Memory size of ent: 14706672014\n"
     ]
    }
   ],
   "source": [
    "#df_ent = pd.read_csv('/archives1/Datasets/TweetsWorld/ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_ent0 = pd.read_csv('ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_ent0.shape)\n",
    "print(df_ent0.columns)\n",
    "#print(df_ent0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_ent=list(df_ent0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_ent= list(zip(df_ent0.user_id, df_ent.tweet))\n",
    "#rows_ent=df_ent0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_ent = df_ent0[['tweet','tweet_created_at']]   \n",
    "\n",
    "del df_ent0\n",
    "\n",
    "print(\"Number of tweets in ent:\",len(df_ent))  #rows_ent0\n",
    "print('Memory size of ent:',sys.getsizeof(df_ent)) #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_mng0 = pd.read_csv('mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#df_mng0 = pd.read_csv('/archives1/Datasets/TweetsWorld/mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "print(df_mng0.shape)\n",
    "print(df_mng0.columns)\n",
    "#print(df_mng0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_mng=list(df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_mng0\n",
    "#rows_mng= list(zip(df_mng0.user_id, df_mng.tweet))\n",
    "#rows_mng=df_mng0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_mng = df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']]\n",
    "\n",
    "del df_mng0\n",
    "\n",
    "print(len(df_mng)) #rows_mng0\n",
    "print('memry size of mng:', sys.getsizeof(df_mng)) #rows_mng0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @MailOnline: New evidence of China's coronavirus cover-up https://t.co/jbGewDBlLC\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ent['tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/ent_cleaning_classification.txt\", \"rb\") as fp:  \n",
    "#    ent_tweets_rows=pickle.load(fp)\n",
    "#len(ent_tweets_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', \"a 'young' fan\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases1('''I'm a 'young' fan. ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'm', \"a ' young fan\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases1('''I ' m a ' young fan '. ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['young fan']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases('''I ' m a ' young fan '. ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"a 'young\", 'fan']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases('''I'm a 'young' fan. ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind_2019=pickle.load(fp)\n",
    "\n",
    "len(ent_year_ind_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind_2017=pickle.load(fp)\n",
    "\n",
    "len(ent_year_ind_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"result_ent_vote.txt\", \"rb\") as fp:  \n",
    "    vote_19=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_ent_label.txt\", \"rb\") as fp:  \n",
    "    label_19=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_2019_rel=[]\n",
    "ent_2019_irrel=[]\n",
    "for i , k in enumerate(ent_year_ind_2019):\n",
    "    if '2019'  not in df_ent['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_19[i]==1:\n",
    "        ent_2019_rel.append(k)\n",
    "    else:\n",
    "        ent_2019_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892698, 1107302)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_2019_rel), len(ent_2019_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"result_ent_vote.txt\", \"rb\") as fp:  \n",
    "    vote_17=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_ent_label.txt\", \"rb\") as fp:  \n",
    "    label_17=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_2017_rel=[]\n",
    "ent_2017_irrel=[]\n",
    "for i , k in enumerate(ent_year_ind_2017):\n",
    "    if '2017' not in df_ent['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_17[i]==1:\n",
    "        ent_2017_rel.append(k)\n",
    "    else:\n",
    "        ent_2017_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063213, 936787)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_2017_rel), len(ent_2017_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ent['tweet_created_at'][ent_2017_rel[0]], df_ent['tweet'][ent_2017_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_phrases(text):\n",
    "    # noun phrase by nltk\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "    grammar = \"\"\"NP: {<DT>?<JJ>*<NN.*>+}\"\"\"\n",
    "\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    #for i in sentences:\n",
    "    #    result = cp.parse(i)\n",
    "    #    print(result)\n",
    "    #    print(type(result))\n",
    "        #result.draw() \n",
    "   # noun_phrases_list = [[' '.join(leaf[0].lower() for leaf in tree.leaves()) \n",
    "\n",
    "    noun_phrases_list = [[' '.join(leaf[0] for leaf in tree.leaves()) \n",
    "                          for tree in cp.parse(sent).subtrees() \n",
    "                          if tree.label()=='NP'] \n",
    "                          for sent in sentences]\n",
    "\n",
    "\n",
    "    phrases_all=[]\n",
    "    for i in noun_phrases_list:\n",
    "        phrases_all.extend(i)\n",
    "    \n",
    "    return phrases_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_phrases1(text):\n",
    "    # noun phrase by Spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    phrases_all=[]\n",
    "    for phrase in doc.noun_chunks:\n",
    "        phrases_all.append(phrase.text)\n",
    "        #phrases_all.append(phrase.text.lower())\n",
    "\n",
    "    return phrases_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' i', 'the book']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases1(' i like the book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./ent_results300_np.txt\", \"rb\") as fp:   \n",
    "    ent_results = pickle.load(fp)\n",
    "\n",
    "num_ent_positive=0\n",
    "for i in ent_results:\n",
    "    if i>=0.5:\n",
    "        num_ent_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to ent:\",num_ent_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./mng_results300.txt\", \"rb\") as fp:   \n",
    "    mng_results = pickle.load(fp)\n",
    "\n",
    "num_mng_positive=0\n",
    "for i in mng_results:\n",
    "    if i>=0.5:\n",
    "        num_mng_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to mng:\", num_mng_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequnent hashtags in ENT (with thier frequencies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "ent_hash_2017=[]\n",
    "for i,value in enumerate(ent_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2017.extend(hashtags)\n",
    "        \n",
    "ent_Hashtags=dict()\n",
    "\n",
    "for ph in ent_hash:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags:\n",
    "            ent_Hashtags[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags = sorted(ent_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.80774474143982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('entrepreneur', 14229),\n",
       " ('business', 10223),\n",
       " ('marketing', 9958),\n",
       " ('startup', 9839),\n",
       " ('quote', 8466),\n",
       " ('startups', 7271),\n",
       " ('entrepreneurship', 6658),\n",
       " ('leadership', 6587),\n",
       " ('ai', 5884),\n",
       " ('entrepreneurs', 5361),\n",
       " ('socialmedia', 5049),\n",
       " ('success', 4955),\n",
       " ('tech', 4884),\n",
       " ('smallbiz', 4344),\n",
       " ('innovation', 3697),\n",
       " ('fintech', 3605),\n",
       " ('mca', 3478),\n",
       " ('seo', 3222),\n",
       " ('growthhacking', 3112),\n",
       " ('bitcoin', 3084),\n",
       " ('digitalmarketing', 2862),\n",
       " ('blockchain', 2835),\n",
       " ('iot', 2313),\n",
       " ('technology', 2246),\n",
       " ('ecommerce', 2189),\n",
       " ('bigdata', 2127),\n",
       " ('motivation', 2001),\n",
       " ('travel', 1965),\n",
       " ('contentmarketing', 1923),\n",
       " ('machinelearning', 1902),\n",
       " ('sales', 1735),\n",
       " ('productivity', 1714),\n",
       " ('crowdfunding', 1689),\n",
       " ('smallbusiness', 1642),\n",
       " ('cybersecurity', 1581),\n",
       " ('smm', 1563),\n",
       " ('shoutout', 1525),\n",
       " ('africa', 1508),\n",
       " ('inspiration', 1452),\n",
       " ('health', 1440),\n",
       " ('hr', 1350),\n",
       " ('mondaymotivation', 1307),\n",
       " ('socent', 1271),\n",
       " ('education', 1234),\n",
       " ('india', 1223),\n",
       " ('news', 1208),\n",
       " ('cryptocurrency', 1184),\n",
       " ('1', 1168),\n",
       " ('women', 1116),\n",
       " ('life', 1104),\n",
       " ('growth', 1097),\n",
       " ('podcast', 1092),\n",
       " ('digital', 1079),\n",
       " ('google', 1071),\n",
       " ('theshrimptank', 1071),\n",
       " ('strategy', 1004),\n",
       " ('data', 995),\n",
       " ('datascience', 990),\n",
       " ('branding', 975),\n",
       " ('artificialintelligence', 972),\n",
       " ('blogging', 969),\n",
       " ('fundicaroadshow', 968),\n",
       " ('healthcare', 956),\n",
       " ('funding', 952),\n",
       " ('quotes', 929),\n",
       " ('coworking', 925),\n",
       " ('content', 918),\n",
       " ('goals', 910),\n",
       " ('ceo', 909),\n",
       " ('deeplearning', 905),\n",
       " ('money', 891),\n",
       " ('free', 886),\n",
       " ('design', 881),\n",
       " ('facebook', 878),\n",
       " ('energy', 871),\n",
       " ('mobile', 854),\n",
       " ('realestate', 851),\n",
       " ('networking', 850),\n",
       " ('vr', 849),\n",
       " ('vc', 838),\n",
       " ('finance', 835),\n",
       " ('businessowner', 825),\n",
       " ('retail', 779),\n",
       " ('wordpress', 775),\n",
       " ('wednesdaywisdom', 772),\n",
       " ('insurtech', 770),\n",
       " ('startupindia', 757),\n",
       " ('investing', 754),\n",
       " ('brand', 751),\n",
       " ('cloud', 744),\n",
       " ('music', 739),\n",
       " ('trump', 738),\n",
       " ('b2b', 737),\n",
       " ('analytics', 726),\n",
       " ('edtech', 719),\n",
       " ('food', 711),\n",
       " ('quoteoftheday', 706),\n",
       " ('security', 693),\n",
       " ('invest', 692),\n",
       " ('govcon', 678)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ent_hash_2017=[]\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2017.extend(hashtags)\n",
    "        \n",
    "ent_Hashtags_2017=dict()\n",
    "\n",
    "for ph in ent_hash_2017:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags_2017:\n",
    "            ent_Hashtags_2017[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags_2017[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags_2017 = sorted(ent_Hashtags_2017.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "ent_Hashtags_2017[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_ent=pd.DataFrame(ent_Hashtags_2017)\n",
    "df_hash_ent.columns =['Hashtags in ent','frequency']\n",
    "df_hash_ent=df_hash_ent.head(100)\n",
    "df_hash_ent.to_csv('hashtags_ent.csv')\n",
    "df_hash_ent_styled = df_hash_ent.style \n",
    "dfi.export(df_hash_ent_styled,\"hashtags_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.47633528709412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('entrepreneur', 8474),\n",
       " ('business', 8091),\n",
       " ('startup', 5494),\n",
       " ('marketing', 5249),\n",
       " ('entrepreneurship', 4524),\n",
       " ('startups', 4263),\n",
       " ('success', 3745),\n",
       " ('leadership', 3622),\n",
       " ('entrepreneurs', 3557),\n",
       " ('ai', 3526),\n",
       " ('digitalmarketing', 3012),\n",
       " ('socialmedia', 2903),\n",
       " ('tech', 2553),\n",
       " ('smallbiz', 2498),\n",
       " ('innovation', 2462),\n",
       " ('smallbusiness', 2439),\n",
       " ('motivation', 2305),\n",
       " ('quote', 2224),\n",
       " ('seo', 1819),\n",
       " ('technology', 1788),\n",
       " ('blockchain', 1723),\n",
       " ('inspiration', 1518),\n",
       " ('realestate', 1509),\n",
       " ('sales', 1442),\n",
       " ('contentmarketing', 1405),\n",
       " ('shopmycloset', 1353),\n",
       " ('bitcoin', 1340),\n",
       " ('fashion', 1321),\n",
       " ('branding', 1315),\n",
       " ('ecommerce', 1277),\n",
       " ('healthcare', 1262),\n",
       " ('podcast', 1257),\n",
       " ('iot', 1252),\n",
       " ('fintech', 1245),\n",
       " ('mondaymotivation', 1167),\n",
       " ('cybersecurity', 1157),\n",
       " ('health', 1121),\n",
       " ('artificialintelligence', 1070),\n",
       " ('crypto', 1034),\n",
       " ('machinelearning', 1019),\n",
       " ('love', 989),\n",
       " ('digital', 891),\n",
       " ('smm', 885),\n",
       " ('socialmediamarketing', 879),\n",
       " ('entrepreneurlife', 877),\n",
       " ('1', 856),\n",
       " ('travel', 851),\n",
       " ('goals', 840),\n",
       " ('rt', 839),\n",
       " ('mindset', 833),\n",
       " ('india', 830),\n",
       " ('life', 827),\n",
       " ('bigdata', 823),\n",
       " ('education', 815),\n",
       " ('instagram', 805),\n",
       " ('ceo', 788),\n",
       " ('design', 785),\n",
       " ('money', 777),\n",
       " ('blogging', 772),\n",
       " ('quotes', 769),\n",
       " ('google', 759),\n",
       " ('repost', 743),\n",
       " ('cryptocurrency', 741),\n",
       " ('news', 735),\n",
       " ('fitness', 732),\n",
       " ('data', 719),\n",
       " ('art', 709),\n",
       " ('brexit', 709),\n",
       " ('management', 701),\n",
       " ('productivity', 694),\n",
       " ('growthhacking', 670),\n",
       " ('businessowner', 666),\n",
       " ('makemoneyonline', 655),\n",
       " ('networking', 629),\n",
       " ('facebook', 629),\n",
       " ('africa', 623),\n",
       " ('leaders', 616),\n",
       " ('datascience', 605),\n",
       " ('investing', 596),\n",
       " ('brand', 592),\n",
       " ('edtech', 590),\n",
       " ('funding', 588),\n",
       " ('digitaltransformation', 587),\n",
       " ('coaching', 583),\n",
       " ('jobs', 579),\n",
       " ('women', 577),\n",
       " ('nigeria', 577),\n",
       " ('climatechange', 575),\n",
       " ('strategy', 571),\n",
       " ('socent', 564),\n",
       " ('photography', 563),\n",
       " ('smb', 560),\n",
       " ('wednesdaywisdom', 553),\n",
       " ('earnmoney', 550),\n",
       " ('investment', 549),\n",
       " ('science', 546),\n",
       " ('hustle', 541),\n",
       " ('video', 539),\n",
       " ('style', 537),\n",
       " ('retail', 536)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ent_hash_2019=[]\n",
    "for i,value in enumerate(ent_year_ind_2019):\n",
    "    if label_19[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2019.extend(hashtags)\n",
    "        \n",
    "ent_Hashtags_2019=dict()\n",
    "\n",
    "for ph in ent_hash_2019:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags_2019:\n",
    "            ent_Hashtags_2019[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags_2019[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags_2019 = sorted(ent_Hashtags_2019.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "ent_Hashtags_2019[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequent hashtags in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "mng_hash=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "\n",
    "        mng_hash.extend(hashtags)\n",
    "    \n",
    "mng_Hashtags=dict()\n",
    "\n",
    "for ph in mng_hash:\n",
    "    if ph != '':\n",
    "        if ph not in mng_Hashtags:\n",
    "            mng_Hashtags[ph]=1\n",
    "        else:\n",
    "            mng_Hashtags[ph]+=1  \n",
    "\n",
    "mng_Hashtags = sorted(mng_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_Hashtags[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_mng=pd.DataFrame(mng_Hashtags)\n",
    "df_hash_mng.columns =['Hashtags in mng','frequency']\n",
    "df_hash_mng=df_hash_mng.head(100)\n",
    "\n",
    "df_hash_mng.to_csv('hashtags_mng.csv')\n",
    "\n",
    "df_hash_mng_styled = df_hash_mng.style \n",
    "dfi.export(df_hash_mng_styled,\"hashtags_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "ent_spacy=[]\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        ent_spacy.extend(concepts)\n",
    "    \n",
    "\n",
    "ent_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in ent_spacy:\n",
    "    if ph not in ent_noun_phrase_spacy:\n",
    "        ent_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        ent_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "ent_noun_phrase_spacy = sorted(ent_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "mng_spacy=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "        \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        mng_spacy.extend(concepts)\n",
    "    \n",
    "    \n",
    "mng_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in mng_spacy:\n",
    "    if ph not in mng_noun_phrase_spacy:\n",
    "        mng_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        mng_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "mng_noun_phrase_spacy = sorted(mng_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7569.7924880981445\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# finding concepts by nltk\n",
    "#ent_2017_nltk=[]\n",
    "\n",
    "ent_noun_phrase_2017_spacy=dict()\n",
    "\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        #text=re.sub('#', ' ', text)\n",
    "        concepts=noun_phrases1(text)\n",
    "    \n",
    "        for concept in concepts:\n",
    "            if concept not in cachedStopWords:\n",
    "                #concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                concept_= re.sub(\"[#\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                concept_=concept_.lower()\n",
    "                #print(concept_)\n",
    "                concept_parts=concept_.split()\n",
    "                temp=[]\n",
    "                for part in concept_parts:\n",
    "                    lemm_part=lemm(part)\n",
    "                    if lemm_part not in cachedStopWords:\n",
    "                        temp.append(lemm_part)\n",
    "                concept_1=' '.join(temp)        \n",
    "                    \n",
    "                if concept_1 !='':\n",
    "                    \n",
    "                    if concept_1 not in ent_noun_phrase_2017_spacy:\n",
    "                        ent_noun_phrase_2017_spacy[concept_1]=1\n",
    "                    else:\n",
    "                        ent_noun_phrase_2017_spacy[concept_1]+=1\n",
    "\n",
    "\n",
    "ent_noun_phrase_2017_spacy = sorted(ent_noun_phrase_2017_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7626.816551208496\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# finding concepts by nltk\n",
    "#ent_2017_nltk=[]\n",
    "\n",
    "ent_noun_phrase_2017_spa=dict()\n",
    "\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        #text=re.sub('#', ' ', text)\n",
    "        concepts_x=noun_phrases1(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                if concept !='':\n",
    "                    if concept not in ent_noun_phrase_2017_spa:\n",
    "                        ent_noun_phrase_2017_spa[concept]=1\n",
    "                    else:\n",
    "                        ent_noun_phrase_2017_spa[concept]+=1\n",
    "                \n",
    "                #concepts.append(concept)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        #ent_2017_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in ent_2017_nltk:\n",
    "    if word !='':\n",
    "        if word not in ent_noun_phrase_2017_nltk:\n",
    "            ent_noun_phrase_2017_nltk[word]=1\n",
    "        else:\n",
    "            ent_noun_phrase_2017_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "ent_noun_phrase_2017_spa = sorted(ent_noun_phrase_2017_spa.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('news', 1890)\n",
      "('new video', 1788)\n",
      "('new photo', 927)\n",
      "('new york', 693)\n",
      "('new year', 684)\n",
      "('good news', 660)\n",
      "('new post', 604)\n",
      "('fake news', 546)\n",
      "('#news', 543)\n",
      "('top new followers', 543)\n",
      "('great news', 483)\n",
      "('happy new year', 464)\n",
      "('small business news', 379)\n",
      "('100 renewables', 370)\n"
     ]
    }
   ],
   "source": [
    "for i in ent_noun_phrase_2017_spa[0:1000]:\n",
    "    if 'new' in i[0]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in ENT found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2734.256057739258\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# finding concepts by nltk\n",
    "#ent_2017_nltk=[]\n",
    "\n",
    "ent_noun_phrase_2017_nltk=dict()\n",
    "\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        #text=re.sub('#', ' ', text)\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                if concept !='':\n",
    "                    if concept not in ent_noun_phrase_2017_nltk:\n",
    "                        ent_noun_phrase_2017_nltk[concept]=1\n",
    "                    else:\n",
    "                        ent_noun_phrase_2017_nltk[concept]+=1\n",
    "                \n",
    "                #concepts.append(concept)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        #ent_2017_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in ent_2017_nltk:\n",
    "    if word !='':\n",
    "        if word not in ent_noun_phrase_2017_nltk:\n",
    "            ent_noun_phrase_2017_nltk[word]=1\n",
    "        else:\n",
    "            ent_noun_phrase_2017_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "ent_noun_phrase_2017_nltk = sorted(ent_noun_phrase_2017_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thanks', 26793),\n",
       " ('business', 24692),\n",
       " ('people', 20447),\n",
       " ('today', 16662),\n",
       " ('entrepreneur', 14971),\n",
       " ('success', 12938),\n",
       " ('time', 12776),\n",
       " ('world', 12327),\n",
       " ('life', 10759),\n",
       " ('thank', 10172),\n",
       " ('marketing', 9946),\n",
       " ('work', 9582),\n",
       " ('https', 9409),\n",
       " ('startups', 9077),\n",
       " ('money', 9057),\n",
       " ('way', 8516),\n",
       " ('leadership', 8394),\n",
       " ('day', 8279),\n",
       " ('entrepreneurs', 8175),\n",
       " ('startup', 7793),\n",
       " ('follow', 7565),\n",
       " ('women', 7508),\n",
       " ('quote', 7316),\n",
       " ('years', 7202),\n",
       " ('future', 6988),\n",
       " ('india', 6736),\n",
       " ('share', 6649),\n",
       " ('ai', 6566),\n",
       " ('team', 6536),\n",
       " ('power', 6252),\n",
       " ('video', 6242),\n",
       " ('entrepreneurship', 6136),\n",
       " ('things', 6045),\n",
       " ('join', 5908),\n",
       " ('trump', 5795),\n",
       " ('innovation', 5598),\n",
       " ('tips', 4929),\n",
       " ('learn', 4862),\n",
       " ('bitcoin', 4723),\n",
       " ('year', 4686),\n",
       " ('ways', 4669),\n",
       " ('this week', 4429),\n",
       " ('company', 4378),\n",
       " ('support', 4279),\n",
       " ('days', 4270),\n",
       " ('tech', 4253),\n",
       " ('usa', 4231),\n",
       " ('twitter', 4048),\n",
       " ('technology', 4028),\n",
       " ('nigeria', 3970),\n",
       " ('africa', 3942),\n",
       " ('country', 3878),\n",
       " ('news', 3833),\n",
       " ('help', 3801),\n",
       " ('goals', 3794),\n",
       " ('lot', 3781),\n",
       " ('tomorrow', 3763),\n",
       " ('book', 3754),\n",
       " ('congratulations', 3680),\n",
       " ('ceo', 3655),\n",
       " ('job', 3617),\n",
       " ('sales', 3606),\n",
       " ('congrats', 3601),\n",
       " ('link', 3575),\n",
       " ('facebook', 3519),\n",
       " ('social media', 3444),\n",
       " ('week', 3418),\n",
       " ('brand', 3380),\n",
       " ('great', 3336),\n",
       " ('president', 3320),\n",
       " ('google', 3259),\n",
       " ('blockchain', 3234),\n",
       " ('data', 3221),\n",
       " ('education', 3197),\n",
       " ('students', 3186),\n",
       " ('opportunity', 3159),\n",
       " ('customers', 3128),\n",
       " ('growth', 3111),\n",
       " ('value', 3102),\n",
       " ('community', 3088),\n",
       " ('change', 3073),\n",
       " ('ideas', 2970),\n",
       " ('pisces', 2956),\n",
       " ('jobs', 2941),\n",
       " ('blog', 2939),\n",
       " ('mca', 2905),\n",
       " ('leaders', 2902),\n",
       " ('socialmedia', 2858),\n",
       " ('seo', 2851),\n",
       " ('this year', 2823),\n",
       " ('focus', 2793),\n",
       " ('home', 2752),\n",
       " ('health', 2735),\n",
       " ('story', 2720),\n",
       " ('problem', 2715),\n",
       " ('mind', 2695),\n",
       " ('event', 2674),\n",
       " ('new', 2673),\n",
       " ('companies', 2669),\n",
       " ('action', 2651),\n",
       " ('place', 2643),\n",
       " ('friends', 2597),\n",
       " ('look', 2574),\n",
       " ('state', 2571),\n",
       " ('smallbiz', 2558),\n",
       " ('employees', 2557),\n",
       " ('hi', 2526),\n",
       " ('followers', 2522),\n",
       " ('china', 2506),\n",
       " ('inspiration', 2490),\n",
       " ('amazon', 2480),\n",
       " ('motivation', 2474),\n",
       " ('uk', 2466),\n",
       " ('leader', 2446),\n",
       " ('want', 2437),\n",
       " ('list', 2404),\n",
       " ('market', 2388),\n",
       " ('app', 2362),\n",
       " ('hey', 2346),\n",
       " ('leo', 2332),\n",
       " ('impact', 2327),\n",
       " ('website', 2325),\n",
       " ('failure', 2309),\n",
       " ('experience', 2297),\n",
       " ('businesses', 2269),\n",
       " ('travel', 2245),\n",
       " ('vision', 2243),\n",
       " ('need', 2229),\n",
       " ('sagittarius', 2225),\n",
       " ('productivity', 2216),\n",
       " ('america', 2183),\n",
       " ('iot', 2181),\n",
       " ('party', 2160),\n",
       " ('details', 2141),\n",
       " ('idea', 2141),\n",
       " ('come', 2134),\n",
       " ('steps', 2122),\n",
       " ('fintech', 2121),\n",
       " ('start', 2116),\n",
       " ('government', 2108),\n",
       " ('chance', 2100),\n",
       " ('cancer', 2091),\n",
       " ('media', 2089),\n",
       " ('order', 2089),\n",
       " ('u', 2089),\n",
       " ('thx', 2068),\n",
       " ('person', 2048),\n",
       " ('truth', 2040),\n",
       " ('recent follow happy', 2039),\n",
       " ('difference', 2033),\n",
       " ('founder', 2028),\n",
       " ('books', 2023),\n",
       " ('sign', 2010),\n",
       " ('gemini', 2010),\n",
       " ('children', 2007),\n",
       " ('http', 1999),\n",
       " ('energy', 1997),\n",
       " ('meet', 1996),\n",
       " ('thoughts', 1994),\n",
       " ('strategy', 1987),\n",
       " ('reasons', 1984),\n",
       " ('tonight', 1958),\n",
       " ('scorpio', 1956),\n",
       " ('art', 1952),\n",
       " ('goal', 1948),\n",
       " ('office', 1947),\n",
       " ('read', 1945),\n",
       " ('product', 1943),\n",
       " ('question', 1924),\n",
       " ('info', 1914),\n",
       " ('passion', 1912),\n",
       " ('virgo', 1902),\n",
       " ('capricorn', 1895),\n",
       " ('register', 1894),\n",
       " ('results', 1890),\n",
       " ('progress', 1876),\n",
       " ('internet', 1861),\n",
       " ('investors', 1859),\n",
       " ('content', 1854),\n",
       " ('this morning', 1851),\n",
       " ('london', 1845),\n",
       " ('law', 1831),\n",
       " ('clients', 1830),\n",
       " ('number', 1828),\n",
       " ('questions', 1821),\n",
       " ('hope', 1819),\n",
       " ('network', 1818),\n",
       " ('bigdata', 1812),\n",
       " ('new video', 1809),\n",
       " ('use', 1798),\n",
       " ('months', 1792),\n",
       " ('thing', 1784),\n",
       " ('history', 1781),\n",
       " ('service', 1780),\n",
       " ('design', 1779),\n",
       " ('love', 1774),\n",
       " ('food', 1763),\n",
       " ('family', 1760),\n",
       " ('podcast', 1748),\n",
       " ('wow', 1742),\n",
       " ('advice', 1737),\n",
       " ('city', 1728),\n",
       " ('kenya', 1724),\n",
       " ('shoutout', 1722),\n",
       " ('dreams', 1716),\n",
       " ('last week', 1711),\n",
       " ('times', 1705),\n",
       " ('purpose', 1705),\n",
       " ('solution', 1705),\n",
       " ('journey', 1700),\n",
       " ('access', 1698),\n",
       " ('science', 1697),\n",
       " ('hours', 1694),\n",
       " ('investment', 1693),\n",
       " ('march', 1682),\n",
       " ('email', 1674),\n",
       " ('lives', 1669),\n",
       " ('nation', 1661),\n",
       " ('fear', 1652),\n",
       " ('libra', 1652),\n",
       " ('plan', 1649),\n",
       " ('peace', 1646),\n",
       " ('end', 1644),\n",
       " ('article', 1644),\n",
       " ('opportunities', 1644),\n",
       " ('economy', 1635),\n",
       " ('site', 1632),\n",
       " ('congress', 1632),\n",
       " ('pm', 1630),\n",
       " ('culture', 1619),\n",
       " ('click', 1610),\n",
       " ('interview', 1607),\n",
       " ('ecommerce', 1603),\n",
       " ('knowledge', 1602),\n",
       " ('june', 1601),\n",
       " ('lots', 1597),\n",
       " ('proud', 1594),\n",
       " ('htt', 1592),\n",
       " ('course', 1587),\n",
       " ('stay', 1586),\n",
       " ('funding', 1583),\n",
       " ('visit', 1582),\n",
       " ('canada', 1576),\n",
       " ('word', 1572),\n",
       " ('point', 1561),\n",
       " ('research', 1558),\n",
       " ('events', 1551),\n",
       " ('sale', 1547),\n",
       " ('aries', 1528),\n",
       " ('minutes', 1526),\n",
       " ('key', 1523),\n",
       " ('talk', 1515),\n",
       " ('s time', 1513),\n",
       " ('diversity', 1513),\n",
       " ('faith', 1510),\n",
       " ('trust', 1497),\n",
       " ('lessons', 1490),\n",
       " ('freedom', 1489),\n",
       " ('problems', 1484),\n",
       " ('process', 1483),\n",
       " ('price', 1472),\n",
       " ('cybersecurity', 1456),\n",
       " ('vote', 1451),\n",
       " ('case', 1451),\n",
       " ('month', 1446),\n",
       " ('ht', 1439),\n",
       " ('no one', 1433),\n",
       " ('h', 1427),\n",
       " ('pakistan', 1421),\n",
       " ('apple', 1419),\n",
       " ('skills', 1418),\n",
       " ('space', 1417),\n",
       " ('answer', 1408),\n",
       " ('importance', 1399),\n",
       " ('tickets', 1391),\n",
       " ('hr', 1390),\n",
       " ('youth', 1389),\n",
       " ('politics', 1386),\n",
       " ('industry', 1386),\n",
       " ('healthcare', 1384),\n",
       " ('friend', 1371),\n",
       " ('career', 1370),\n",
       " ('yesterday', 1367),\n",
       " ('attention', 1353),\n",
       " ('btc', 1352),\n",
       " ('july', 1350),\n",
       " ('cryptocurrency', 1350),\n",
       " ('security', 1347),\n",
       " ('instagram', 1346),\n",
       " ('step', 1346),\n",
       " ('words', 1345),\n",
       " ('reality', 1337),\n",
       " ('launch', 1330),\n",
       " ('smm', 1330),\n",
       " ('talent', 1322),\n",
       " ('benefits', 1314),\n",
       " ('new followers', 1311),\n",
       " ('man', 1308),\n",
       " ('creativity', 1303),\n",
       " ('stories', 1301)]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_noun_phrase_2017_nltk[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i,value in enumerate(ent_year_ind_2017[0:1000000]):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        #text= re.sub(\"#\", '',text0)\n",
    "        if 'Great ' in text:\n",
    "            if 'Great' in noun_phrases(text):\n",
    "                \n",
    "                print(text)\n",
    "                print(noun_phrases(text))\n",
    "                print('-----')\n",
    "                print(noun_phrases1(text))\n",
    "                print()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813.1019616127014\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# finding concepts by nltk\n",
    "#ent_2017_nltk=[]\n",
    "\n",
    "ent_noun_phrase_2017_nltk_non=dict()\n",
    "\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==0:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        #text=re.sub('#', ' ', text)\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                \n",
    "                if concept !='':\n",
    "                    if concept not in ent_noun_phrase_2017_nltk_non:\n",
    "                        ent_noun_phrase_2017_nltk_non[concept]=1\n",
    "                    else:\n",
    "                        ent_noun_phrase_2017_nltk_non[concept]+=1\n",
    "                \n",
    "                #concepts.append(concept)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        #ent_2017_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in ent_2017_nltk:\n",
    "    if word !='':\n",
    "        if word not in ent_noun_phrase_2017_nltk:\n",
    "            ent_noun_phrase_2017_nltk[word]=1\n",
    "        else:\n",
    "            ent_noun_phrase_2017_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "ent_noun_phrase_2017_nltk_non = sorted(ent_noun_phrase_2017_nltk_non.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_noun_phrase_2017_nltk_non[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai',\n",
       " 'bitcoin',\n",
       " 'business',\n",
       " 'company',\n",
       " 'entrepreneur',\n",
       " 'entrepreneurs',\n",
       " 'entrepreneurship',\n",
       " 'follow',\n",
       " 'future',\n",
       " 'india',\n",
       " 'innovation',\n",
       " 'join',\n",
       " 'leadership',\n",
       " 'learn',\n",
       " 'marketing',\n",
       " 'nigeria',\n",
       " 'power',\n",
       " 'share',\n",
       " 'startup',\n",
       " 'startups',\n",
       " 'success',\n",
       " 'support',\n",
       " 'team',\n",
       " 'tech',\n",
       " 'technology',\n",
       " 'this week',\n",
       " 'tips',\n",
       " 'usa',\n",
       " 'ways'}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_17=[]\n",
    "for i in ent_noun_phrase_2017_nltk[0:50]:\n",
    "    yes_17.append(i[0])\n",
    "\n",
    "non_17=[]\n",
    "for i in ent_noun_phrase_2017_nltk_non[0:50]:\n",
    "    non_17.append(i[0])\n",
    "A\n",
    "aaa=set(yes_17)-set(non_17)\n",
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())\n",
    "\n",
    "aaa=list(set(yes)-set(non))\n",
    "aaa=list(set(yes))\n",
    "concept_vec=np.zeros((len(aaa),300))\n",
    "\n",
    "for i,concept in enumerate(aaa):\n",
    "    concept_vec[i]=ft.get_word_vector(concept)\n",
    "\n",
    "\n",
    "np.shape(concept_vec)\n",
    "\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustering_dbscan = DBSCAN(eps=5, min_samples=2).fit(concept_vec)\n",
    "print(clustering_dbscan.labels_)\n",
    "\n",
    "clustering_optics = OPTICS(min_samples=3).fit(concept_vec)\n",
    "print(clustering_optics.labels_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2) \n",
    "ent_kmeans_clustering = kmeans.fit(concept_vec)\n",
    "print(ent_kmeans_clustering.labels_)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_ent=pd.DataFrame(ent_noun_phrase_nltk)\n",
    "df_ph_ent.columns =['noun_phrase in ent by NLTK','frequency']\n",
    "df_ph_ent=df_ph_ent.head(100)\n",
    "df_ph_ent.to_csv('noun_phrase_ent.csv')\n",
    "df_ph_ent_styled = df_ph_ent.style \n",
    "dfi.export(df_ph_ent_styled,\"noun_phrase_ent_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2047.6628410816193\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# finding concepts by nltk\n",
    "#ent_2019_nltk=[]\n",
    "\n",
    "ent_noun_phrase_2019_nltk=dict()\n",
    "\n",
    "for i,value in enumerate(ent_year_ind_2019):\n",
    "    if label_19[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        #text=re.sub('#', ' ', text)\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                if concept !='':\n",
    "                    if concept not in ent_noun_phrase_2019_nltk:\n",
    "                        ent_noun_phrase_2019_nltk[concept]=1\n",
    "                    else:\n",
    "                        ent_noun_phrase_2019_nltk[concept]+=1\n",
    "                \n",
    "                #concepts.append(concept)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        #ent_2019_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in ent_2019_nltk:\n",
    "    if word !='':\n",
    "        if word not in ent_noun_phrase_2019_nltk:\n",
    "            ent_noun_phrase_2019_nltk[word]=1\n",
    "        else:\n",
    "            ent_noun_phrase_2019_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "ent_noun_phrase_2019_nltk = sorted(ent_noun_phrase_2019_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1927.2420752048492\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# finding concepts by nltk\n",
    "#ent_2019_nltk=[]\n",
    "\n",
    "ent_noun_phrase_2019_nltk_non=dict()\n",
    "\n",
    "for i,value in enumerate(ent_year_ind_2019):\n",
    "    if label_19[i]==0:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        #text=re.sub('#', ' ', text)\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                \n",
    "                if concept !='':\n",
    "                    if concept not in ent_noun_phrase_2019_nltk_non:\n",
    "                        ent_noun_phrase_2019_nltk_non[concept]=1\n",
    "                    else:\n",
    "                        ent_noun_phrase_2019_nltk_non[concept]+=1\n",
    "                \n",
    "                #concepts.append(concept)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        #ent_2019_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in ent_2019_nltk:\n",
    "    if word !='':\n",
    "        if word not in ent_noun_phrase_2019_nltk:\n",
    "            ent_noun_phrase_2019_nltk[word]=1\n",
    "        else:\n",
    "            ent_noun_phrase_2019_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "ent_noun_phrase_2019_nltk_non = sorted(ent_noun_phrase_2019_nltk_non.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai',\n",
       " 'book',\n",
       " 'business',\n",
       " 'company',\n",
       " 'country',\n",
       " 'entrepreneur',\n",
       " 'entrepreneurs',\n",
       " 'entrepreneurship',\n",
       " 'future',\n",
       " 'government',\n",
       " 'india',\n",
       " 'innovation',\n",
       " 'job',\n",
       " 'join',\n",
       " 'leadership',\n",
       " 'learn',\n",
       " 'marketing',\n",
       " 'news',\n",
       " 'nigeria',\n",
       " 'power',\n",
       " 'president',\n",
       " 'share',\n",
       " 'social media',\n",
       " 'startup',\n",
       " 'startups',\n",
       " 'success',\n",
       " 'support',\n",
       " 'team',\n",
       " 'technology',\n",
       " 'usa'}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_19=[]\n",
    "for i in ent_noun_phrase_2019_nltk[0:50]:\n",
    "    yes_19.append(i[0])\n",
    "\n",
    "non_19=[]\n",
    "for i in ent_noun_phrase_2019_nltk_non[0:50]:\n",
    "    non_19.append(i[0])\n",
    "\n",
    "bbb=set(yes_19)-set(non_19)\n",
    "bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in MNG found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "#finding concepts by nltk \n",
    "mng_nltk=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                #concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                #concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                concepts.append(concept)\n",
    "        \n",
    "#        print(concepts)       \n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "        mng_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "mng_noun_phrase_nltk=dict()\n",
    "\n",
    "for word in mng_nltk:\n",
    "    if word !='':\n",
    "        if word not in mng_noun_phrase_nltk:\n",
    "            mng_noun_phrase_nltk[word]=1\n",
    "        else:\n",
    "            mng_noun_phrase_nltk[word]+=1   \n",
    "            \n",
    "mng_noun_phrase_nltk = sorted(mng_noun_phrase_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_noun_phrase_nltk[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_mng=pd.DataFrame(mng_noun_phrase_nltk)\n",
    "df_ph_mng.columns =['noun_phrase in mng by NLTK','frequency']\n",
    "df_ph_mng=df_ph_mng.head(100)\n",
    "\n",
    "df_ph_mng.to_csv('noun_phrase_mng.csv')\n",
    "\n",
    "df_ph_mng_styled = df_ph_mng.style \n",
    "dfi.export(df_ph_mng_styled,\"noun_phrase_mng_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.42457914352417\n",
      "number of words: 406098\n",
      "First 100 frequent words in ent:\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "\n",
    "words_ent=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                word= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',word)\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "    \n",
    "        for word in words_nostop:\n",
    "            #word= lemm(wordx)\n",
    "            if word not in words_ent:\n",
    "                words_ent[word]=1\n",
    "            else:\n",
    "                words_ent[word] +=1\n",
    "            \n",
    "words_ent = sorted(words_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_ent))\n",
    "\n",
    "print('First 100 frequent words in ent:')\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 442846),\n",
       " ('-', 84721),\n",
       " ('new', 61008),\n",
       " ('business', 45906),\n",
       " ('great', 36474),\n",
       " ('today', 34504),\n",
       " ('thanks', 33845),\n",
       " ('people', 32435),\n",
       " ('time', 26833),\n",
       " ('work', 26603),\n",
       " ('need', 25373),\n",
       " ('help', 23725),\n",
       " ('check', 22877),\n",
       " ('good', 21920),\n",
       " ('day', 21759),\n",
       " ('want', 21393),\n",
       " ('know', 20620),\n",
       " ('join', 20020),\n",
       " ('2017', 19599),\n",
       " ('world', 19314),\n",
       " ('free', 18720),\n",
       " ('best', 18364),\n",
       " ('learn', 17892),\n",
       " ('follow', 17366),\n",
       " ('https', 17329),\n",
       " ('2', 16228),\n",
       " ('team', 15814),\n",
       " ('life', 15801),\n",
       " ('week', 15188),\n",
       " ('it', 15161),\n",
       " ('latest', 15094),\n",
       " ('video', 15085),\n",
       " ('way', 15008),\n",
       " ('you', 14788),\n",
       " ('social', 14618),\n",
       " ('money', 14564),\n",
       " ('year', 14498),\n",
       " ('like', 14383),\n",
       " ('#entrepreneur', 14230),\n",
       " ('marketing', 14023),\n",
       " ('read', 13865),\n",
       " ('success', 13479),\n",
       " ('5', 13423),\n",
       " ('news', 13287),\n",
       " ('thank', 13271),\n",
       " ('media', 13245),\n",
       " ('use', 13185),\n",
       " ('3', 12996),\n",
       " ('start', 12777),\n",
       " ('let', 12723),\n",
       " ('support', 12432),\n",
       " ('change', 12229),\n",
       " ('looking', 12181),\n",
       " ('right', 11902),\n",
       " ('women', 11686),\n",
       " ('better', 11543),\n",
       " ('4', 11511),\n",
       " ('1', 11416),\n",
       " ('think', 11219),\n",
       " ('event', 11144),\n",
       " ('things', 11114),\n",
       " ('future', 10933),\n",
       " ('power', 10748),\n",
       " ('live', 10746),\n",
       " ('10', 10689),\n",
       " ('tips', 10458),\n",
       " ('share', 10452),\n",
       " ('come', 10449),\n",
       " ('big', 10395),\n",
       " ('#business', 10223),\n",
       " ('trump', 10206),\n",
       " ('company', 9990),\n",
       " ('#marketing', 9958),\n",
       " ('#startup', 9839),\n",
       " ('startup', 9638),\n",
       " ('tech', 9427),\n",
       " ('ways', 9413),\n",
       " ('entrepreneurs', 9378),\n",
       " ('build', 9279),\n",
       " ('here', 9247),\n",
       " ('happy', 9175),\n",
       " ('now', 9165),\n",
       " ('job', 9153),\n",
       " ('going', 9053),\n",
       " ('facebook', 9023),\n",
       " ('usa', 8970),\n",
       " ('online', 8903),\n",
       " ('daily', 8823),\n",
       " ('years', 8822),\n",
       " ('u', 8800),\n",
       " ('book', 8791),\n",
       " ('create', 8725),\n",
       " ('working', 8711),\n",
       " ('president', 8581),\n",
       " ('#quote', 8466),\n",
       " ('look', 8444),\n",
       " ('data', 8433),\n",
       " ('small', 8331),\n",
       " ('market', 7973),\n",
       " ('following', 7838)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_ent[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_ent=pd.DataFrame(words_ent)\n",
    "df_word_ent.columns =['word in ent','frequency']\n",
    "df_word_ent=df_word_ent.head(100)\n",
    "df_word_ent.to_csv('word_ent.csv')\n",
    "df_word_ent_styled = df_word_ent.style \n",
    "dfi.export(df_word_ent_styled,\"words_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "words_mng=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                    \n",
    "        for word in words_nostop:\n",
    "            #word=lemm(wordx)\n",
    "            if word not in words_mng:\n",
    "                words_mng[word]=1\n",
    "            else:\n",
    "                words_mng[word] +=1\n",
    "                \n",
    "words_mng = sorted(words_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('First 100 frequent words in mng:')\n",
    "#words_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_mng=pd.DataFrame(words_mng)\n",
    "df_word_mng.columns =['word in mng','frequency']\n",
    "df_word_mng=df_word_mng.head(100)\n",
    "\n",
    "df_word_mng.to_csv('word_mng.csv')\n",
    "\n",
    "df_word_mng_styled = df_word_mng.style \n",
    "dfi.export(df_word_mng_styled,\"words_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_ent=dict()\n",
    "\n",
    "for i, value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_ent:\n",
    "                bigrams_ent[combi]=1\n",
    "            else:\n",
    "                bigrams_ent[combi] +=1\n",
    "                \n",
    "bigrams_ent = sorted(bigrams_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_ent))\n",
    "\n",
    "print('first 100 frequent bigrams in ent:')\n",
    "\n",
    "bigrams_ent[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_ent=pd.DataFrame(bigrams_ent)\n",
    "df_bigram_ent.columns =['bigram in ent','frequency']\n",
    "df_bigram_ent=df_bigram_ent.head(100)\n",
    "df_bigram_ent.to_csv('bigram_ent.csv')\n",
    "df_bigram_ent_styled = df_bigram_ent.style \n",
    "dfi.export(df_bigram_ent_styled,\"bigrams_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_mng:\n",
    "                bigrams_mng[combi]=1\n",
    "            else:\n",
    "                bigrams_mng[combi] +=1\n",
    "                \n",
    "bigrams_mng = sorted(bigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent bigrams in mng:')\n",
    "#bigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_mng=pd.DataFrame(bigrams_mng)\n",
    "df_bigram_mng.columns =['bigram in mng','frequency']\n",
    "df_bigram_mng=df_bigram_mng.head(100)\n",
    "\n",
    "df_bigram_mng.to_csv('bigram_mng.csv')\n",
    "\n",
    "df_bigram_mng_styled = df_bigram_mng.style \n",
    "dfi.export(df_bigram_mng_styled,\"bigrams_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bigram_mng_styled.to_excel('jjdsfhj.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_ent=dict()\n",
    "\n",
    "for i, value in enumerate(ent_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)  \n",
    "            if combi not in trigrams_ent:\n",
    "                trigrams_ent[combi]=1\n",
    "            else:\n",
    "                trigrams_ent[combi] +=1\n",
    "                \n",
    "trigrams_ent = sorted(trigrams_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('Number of trigrams:',len(trigrams_ent))\n",
    "\n",
    "print('first 100 frequent trigrams in ent:')\n",
    "trigrams_ent[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)\n",
    "            if combi not in trigrams_mng:\n",
    "                trigrams_mng[combi]=1\n",
    "            else:\n",
    "                trigrams_mng[combi] +=1\n",
    "                \n",
    "trigrams_mng = sorted(trigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of trigrams:',len(trigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent trigrams in mng:')\n",
    "#trigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
