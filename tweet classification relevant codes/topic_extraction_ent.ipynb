{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import html\n",
    "import xml.sax.saxutils as saxutils\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "import random\n",
    "import operator\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "#tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import fasttext.util\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(cachedStopWords))\n",
    "#print(len(cachedStopWords))\n",
    "#print(type(cachedStopWords))\n",
    "\n",
    "nltk_stopwords=set(cachedStopWords)\n",
    "\n",
    "english_alghabet=['b','c','e','f','g','h','j','k','l','n','p','q','r','u','v','w','x','z']\n",
    "\n",
    "numbers_remove=['one','two','three','four','five','six','seven','eight','nine','ten','tens','twenty',\n",
    "                'fourty','fifty','sixty','seventy','eighty','ninety','hundred','hundreds','million','billion','trillion',\n",
    "                'millions','thousand','thousands','second','third','forth','tenth','billions','trillions'] \n",
    "\n",
    "miscellaneous_remove=['absolutely', 'actually', 'adieu', 'ain', \"ain't\", 'aint', 'almost',\n",
    "                       'awesome','awfully','amazing','interesting',\n",
    "                       'alright','alrighty', 'amoungst', 'anybody', 'anymore', 'anyways', 'apart', 'apparently', 'anytime',\n",
    "                       'appropriate',  'approximately', 'arent', 'behold', 'better', 'bravo','briefly','bad','best','brilliant',\n",
    "                       'bye', 'cant', 'certainly', 'chrissakes', 'clearly', 'completely',\n",
    "                       'congrat', 'congrats','congratulation', 'congratulations', 'consequently', 'cool', 'couldnt',\n",
    "                       'darnit', 'de','dear', 'definitely','disappointing', 'didn', 'doesn', 'don', 'downwards',\n",
    "                       'disgusting','dude','down','eg',\"e.g.\",'i.e.',\n",
    "                       'encore','entirely', 'especially', 'et', 'etc', 'everybody', 'ex', 'exactly', 'excellent',\n",
    "                       'fantastic','far', 'farewell','funny',\n",
    "                       'felicitation', 'felicitations','finally', 'fully','furthermore', 'gadzooks', \n",
    "                       'good', 'goodby','goodness', 'gracious', 'great', \n",
    "                       'greetings', 'hallo', 'hardly', 'hasnt', 'haven', 'hello', 'here','hi', 'hither','higher','hopefully',\n",
    "                       'here','there','including',\n",
    "                       'howbeit', 'ie', 'immediately', 'inasmuch', 'inner', 'insofar', 'instead', 'inward', 'important',\n",
    "                       'indeed','just', \"it'd\", \"it'll\", 'inside','kertyschoo', 'kg', 'km', 'lackaday', \n",
    "                       'largely', 'lately', 'later','lovely',\n",
    "                       'lest', 'let', 'lets', 'likely', 'little', 'ltd', 'lower','magnificent', 'mainly', 'marvelous',\n",
    "                       'myself','yourself','yourselves','himself','herself','hisself','ourselves','themsleves',\n",
    "                       'maybe', 'meantime', 'merely', 'minus', 'near', 'nearly', 'necessary', 'never', \n",
    "                       'non', 'normally', 'obviously', 'ok', 'okay', 'ones', 'outside', 'over','other','others','only',\n",
    "                       'overall', 'particular', 'particularly', 'please', 'plus', 'poorly', 'possible','up',\n",
    "                       'possibly', 'potentially', 'predominantly', 'presumably', 'previously','primarily', 'probably',\n",
    "                       'promising',\n",
    "                       'promptly', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref',\n",
    "                       'refs', 'regardless', 'related', 'relatively', 'respectively', 'resulting', 'right', 'sec', \n",
    "                       'secondly','self', 'selves', 'seriously', 'shall', 'shucks','somebody', 'somethan','sorry',\n",
    "                       'somewhat', 'soon', 'late' , 'sorry', 'stupid', 'sub', 'substantially', 'successfully', 'sufficiently',\n",
    "                       \n",
    "                       'super', 'sure', \"t's\", 'th', 'thank', 'thanks', 'thanx', \"that've\", 'thats', 'there', \"there'll\",\n",
    "                       \"there've\", 'thered', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyre', 'thorough',\n",
    "                       'then','thankfully','too','today','yesterday','tomorrow','night',\"morning\",'afternoon','noon','tonight',\n",
    "                       'evening','day','everyday', 'everynight','todays','nights','mornings','noons','afternoons','days',\n",
    "                       'evenings','week','month','year',\n",
    "                       'thoroughly', 'tnx', 'too','truly', 'twice', 'undoubtedly','unfortunately', 'unlike','unlikely',\n",
    "                       'unto',  'usually', 'vs', 'welcome', 'well', 'went', 'werent', 'what', 'whatever', 'wheres', 'widely',\n",
    "                       'wonderful', 'wont', 'wouldnt', 'wrong', 'worst','worse','www', 'yes', 'youd', 'youre', 'yummy', \n",
    "                       'zoinks','shit','literally','literal','pleasure','effective','fabulous','delighted',\n",
    "                       'saturday','sunday','monday','tuesday','wednesday','thursday', 'friday','past','future','suitable',\n",
    "                       'much','many','less','least','few','lots','lot','fewer','fewset','therefore','pm',\n",
    "                       'afaik', 'br', 'idk','smh','qotd', 'ftw','bfn','yw', 'icymi','fomo','smdh', 'b4','imho',\n",
    "                       'urdddd','fab' ,'delightful','absolute','pleasure','huge','latest','nowadays',\n",
    "                       'january','february','april','june','july','august','september','october',\n",
    "                       'november','december', 'autumn' ,'spring','winter','summer',\n",
    "                       'mr','madam','sir','mrs','easy', 'difficult','mr,'\n",
    "                       'weekend','south','north','west','east','asia','africa','europe','america','totally',\n",
    "                       'come', 'comes', 'coming', 'came', 'seems', 'gives', 'gave', 'makes', 'made', 'keeps', 'kept', \n",
    "                       'calls', 'called', 'says', 'saying', 'said', 'goes', 'went', 'gone', 'got', 'saw', 'seen', 'shows',\n",
    "                       'shown', 'took', 'taken', 'uses', 'moved', 'moves', 'puts',\n",
    "                       'using','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "                       'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use',\n",
    "                       'find', 'finds', 'finding','aka',\n",
    "                       'lol' , 'brb', 'lmk', 'ama', 'tbh', 'irl', \"tl;dr\", 'fml', 'bfn' ,' br', 'ht', \"hth\",'j/k', 'lmao' ] #cool\n",
    "                        # small big large useful \n",
    "\n",
    "interjection_remove=['aaaahh', 'aaah', 'aaargh', 'aaay', 'aagh', 'aah',\n",
    "                   'aargh', 'achoo', 'adios', 'ah', 'aha', 'ahem', 'ahh', 'ahhh',\n",
    "                   'ahoy', 'alas', 'allo', 'amen', 'areet', 'argh', 'arrggh',\n",
    "                   'arrividerci', 'asap', 'attaboy', 'avaunt', 'aw', 'aw', 'aww',\n",
    "                   'awww', 'ay', 'ay', 'aye', 'ayeaugh', 'bada', 'badum', 'bah',\n",
    "                   'bahaha', 'bam', 'bazinga', 'behold', 'bingce', 'bingo', 'blah',\n",
    "                   'blech', 'bleh', 'blimey', 'bonjour', 'boo', 'booh', 'boohoo',\n",
    "                   'booyah', 'bravo', 'brr', 'brrrr', 'btw', 'bwahaha', 'capeesh',\n",
    "                   'capisce', 'cheerio', 'cheers', 'ciao', 'cor', 'cowabunga',\n",
    "                   'crikey', 'cripes', 'da', 'dabba', 'dah', 'dammit', 'damn', 'dang',\n",
    "                   'darn', 'de', 'dee', 'di', 'dizamn', 'doh', 'doo', 'drat', 'duh',\n",
    "                   'dum', 'eeeek', 'eek', 'eep', 'egad', 'egads', 'eh', 'ehem', 'em',\n",
    "                   'er', 'eureka', 'eww', 'ewww', 'eyh', 'fiddledeedee', 'fie',\n",
    "                   'fore', 'foul', 'fuff', 'gah', 'gak', 'gee', 'geez', 'gesundheit',\n",
    "                   'giddyap', 'golly', 'gosh', 'grr', 'grrrr', 'ha', 'hah', 'haha',\n",
    "                   'hahaha', 'hallelujah', 'halloa', 'harrumph', 'harumph', 'haw',\n",
    "                   'heck', 'heck', 'heeey', 'heh', 'hehe', 'hey', 'hhh', 'hic', 'hm',\n",
    "                   'hmm', 'hmmm', 'hmmmm', 'hmmph', 'hmpf', 'ho', 'hola', 'hoo',\n",
    "                   'hooray', 'howdy', 'hrmm', 'hrmph', 'hrmph', 'hrrmph', 'hu', 'huh',\n",
    "                   'hullo', 'humph', 'hurrah', 'huzza', 'huzzah', 'ich', 'ick',\n",
    "                   'ixnay', 'jeepers', 'jeez', 'kaboom', 'kapow', 'kerwham', 'la',\n",
    "                   'lala', 'lo', 'lordy', 'meh', 'mhm', 'ml', 'mm', 'mmh', 'mmhm',\n",
    "                   'mmm', 'muahaha', 'mwah', 'mwahaha', 'na','nay','nah', 'nanu', 'nooo', 'nope',\n",
    "                   'nuh', 'oh', 'ohh', 'oho', 'oi', 'okeydoke', 'om', 'oof', 'ooh',\n",
    "                   'oomph', 'oooh', 'ooooh', 'oops', 'ouch', 'ow', 'oww', 'oy',\n",
    "                   'oyez', 'oyh', 'pew', 'pff', 'pffh', 'pfft', 'phew', 'phut',\n",
    "                   'phweep', 'phwoar', 'phwoarr', 'poof', 'poogh', 'prethee',\n",
    "                   'prithee', 'prosit', 'pssh', 'psst', 'queep', 'roger', 'salaam',\n",
    "                   'salam', 'sheesh', 'shh', 'shhh', 'shitfire', 'shoo', 'shoop',\n",
    "                   'shush', 'sigh', 'sssh', 'strewth', 'ta', 'tarnations', 'tchah',\n",
    "                   'teehee', 'tish', 'touché', 'tsk', 'tss', 'tut', 'uggh', 'ugh',\n",
    "                   'uh', 'uhh', 'uhm', 'um', 'umm', 'ummm', 'umph', 'unh', 'upadaisy',\n",
    "                   'upsadaisy', 'ur', 'urgh', 'vay', 'vayf', 'viva', 'voila', 'waa',\n",
    "                   'waaaaah', 'waah', 'wah', 'wahey', 'wassup', 'weee', 'welp',\n",
    "                   'wham', 'whamo', 'whee', 'whew', 'whizz', 'whoa',\n",
    "                   'whoo', 'whoopee','whoop', 'whoops', 'whoopsy', 'whoosh', 'woah', 'woo',\n",
    "                   'woohoo', 'wotcha', 'wotcher', 'wow', 'wowsers', 'wowsers',\n",
    "                   'wuzzup', 'wuzzup', 'wuzzup', 'ya', 'yabba', 'yada', 'yadda',\n",
    "                   'yak', 'yarooh', 'yay', 'yea', 'yeah', 'yech', 'yee', 'yeeeeaah',\n",
    "                   'yeehaw', 'yeow', 'yes', 'yessiree', 'yew', 'yikes', 'yippee',\n",
    "                   'yo', 'yoo', 'yoohoo', 'yow', 'yowza', 'yuck', 'yuh', 'zing',\n",
    "                   'zoiks', 'zomfg', 'zomg', 'zounds', 'zut']\n",
    "             \n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "type(spacy_stopwords)\n",
    "#spacy_exclude=['using','name','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "#               'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use']# serious\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words1 = get_stop_words('english')\n",
    "#print(type(stop_words1))\n",
    "#print()\n",
    "#print(stop_words1)\n",
    "lib_stopwords=set(stop_words1)\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#print(type(ENGLISH_STOP_WORDS))\n",
    "#print()\n",
    "#print(set(ENGLISH_STOP_WORDS))\n",
    "#sklearn_exclude=['find','get','found','go','see','seem','seems','give','seemed','take','keep','show','put','made'] # system  cry\n",
    "sklearn_stopwords=set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "#spacy_stopwords.difference_update(set(spacy_exclude))\n",
    "#sklearn_stopwords.difference_update(set(sklearn_stopwords))\n",
    "#for removing \"just\" one item, use \"remove\"\n",
    "temp_1=set([])\n",
    "#######################\n",
    "temp_1.update(nltk_stopwords)\n",
    "temp_1.update(lib_stopwords)\n",
    "temp_1.update(sklearn_stopwords)\n",
    "temp_1.update(spacy_stopwords)\n",
    "temp_1.update(set(english_alghabet))\n",
    "temp_1.update(set(numbers_remove)) \n",
    "#temp_1.update(set(miscellaneous_remove))\n",
    "temp_1.update(set(interjection_remove))\n",
    "temp_1.update(['rt','be','will','was','were','is','am','are','have','has','had','do','does','done'])\n",
    "###              ,'thing'  ,'want','getting', 'looking','way'])\n",
    "###temp_1.update(['rt','like','look','get','take'])\n",
    "cachedStopWords=temp_1\n",
    "len(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "#print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "#print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "#print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)rt ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "    text=' '.join(text)\n",
    "    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'break break like book'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(\"thanks I broke breaking like the books thanks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningA (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    #text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    #text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    #text=re.sub('Dollar|Dollars|Yen|Yens|Euros', ' money ', text)   # not euro \n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    text = re.sub(r'(HTTP://|HTTPS://)\\S+', '', text)\n",
    "\n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    text=re.sub(r'WWW\\.\\S+', '', text)\n",
    "\n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"United States\",'USA', text)\n",
    "    text=re.sub(\"United Kingdom\",'UK', text)\n",
    "    text=re.sub(\" the US \",' USA ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"U\\.S\\.A\", 'USA', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text)  \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)RT | RT ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_punc(text):\n",
    "    '''\n",
    "    text=re.sub(\"looking forward|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", ' ',text) #except  \\- _\n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    '''\n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    text= re.sub(\">\", ' ',text)\n",
    "    text= re.sub(\"<\", ' ',text)\n",
    "    text= re.sub(\" - \", ' ',text)\n",
    "    text= re.sub(\" --\", ' ',text)\n",
    "    '''\n",
    "    \n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "\n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47604376, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "Number of tweets in ent: 47604376\n"
     ]
    }
   ],
   "source": [
    "#df_ent = pd.read_csv('/archives1/Datasets/TweetsWorld/ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_ent0 = pd.read_csv('ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_ent0.shape)\n",
    "print(df_ent0.columns)\n",
    "#print(df_ent0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_ent=list(df_ent0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_ent= list(zip(df_ent0.user_id, df_ent.tweet))\n",
    "#rows_ent=df_ent0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_ent = df_ent0[['tweet','tweet_created_at']]   \n",
    "\n",
    "\n",
    "del df_ent0\n",
    "\n",
    "print(\"Number of tweets in ent:\",len(df_ent))  #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Memory size of ent:',sys.getsizeof(df_ent)) #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_mng0 = pd.read_csv('mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#df_mng0 = pd.read_csv('/archives1/Datasets/TweetsWorld/mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "print(df_mng0.shape)\n",
    "print(df_mng0.columns)\n",
    "#print(df_mng0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_mng=list(df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_mng0\n",
    "#rows_mng= list(zip(df_mng0.user_id, df_mng.tweet))\n",
    "#rows_mng=df_mng0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_mng = df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']]\n",
    "\n",
    "del df_mng0\n",
    "\n",
    "print(len(df_mng)) #rows_mng0\n",
    "print('memry size of mng:', sys.getsizeof(df_mng)) #rows_mng0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/ent_cleaning_classification.txt\", \"rb\") as fp:  \n",
    "#    ent_tweets_rows=pickle.load(fp)\n",
    "#len(ent_tweets_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind_2019=pickle.load(fp)\n",
    "\n",
    "len(ent_year_ind_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind_2017=pickle.load(fp)\n",
    "\n",
    "len(ent_year_ind_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"result_ent_vote.txt\", \"rb\") as fp:  \n",
    "    vote_19=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_ent_label.txt\", \"rb\") as fp:  \n",
    "    label_19=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_2019_rel=[]\n",
    "ent_2019_irrel=[]\n",
    "for i , k in enumerate(ent_year_ind_2019):\n",
    "\n",
    "    if '2019'  not in df_ent['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_19[i]==1:\n",
    "        ent_2019_rel.append(k)\n",
    "    else:\n",
    "        ent_2019_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892698, 1107302)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_2019_rel), len(ent_2019_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"result_ent_vote.txt\", \"rb\") as fp:  \n",
    "    vote_17=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_ent_label.txt\", \"rb\") as fp:  \n",
    "    label_17=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_2017_rel=[]\n",
    "ent_2017_irrel=[]\n",
    "for i , k in enumerate(ent_year_ind_2017):\n",
    "    if '2017' not in df_ent['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_17[i]==1:\n",
    "        ent_2017_rel.append(k)\n",
    "    else:\n",
    "        ent_2017_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063213, 936787)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_2017_rel), len(ent_2017_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ent['tweet_created_at'][ent_2017_rel[0]], df_ent['tweet'][ent_2017_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454.65953493118286"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0=time.time()\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "#X_train_test=[]\n",
    "\n",
    "tweet_vectors_19=np.zeros([len(ent_2019_rel),embedding_dim],dtype='float32')\n",
    "\n",
    "for i,index in enumerate(ent_2019_rel):\n",
    "    tweet=df_ent['tweet'][index]\n",
    "    \n",
    "    #tweet='I like it.'\n",
    "    text=cleaning(tweet) # the result for cleaning and cleaningA is different \n",
    "\n",
    "    #text=cleaning_punc(text)\n",
    "    #text=cleaning_numbers(text)\n",
    "#train_texts_tokens=[]\n",
    "#for text in train_texts:\n",
    "#    tokens=text.split()\n",
    "    #tokens=nltk.word_tokenize(text)\n",
    "    tokens=tknzr.tokenize(text)\n",
    "    #train_texts_tokens.append(tokens)\n",
    "    \n",
    "#print(len(train_texts_tokens))\n",
    "\n",
    "#for kk,tokens in enumerate(train_texts_tokens):\n",
    "    if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "        a1=[]\n",
    "        num_tok=0\n",
    "        for token in tokens:\n",
    "            num_tok +=1\n",
    "            a1.append( ft.get_word_vector(token) )\n",
    "                    \n",
    "        #print(a1)\n",
    "        #print()\n",
    "        #temp1=np.sum(a1,axis=0)/num_tok\n",
    "        #print(temp1)\n",
    "        #print()\n",
    "        temp=np.mean(a1,axis=0)\n",
    "        #print(abc-temp1)\n",
    "        \n",
    "        \n",
    "        #if len(tokens)>max_tokens:\n",
    "        #    temp=a1[0:max_tokens]\n",
    "        #elif len(tokens)==max_tokens:\n",
    "        #    temp=a1\n",
    "        #else: # if len(tokens)<max_tokens:\n",
    "        #    temp[0:len(tokens)]=a1\n",
    "        #X_train_test.append(temp) \n",
    "        tweet_vectors_19[i]=temp\n",
    "\n",
    "    else:\n",
    "        #print(index)\n",
    "        #print(tweet)\n",
    "        #print('strange')\n",
    "        temp=np.zeros([1,embedding_dim])\n",
    "#        X_train_test.append(temp)\n",
    "        tweet_vectors_19[i]=temp\n",
    "        \n",
    "time.time()-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892698, 300)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_umap = umap.UMAP().fit_transform(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.704980373382568\n",
      "6.704980373382568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "t0=time.time()\n",
    "\n",
    "#kpca = KernelPCA(n_components=2, kernel='linear')\n",
    "#X_kpca = kpca.fit_transform(tweet_vectors_19)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2019 = pca.fit_transform(tweet_vectors_19)\n",
    "\n",
    "print(time.time()-t0)\n",
    "\n",
    "#tsne = TSNE()\n",
    "#X_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892698, 2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = LocallyLinearEmbedding(n_components=2)\n",
    "#X_transformed = embedding.fit_transform(tweet_vectors_19)\n",
    "#X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "x_svd=svd.fit_transform(tweet_vectors_19)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "x_svd.shape\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = Isomap(n_components=2)\n",
    "#X_transformed_iso = embedding.fit_transform(tweet_vectors_19)\n",
    "#X_transformed_iso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.manifold import MDS\n",
    "embedding = MDS(n_components=2)\n",
    "X_transformed = embedding.fit_transform(tweet_vectors_19)\n",
    "X_transformed.shape\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer_19 = hdbscan.HDBSCAN(min_cluster_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.24372839927673\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "clusterer_19.fit(X_pca2019)\n",
    "#clusterer_19.fit(x_svd)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer_19.labels_\n",
    "clusterer_19.labels_[0:10]\n",
    "clusterer_19.labels_.max()\n",
    "np.unique(clusterer_19.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of tweets in each cluster for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700700\n",
      "686\n",
      "784\n",
      "802\n",
      "669\n",
      "2854\n",
      "648\n",
      "24616\n",
      "4015\n",
      "559\n",
      "596\n",
      "853\n",
      "691\n",
      "946\n",
      "1884\n",
      "679\n",
      "536\n",
      "996\n",
      "1483\n",
      "934\n",
      "580\n",
      "803\n",
      "870\n",
      "619\n",
      "549\n",
      "658\n",
      "557\n",
      "711\n",
      "798\n",
      "770\n",
      "544\n",
      "2156\n",
      "136477\n",
      "675\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "cluster_index=[None]*(clusterer_19.labels_.max()+1)\n",
    "#display(cluster_index)\n",
    "\n",
    "for i in range(-1,clusterer_19.labels_.max()+1):\n",
    "    aa1=np.where(clusterer_19.labels_ == i)\n",
    "    if i!=-1:\n",
    "        cluster_index[i]=aa1[0]\n",
    "    print(len(aa1[0]))\n",
    "\n",
    "print('----')\n",
    "#len(cluster_index[4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_index[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=cachedStopWords\n",
    "stop.update(set(english_alghabet))\n",
    "stop.update(set(numbers_remove))\n",
    "stop.update(set(['rt','u']))\n",
    "\n",
    "exclude_punc = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "def clean(doc):\n",
    "    \n",
    "    doc=cleaning(doc)\n",
    "    doc=cleaning_punc(doc)\n",
    "    doc=cleaning_numbers(doc)\n",
    "    doc= re.sub(\" 00am | 00pm | pm | am \", '',doc)\n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() re.sub(\"(\\d+)\", '',i))\n",
    "\n",
    "    stop_free_=[]                      \n",
    "    for i in stop_free.lower().split():\n",
    "                          \n",
    "        if not i.isdigit():\n",
    "            stop_free_.append(i)\n",
    "                          \n",
    "    stop_free = \" \".join(stop_free_)\n",
    "\n",
    "    #text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude_punc)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    cleaned = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Topics and thier words for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for ii in range(0,clusterer_19.labels_.max()+1):\n",
    "    print(\"cluster index : \",ii)\n",
    "\n",
    "    set_x=[]\n",
    "    for i in cluster_index[ii]:\n",
    "        text=df_ent.tweet[ent_2019_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 1, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whoa! @Tesla’s 40.2.1 now shows your cars name when connecting though Bluetooth. Nice touch, @elonmusk and team! https://t.co/QMeOJE10PU'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ent.tweet[ent_2019_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "314354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['money' 'bitcoin' 'bank' 'crypto' 'trade' 'price' 'blockchain' 'btc'\n",
      " 'buy' 'new' 'cryptocurrency' 'stock' 'market' 'cannabis' 'exchange'\n",
      " 'china' 'xrp' 'currency' 'know' 'scam']\n",
      "----------\n",
      "Topic 2 ['follow' 'team' 'follower' 'check' 'update' 'city' 'new' 'automatically'\n",
      " 'win' 'people' 'like' 'player' 'ifb' 'mastermind' 'world' 'league'\n",
      " 'winner' 'retweets' 'money' 'appreciate']\n",
      "----------\n",
      "Topic 3 ['ai' 'share' 'technology' 'data' 'new' 'tech' 'cloud' 'iot' 'business'\n",
      " 'daily' 'car' 'cybersecurity' 'learn' 'digital' 'software' 'intelligence'\n",
      " 'security' 'machine' 'development' 'innovation']\n",
      "----------\n",
      "Topic 4 ['join' 'entrepreneur' 'event' 'startup' 'business' 'look' 'support'\n",
      " 'meet' 'new' 'student' 'woman' 'excite' 'forward' 'team' 'community'\n",
      " 'entrepreneurship' 'work' 'opportunity' 'host' 'share']\n",
      "----------\n",
      "Topic 5 ['point' 'job' 'help' 'problem' 'new' 'follow' 'nasa' 'powerful' 'domain'\n",
      " 'musk' 'elon' 'pls' 'sharperly' 'book' 'launch' 'space' 'horse' 'solve'\n",
      " 'outstanding' 'list']\n",
      "----------\n",
      "Topic 6 ['president' 'trump' 'people' 'country' 'vote' 'news' 'state' 'government'\n",
      " 'nigeria' 'election' 'know' 'india' 'law' 'need' 'usa' 'think' 'party'\n",
      " 'money' 'time' 'medium']\n",
      "----------\n",
      "Topic 7 ['people' 'success' 'life' 'learn' 'work' 'thing' 'agree' 'change' 'think'\n",
      " 'business' 'goal' 'time' 'know' 'entrepreneur' 'need' 'want' 'new' 'idea'\n",
      " 'focus' 'help']\n",
      "----------\n",
      "Topic 8 ['new' 'post' 'check' 'video' 'read' 'link' 'facebook' 'book' 'free'\n",
      " 'follow' 'true' 'website' 'app' 'write' 'blog' 'email' 'business' 'share'\n",
      " 'need' 'online']\n",
      "----------\n",
      "Topic 9 ['market' 'content' 'social' 'medium' 'brand' 'seo' 'business'\n",
      " 'socialmedia' 'digitalmarketing' 'tip' 'strategy' 'question' 'answer'\n",
      " 'sale' 'marketer' 'story' 'check' 'daily' 'guide' 'list']\n",
      "----------\n",
      "Topic 10 ['money' 'business' 'work' 'time' 'need' 'company' 'way' 'pay' 'start'\n",
      " 'tax' 'invest' 'people' 'startup' 'help' 'market' 'new' 'job' 'know'\n",
      " 'entrepreneur' 'grow']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    set_x=[]\n",
    "    for i in range(0,len(ent_2019_rel)):\n",
    "        #print(ent_2019_rel[i])\n",
    "        text=df_ent.tweet[ent_2019_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 10, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusterer_optics = OPTICS(min_samples=5).fit(X_pca)\n",
    "#clusterer_dbscan = DBSCAN(eps=5, min_samples=2).fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#kmeans = KMeans(n_clusters=3) \n",
    "#ent_kmeans_clustering = kmeans.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565.7883365154266"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0=time.time()\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "#X_train_test=[]\n",
    "\n",
    "tweet_vectors_17=np.zeros([len(ent_2017_rel),embedding_dim],dtype='float32')\n",
    "\n",
    "for i,index in enumerate(ent_2017_rel):\n",
    "    tweet=df_ent['tweet'][index]\n",
    "    \n",
    "    #tweet='I like it.'\n",
    "    text=cleaning(tweet) # the result for cleaning and cleaningA is different \n",
    "    #text=cleaning_punc(text)\n",
    "    #text=cleaning_numbers(text)\n",
    "#train_texts_tokens=[]\n",
    "#for text in train_texts:\n",
    "#    tokens=text.split()\n",
    "    #tokens=nltk.word_tokenize(text)\n",
    "    tokens=tknzr.tokenize(text)\n",
    "    #train_texts_tokens.append(tokens)\n",
    "    \n",
    "#print(len(train_texts_tokens))\n",
    "\n",
    "#for kk,tokens in enumerate(train_texts_tokens):\n",
    "    if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "        a1=[]\n",
    "        num_tok=0\n",
    "        for token in tokens:\n",
    "            num_tok +=1\n",
    "            a1.append( ft.get_word_vector(token) )\n",
    "                    \n",
    "        #print(a1)\n",
    "        #print()\n",
    "        #temp1=np.sum(a1,axis=0)/num_tok\n",
    "        #print(temp1)\n",
    "        #print()\n",
    "        temp=np.mean(a1,axis=0)\n",
    "        #print(abc-temp1)\n",
    "        \n",
    "        \n",
    "        #if len(tokens)>max_tokens:\n",
    "        #    temp=a1[0:max_tokens]\n",
    "        #elif len(tokens)==max_tokens:\n",
    "        #    temp=a1\n",
    "        #else: # if len(tokens)<max_tokens:\n",
    "        #    temp[0:len(tokens)]=a1\n",
    "        #X_train_test.append(temp) \n",
    "        tweet_vectors_17[i]=temp\n",
    "\n",
    "    else:\n",
    "        #print(index)\n",
    "        #print(tweet)\n",
    "        #print('strange')\n",
    "        temp=np.zeros([1,embedding_dim])\n",
    "#        X_train_test.append(temp)\n",
    "        tweet_vectors_17[i]=temp\n",
    "        \n",
    "time.time()-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063213, 300)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tweet_vectors_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.109500646591187\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2017 = pca.fit_transform(tweet_vectors_17)\n",
    "\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer_17 = hdbscan.HDBSCAN(min_cluster_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.28069949150085\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "clusterer_17.fit(X_pca2017)\n",
    "#clusterer.fit(x_svd)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "       33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
       "       50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
       "       67, 68, 69, 70, 71, 72], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer_17.labels_\n",
    "\n",
    "clusterer_17.labels_[0:10]\n",
    "\n",
    "clusterer_17.labels_.max()\n",
    "\n",
    "np.unique(clusterer_17.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of tweets in each cluster for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857986\n",
      "916\n",
      "533\n",
      "546\n",
      "1616\n",
      "1418\n",
      "549\n",
      "551\n",
      "537\n",
      "78219\n",
      "838\n",
      "2177\n",
      "555\n",
      "1268\n",
      "1335\n",
      "508\n",
      "706\n",
      "17181\n",
      "628\n",
      "1096\n",
      "836\n",
      "954\n",
      "688\n",
      "578\n",
      "656\n",
      "545\n",
      "770\n",
      "662\n",
      "670\n",
      "769\n",
      "617\n",
      "962\n",
      "955\n",
      "1139\n",
      "602\n",
      "995\n",
      "529\n",
      "545\n",
      "1128\n",
      "1061\n",
      "1422\n",
      "1210\n",
      "1385\n",
      "1339\n",
      "783\n",
      "729\n",
      "1124\n",
      "1408\n",
      "1674\n",
      "742\n",
      "1180\n",
      "1416\n",
      "1444\n",
      "2586\n",
      "763\n",
      "667\n",
      "1540\n",
      "999\n",
      "1420\n",
      "2728\n",
      "619\n",
      "1151\n",
      "594\n",
      "2145\n",
      "2176\n",
      "984\n",
      "626\n",
      "1386\n",
      "1467\n",
      "1262\n",
      "682\n",
      "1361\n",
      "1974\n",
      "35403\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "cluster_index=[None]*(clusterer_17.labels_.max()+1)\n",
    "#display(cluster_index)\n",
    "\n",
    "for i in range(-1,clusterer_17.labels_.max()+1):\n",
    "    aa1=np.where(clusterer_17.labels_ == i)\n",
    "    if i!=-1:\n",
    "        cluster_index[i]=aa1[0]\n",
    "    print(len(aa1[0]))\n",
    "\n",
    "print('----')\n",
    "#len(cluster_index[4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics and thier words for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for ii in range(0,clusterer_17.labels_.max()+1):\n",
    "    print(\"cluster index : \",ii)\n",
    "\n",
    "    set_x=[]\n",
    "    for i in cluster_index[ii]:\n",
    "        text=df_ent.tweet[ent_2017_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2017_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 1, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "301500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['president' 'people' 'country' 'tax' 'vote' 'trump' 'state' 'nigeria'\n",
      " 'india' 'link' 'need' 'know' 'government' 'money' 'election' 'follow'\n",
      " 'lose' 'way' 'video' 'time']\n",
      "----------\n",
      "Topic 2 ['work' 'success' 'new' 'life' 'business' 'time' 'post' 'entrepreneur'\n",
      " 'people' 'change' 'thing' 'learn' 'video' 'start' 'goal' 'leadership'\n",
      " 'want' 'facebook' 'way' 'world']\n",
      "----------\n",
      "Topic 3 ['news' 'agree' 'trump' 'new' 'stats' 'power' 'usa' 'follower' 'dm'\n",
      " 'unfollowers' 'fake' 'need' 'daily' 'fact' 'washington' 'know' 'russia'\n",
      " 'progress' 'lisa' 'help']\n",
      "----------\n",
      "Topic 4 ['startup' 'look' 'entrepreneur' 'business' 'work' 'book' 'apply'\n",
      " 'student' 'forward' 'new' 'team' 'pitch' 'join' 'innovation' 'help'\n",
      " 'program' 'need' 'school' 'money' 'learn']\n",
      "----------\n",
      "Topic 5 ['health' 'intelligence' 'artificial' 'help' 'new' 'water' 'work' 'need'\n",
      " 'mental' 'time' 'cost' 'learn' 'money' 'entrepreneurship' 'http' 'care'\n",
      " 'suesparrow' 'study' 'world' 'sense']\n",
      "----------\n",
      "Topic 6 ['problem' 'quote' 'true' 'help' 'pisces' 'leo' 'solve' 'support'\n",
      " 'question' 'build' 'plan' 'work' 'answer' 'run' 'sagittarius' 'solution'\n",
      " 'road' 'new' 'map' 'money']\n",
      "----------\n",
      "Topic 7 ['money' 'business' 'startup' 'market' 'bitcoin' 'ai' 'tech' 'social'\n",
      " 'entrepreneur' 'medium' 'blockchain' 'company' 'invest' 'digital'\n",
      " 'technology' 'new' 'socialmedia' 'fintech' 'fund' 'bank']\n",
      "----------\n",
      "Topic 8 ['follow' 'connect' 'check' 'happy' 'follower' 'new' 'want' 'appreciate'\n",
      " 'thx' 'twitter' 'shoutout' 'grow' 'people' 'automatically' 'join'\n",
      " 'growthhacking' 'truth' 'crowdfunding' 'gain' 'community']\n",
      "----------\n",
      "Topic 9 ['market' 'business' 'tip' 'content' 'seo' 'read' 'share' 'need' 'sale'\n",
      " 'website' 'customer' 'brand' 'way' 'strategy' 'new' 'email' 'online'\n",
      " 'blog' 'tool' 'google']\n",
      "----------\n",
      "Topic 10 ['event' 'join' 'excite' 'new' 'meet' 'business' 'award' 'entrepreneur'\n",
      " 'woman' 'check' 'host' 'conference' 'idea' 'team' 'startup' 'http'\n",
      " 'attend' 'free' 'look' 'city']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "    set_x=[]\n",
    "    for i in range(0,len(ent_2017_rel)):\n",
    "        #print(ent_2017_rel[i])\n",
    "        text=df_ent.tweet[ent_2017_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 10, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment codes (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def noun_phrases(text):\n",
    "    # noun phrase by nltk\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "    grammar = \"\"\"NP: {<DT>?<JJ>*<NN.*>+}\"\"\"\n",
    "\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    #for i in sentences:\n",
    "    #    result = cp.parse(i)\n",
    "    #    print(result)\n",
    "    #    print(type(result))\n",
    "        #result.draw() \n",
    "   # noun_phrases_list = [[' '.join(leaf[0].lower() for leaf in tree.leaves()) \n",
    "\n",
    "    noun_phrases_list = [[' '.join(leaf[0] for leaf in tree.leaves()) \n",
    "                          for tree in cp.parse(sent).subtrees() \n",
    "                          if tree.label()=='NP'] \n",
    "                          for sent in sentences]\n",
    "\n",
    "\n",
    "    phrases_all=[]\n",
    "    for i in noun_phrases_list:\n",
    "        phrases_all.extend(i)\n",
    "    \n",
    "    return phrases_all\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def noun_phrases1(text):\n",
    "    # noun phrase by Spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    phrases_all=[]\n",
    "    for phrase in doc.noun_chunks:\n",
    "        phrases_all.append(phrase.text)\n",
    "        #phrases_all.append(phrase.text.lower())\n",
    "\n",
    "    return phrases_all\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./ent_results300_np.txt\", \"rb\") as fp:   \n",
    "    ent_results = pickle.load(fp)\n",
    "\n",
    "num_ent_positive=0\n",
    "for i in ent_results:\n",
    "    if i>=0.5:\n",
    "        num_ent_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to ent:\",num_ent_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./mng_results300.txt\", \"rb\") as fp:   \n",
    "    mng_results = pickle.load(fp)\n",
    "\n",
    "num_mng_positive=0\n",
    "for i in mng_results:\n",
    "    if i>=0.5:\n",
    "        num_mng_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to mng:\", num_mng_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequnent hashtags in ENT (with thier frequencies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "ent_hash_2017=[]\n",
    "for i,value in enumerate(ent_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2017.extend(hashtags)\n",
    "        \n",
    "ent_Hashtags=dict()\n",
    "\n",
    "for ph in ent_hash:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags:\n",
    "            ent_Hashtags[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags = sorted(ent_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.41263008117676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('entrepreneur', 13619),\n",
       " ('business', 10195),\n",
       " ('marketing', 9952),\n",
       " ('startup', 9195),\n",
       " ('quote', 8460),\n",
       " ('startups', 7288),\n",
       " ('entrepreneurship', 6650),\n",
       " ('leadership', 6580),\n",
       " ('ai', 5865),\n",
       " ('entrepreneurs', 5327),\n",
       " ('socialmedia', 5043),\n",
       " ('success', 4946),\n",
       " ('tech', 4877),\n",
       " ('smallbiz', 3719),\n",
       " ('innovation', 3690),\n",
       " ('fintech', 3594),\n",
       " ('mca', 3478),\n",
       " ('seo', 3221),\n",
       " ('growthhacking', 3111),\n",
       " ('bitcoin', 3051),\n",
       " ('digitalmarketing', 2861),\n",
       " ('blockchain', 2831),\n",
       " ('iot', 2307),\n",
       " ('technology', 2245),\n",
       " ('ecommerce', 2185),\n",
       " ('bigdata', 2124),\n",
       " ('motivation', 1998),\n",
       " ('travel', 1964),\n",
       " ('contentmarketing', 1921),\n",
       " ('machinelearning', 1899),\n",
       " ('sales', 1726),\n",
       " ('productivity', 1707),\n",
       " ('crowdfunding', 1688),\n",
       " ('smallbusiness', 1639),\n",
       " ('cybersecurity', 1581),\n",
       " ('smm', 1561),\n",
       " ('shoutout', 1525),\n",
       " ('inspiration', 1451),\n",
       " ('africa', 1448),\n",
       " ('health', 1435),\n",
       " ('hr', 1350),\n",
       " ('mondaymotivation', 1289),\n",
       " ('socent', 1268),\n",
       " ('education', 1230),\n",
       " ('news', 1208),\n",
       " ('cryptocurrency', 1182),\n",
       " ('1', 1163),\n",
       " ('india', 1145),\n",
       " ('growth', 1097),\n",
       " ('life', 1094),\n",
       " ('women', 1087),\n",
       " ('podcast', 1087),\n",
       " ('digital', 1078),\n",
       " ('theshrimptank', 1069),\n",
       " ('google', 1027),\n",
       " ('strategy', 1004),\n",
       " ('data', 995),\n",
       " ('datascience', 990),\n",
       " ('branding', 974),\n",
       " ('artificialintelligence', 971),\n",
       " ('blogging', 970),\n",
       " ('fundicaroadshow', 961),\n",
       " ('healthcare', 956),\n",
       " ('funding', 950),\n",
       " ('quotes', 929),\n",
       " ('coworking', 924),\n",
       " ('content', 916),\n",
       " ('deeplearning', 904),\n",
       " ('ceo', 900),\n",
       " ('goals', 899),\n",
       " ('money', 887),\n",
       " ('free', 884),\n",
       " ('design', 879),\n",
       " ('energy', 869),\n",
       " ('facebook', 854),\n",
       " ('mobile', 853),\n",
       " ('realestate', 850),\n",
       " ('vr', 849),\n",
       " ('networking', 846),\n",
       " ('finance', 835),\n",
       " ('businessowner', 825),\n",
       " ('vc', 821),\n",
       " ('retail', 779),\n",
       " ('wordpress', 775),\n",
       " ('wednesdaywisdom', 770),\n",
       " ('insurtech', 769),\n",
       " ('startupindia', 755),\n",
       " ('investing', 753),\n",
       " ('brand', 745),\n",
       " ('cloud', 742),\n",
       " ('music', 737),\n",
       " ('b2b', 737),\n",
       " ('analytics', 726),\n",
       " ('edtech', 718),\n",
       " ('food', 708),\n",
       " ('quoteoftheday', 704),\n",
       " ('invest', 692),\n",
       " ('security', 692),\n",
       " ('trump', 682),\n",
       " ('govcon', 678)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "ent_hash_2017=[]\n",
    "\n",
    "for i in range(0,len(ent_2017_rel)):\n",
    "    #print(ent_2017_rel[i])\n",
    "    text=df_ent.tweet[ent_2017_rel[i]]\n",
    "    \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "    text=re.sub(\"…\", '' ,text)\n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.)+\", ' ',text)  \n",
    "    \n",
    "    hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "    ent_hash_2017.extend(hashtags)\n",
    "\n",
    "'''\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2017.extend(hashtags)\n",
    "'''     \n",
    "     \n",
    "ent_Hashtags_2017=dict()\n",
    "\n",
    "for ph in ent_hash_2017:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags_2017:\n",
    "            ent_Hashtags_2017[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags_2017[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags_2017 = sorted(ent_Hashtags_2017.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "\n",
    "ent_Hashtags_2017[0:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_ent=pd.DataFrame(ent_Hashtags_2017)\n",
    "df_hash_ent.columns =['Hashtags in ent','frequency']\n",
    "df_hash_ent=df_hash_ent.head(100)\n",
    "df_hash_ent.to_csv('hashtags_ent.csv')\n",
    "df_hash_ent_styled = df_hash_ent.style \n",
    "dfi.export(df_hash_ent_styled,\"hashtags_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.53921341896057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('entrepreneur', 8251),\n",
       " ('business', 8065),\n",
       " ('startup', 5290),\n",
       " ('marketing', 5244),\n",
       " ('entrepreneurship', 4512),\n",
       " ('startups', 4263),\n",
       " ('success', 3738),\n",
       " ('leadership', 3620),\n",
       " ('entrepreneurs', 3554),\n",
       " ('ai', 3511),\n",
       " ('digitalmarketing', 3012),\n",
       " ('socialmedia', 2899),\n",
       " ('tech', 2546),\n",
       " ('innovation', 2459),\n",
       " ('smallbusiness', 2437),\n",
       " ('smallbiz', 2350),\n",
       " ('motivation', 2302),\n",
       " ('quote', 2223),\n",
       " ('seo', 1819),\n",
       " ('technology', 1785),\n",
       " ('blockchain', 1719),\n",
       " ('inspiration', 1515),\n",
       " ('realestate', 1506),\n",
       " ('sales', 1442),\n",
       " ('contentmarketing', 1403),\n",
       " ('shopmycloset', 1353),\n",
       " ('bitcoin', 1326),\n",
       " ('fashion', 1317),\n",
       " ('branding', 1315),\n",
       " ('ecommerce', 1276),\n",
       " ('healthcare', 1259),\n",
       " ('podcast', 1255),\n",
       " ('iot', 1249),\n",
       " ('fintech', 1243),\n",
       " ('mondaymotivation', 1164),\n",
       " ('cybersecurity', 1156),\n",
       " ('health', 1119),\n",
       " ('artificialintelligence', 1071),\n",
       " ('crypto', 1031),\n",
       " ('machinelearning', 1019),\n",
       " ('rt', 985),\n",
       " ('love', 980),\n",
       " ('digital', 891),\n",
       " ('smm', 885),\n",
       " ('socialmediamarketing', 879),\n",
       " ('entrepreneurlife', 877),\n",
       " ('travel', 851),\n",
       " ('1', 847),\n",
       " ('goals', 836),\n",
       " ('mindset', 833),\n",
       " ('life', 824),\n",
       " ('bigdata', 823),\n",
       " ('education', 812),\n",
       " ('india', 796),\n",
       " ('instagram', 795),\n",
       " ('ceo', 786),\n",
       " ('design', 785),\n",
       " ('money', 774),\n",
       " ('blogging', 770),\n",
       " ('quotes', 766),\n",
       " ('cryptocurrency', 738),\n",
       " ('repost', 737),\n",
       " ('google', 736),\n",
       " ('news', 734),\n",
       " ('fitness', 731),\n",
       " ('data', 718),\n",
       " ('brexit', 708),\n",
       " ('art', 705),\n",
       " ('management', 701),\n",
       " ('productivity', 694),\n",
       " ('growthhacking', 670),\n",
       " ('businessowner', 666),\n",
       " ('makemoneyonline', 655),\n",
       " ('networking', 626),\n",
       " ('facebook', 607),\n",
       " ('datascience', 606),\n",
       " ('africa', 598),\n",
       " ('investing', 595),\n",
       " ('edtech', 590),\n",
       " ('leaders', 589),\n",
       " ('funding', 587),\n",
       " ('digitaltransformation', 585),\n",
       " ('coaching', 582),\n",
       " ('brand', 581),\n",
       " ('jobs', 577),\n",
       " ('climatechange', 574),\n",
       " ('strategy', 571),\n",
       " ('women', 565),\n",
       " ('socent', 562),\n",
       " ('smb', 558),\n",
       " ('earnmoney', 550),\n",
       " ('wednesdaywisdom', 549),\n",
       " ('photography', 546),\n",
       " ('investment', 546),\n",
       " ('science', 544),\n",
       " ('hustle', 540),\n",
       " ('nigeria', 540),\n",
       " ('video', 539),\n",
       " ('style', 537),\n",
       " ('security', 535)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "ent_hash_2019=[]\n",
    "\n",
    "for i in range(0,len(ent_2019_rel)):\n",
    "    #print(ent_2019_rel[i])\n",
    "    text=df_ent.tweet[ent_2019_rel[i]]\n",
    "    \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "    text=re.sub(\"…\", '' ,text)\n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.)+\", ' ',text)  \n",
    "    \n",
    "    hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "    ent_hash_2019.extend(hashtags)\n",
    "\n",
    "\n",
    "'''\n",
    "for i,value in enumerate(ent_year_ind_2019):\n",
    "    if label_19[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2019.extend(hashtags)\n",
    "'''        \n",
    "        \n",
    "        \n",
    "ent_Hashtags_2019=dict()\n",
    "\n",
    "for ph in ent_hash_2019:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags_2019:\n",
    "            ent_Hashtags_2019[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags_2019[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags_2019 = sorted(ent_Hashtags_2019.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "ent_Hashtags_2019[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequent hashtags in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "mng_hash=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "\n",
    "        mng_hash.extend(hashtags)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "mng_Hashtags=dict()\n",
    "\n",
    "for ph in mng_hash:\n",
    "    if ph != '':\n",
    "        if ph not in mng_Hashtags:\n",
    "            mng_Hashtags[ph]=1\n",
    "        else:\n",
    "            mng_Hashtags[ph]+=1  \n",
    "\n",
    "mng_Hashtags = sorted(mng_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_Hashtags[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_mng=pd.DataFrame(mng_Hashtags)\n",
    "df_hash_mng.columns =['Hashtags in mng','frequency']\n",
    "df_hash_mng=df_hash_mng.head(100)\n",
    "\n",
    "df_hash_mng.to_csv('hashtags_mng.csv')\n",
    "\n",
    "df_hash_mng_styled = df_hash_mng.style \n",
    "dfi.export(df_hash_mng_styled,\"hashtags_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignore below lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "ent_spacy=[]\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        ent_spacy.extend(concepts)\n",
    "    \n",
    "\n",
    "ent_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in ent_spacy:\n",
    "    if ph not in ent_noun_phrase_spacy:\n",
    "        ent_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        ent_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "ent_noun_phrase_spacy = sorted(ent_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "mng_spacy=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "        \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        mng_spacy.extend(concepts)\n",
    "    \n",
    "    \n",
    "mng_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in mng_spacy:\n",
    "    if ph not in mng_noun_phrase_spacy:\n",
    "        mng_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        mng_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "mng_noun_phrase_spacy = sorted(mng_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in ENT found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "## finding concepts by nltk\n",
    "#ent_2017_nltk=[]\n",
    "\n",
    "#ent_noun_phrase_2017_nltk=dict()\n",
    "\n",
    "#for i,value in enumerate(ent_year_ind_2017):\n",
    "#    if label_17[i]==1:\n",
    "#        text=cleaningA(df_ent['tweet'][value])\n",
    "##        print(text)\n",
    "##        print('----')\n",
    "#        #text=re.sub('#', ' ', text)\n",
    "#        concepts_x=noun_phrases(text)\n",
    "    \n",
    "#        concepts=[]\n",
    "#        for concept in concepts_x:\n",
    "#            if concept not in cachedStopWords:\n",
    "#                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "#                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "#                if concept !='':\n",
    "#                    if concept not in ent_noun_phrase_2017_nltk:\n",
    "#                        ent_noun_phrase_2017_nltk[concept]=1\n",
    "#                    else:\n",
    "#                        ent_noun_phrase_2017_nltk[concept]+=1\n",
    "                \n",
    "#                #concepts.append(concept)\n",
    "                \n",
    "##        print(concepts)\n",
    "##        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "#        #ent_2017_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in ent_2017_nltk:\n",
    "    if word !='':\n",
    "        if word not in ent_noun_phrase_2017_nltk:\n",
    "            ent_noun_phrase_2017_nltk[word]=1\n",
    "        else:\n",
    "            ent_noun_phrase_2017_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "#ent_noun_phrase_2017_nltk = sorted(ent_noun_phrase_2017_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "#print( time.time() - t0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_noun_phrase_2017_nltk[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i,value in enumerate(ent_year_ind_2017[0:1000000]):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        #text= re.sub(\"#\", '',text0)\n",
    "        if 'Great ' in text:\n",
    "            if 'Great' in noun_phrases(text):\n",
    "                \n",
    "                print(text)\n",
    "                print(noun_phrases(text))\n",
    "                print('-----')\n",
    "                print(noun_phrases1(text))\n",
    "                print()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_noun_phrase_2017_nltk_non[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "yes_17=[]\n",
    "for i in ent_noun_phrase_2017_nltk[0:50]:\n",
    "    yes_17.append(i[0])\n",
    "\n",
    "non_17=[]\n",
    "for i in ent_noun_phrase_2017_nltk_non[0:50]:\n",
    "    non_17.append(i[0])\n",
    "A\n",
    "aaa=set(yes_17)-set(non_17)\n",
    "aaa\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())\n",
    "\n",
    "aaa=list(set(yes)-set(non))\n",
    "aaa=list(set(yes))\n",
    "concept_vec=np.zeros((len(aaa),300))\n",
    "\n",
    "for i,concept in enumerate(aaa):\n",
    "    concept_vec[i]=ft.get_word_vector(concept)\n",
    "\n",
    "\n",
    "np.shape(concept_vec)\n",
    "\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustering_dbscan = DBSCAN(eps=5, min_samples=2).fit(concept_vec)\n",
    "print(clustering_dbscan.labels_)\n",
    "\n",
    "clustering_optics = OPTICS(min_samples=3).fit(concept_vec)\n",
    "print(clustering_optics.labels_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2) \n",
    "ent_kmeans_clustering = kmeans.fit(concept_vec)\n",
    "print(ent_kmeans_clustering.labels_)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_ent=pd.DataFrame(ent_noun_phrase_nltk)\n",
    "df_ph_ent.columns =['noun_phrase in ent by NLTK','frequency']\n",
    "df_ph_ent=df_ph_ent.head(100)\n",
    "df_ph_ent.to_csv('noun_phrase_ent.csv')\n",
    "df_ph_ent_styled = df_ph_ent.style \n",
    "dfi.export(df_ph_ent_styled,\"noun_phrase_ent_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "yes_19=[]\n",
    "for i in ent_noun_phrase_2019_nltk[0:50]:\n",
    "    yes_19.append(i[0])\n",
    "\n",
    "non_19=[]\n",
    "for i in ent_noun_phrase_2019_nltk_non[0:50]:\n",
    "    non_19.append(i[0])\n",
    "\n",
    "bbb=set(yes_19)-set(non_19)\n",
    "bbb\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in MNG found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "#finding concepts by nltk \n",
    "mng_nltk=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                #concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                #concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                concepts.append(concept)\n",
    "        \n",
    "#        print(concepts)       \n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "        mng_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "mng_noun_phrase_nltk=dict()\n",
    "\n",
    "for word in mng_nltk:\n",
    "    if word !='':\n",
    "        if word not in mng_noun_phrase_nltk:\n",
    "            mng_noun_phrase_nltk[word]=1\n",
    "        else:\n",
    "            mng_noun_phrase_nltk[word]+=1   \n",
    "            \n",
    "mng_noun_phrase_nltk = sorted(mng_noun_phrase_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_noun_phrase_nltk[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_mng=pd.DataFrame(mng_noun_phrase_nltk)\n",
    "df_ph_mng.columns =['noun_phrase in mng by NLTK','frequency']\n",
    "df_ph_mng=df_ph_mng.head(100)\n",
    "\n",
    "df_ph_mng.to_csv('noun_phrase_mng.csv')\n",
    "\n",
    "df_ph_mng_styled = df_ph_mng.style \n",
    "dfi.export(df_ph_mng_styled,\"noun_phrase_mng_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "words_ent=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                word= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',word)\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "    \n",
    "        for word in words_nostop:\n",
    "            #word= lemm(wordx)\n",
    "            if word not in words_ent:\n",
    "                words_ent[word]=1\n",
    "            else:\n",
    "                words_ent[word] +=1\n",
    "            \n",
    "words_ent = sorted(words_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_ent))\n",
    "\n",
    "print('First 100 frequent words in ent:')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_ent[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_ent=pd.DataFrame(words_ent)\n",
    "df_word_ent.columns =['word in ent','frequency']\n",
    "df_word_ent=df_word_ent.head(100)\n",
    "df_word_ent.to_csv('word_ent.csv')\n",
    "df_word_ent_styled = df_word_ent.style \n",
    "dfi.export(df_word_ent_styled,\"words_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "words_mng=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                    \n",
    "        for word in words_nostop:\n",
    "            #word=lemm(wordx)\n",
    "            if word not in words_mng:\n",
    "                words_mng[word]=1\n",
    "            else:\n",
    "                words_mng[word] +=1\n",
    "                \n",
    "words_mng = sorted(words_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('First 100 frequent words in mng:')\n",
    "#words_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_mng=pd.DataFrame(words_mng)\n",
    "df_word_mng.columns =['word in mng','frequency']\n",
    "df_word_mng=df_word_mng.head(100)\n",
    "\n",
    "df_word_mng.to_csv('word_mng.csv')\n",
    "\n",
    "df_word_mng_styled = df_word_mng.style \n",
    "dfi.export(df_word_mng_styled,\"words_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_ent=dict()\n",
    "\n",
    "for i, value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_ent:\n",
    "                bigrams_ent[combi]=1\n",
    "            else:\n",
    "                bigrams_ent[combi] +=1\n",
    "                \n",
    "bigrams_ent = sorted(bigrams_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_ent))\n",
    "\n",
    "print('first 100 frequent bigrams in ent:')\n",
    "\n",
    "bigrams_ent[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_ent=pd.DataFrame(bigrams_ent)\n",
    "df_bigram_ent.columns =['bigram in ent','frequency']\n",
    "df_bigram_ent=df_bigram_ent.head(100)\n",
    "df_bigram_ent.to_csv('bigram_ent.csv')\n",
    "df_bigram_ent_styled = df_bigram_ent.style \n",
    "dfi.export(df_bigram_ent_styled,\"bigrams_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_mng:\n",
    "                bigrams_mng[combi]=1\n",
    "            else:\n",
    "                bigrams_mng[combi] +=1\n",
    "                \n",
    "bigrams_mng = sorted(bigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent bigrams in mng:')\n",
    "#bigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_mng=pd.DataFrame(bigrams_mng)\n",
    "df_bigram_mng.columns =['bigram in mng','frequency']\n",
    "df_bigram_mng=df_bigram_mng.head(100)\n",
    "\n",
    "df_bigram_mng.to_csv('bigram_mng.csv')\n",
    "\n",
    "df_bigram_mng_styled = df_bigram_mng.style \n",
    "dfi.export(df_bigram_mng_styled,\"bigrams_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bigram_mng_styled.to_excel('jjdsfhj.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_ent=dict()\n",
    "\n",
    "for i, value in enumerate(ent_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)  \n",
    "            if combi not in trigrams_ent:\n",
    "                trigrams_ent[combi]=1\n",
    "            else:\n",
    "                trigrams_ent[combi] +=1\n",
    "                \n",
    "trigrams_ent = sorted(trigrams_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('Number of trigrams:',len(trigrams_ent))\n",
    "\n",
    "print('first 100 frequent trigrams in ent:')\n",
    "trigrams_ent[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)\n",
    "            if combi not in trigrams_mng:\n",
    "                trigrams_mng[combi]=1\n",
    "            else:\n",
    "                trigrams_mng[combi] +=1\n",
    "                \n",
    "trigrams_mng = sorted(trigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of trigrams:',len(trigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent trigrams in mng:')\n",
    "#trigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
