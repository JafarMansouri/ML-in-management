{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import html\n",
    "import xml.sax.saxutils as saxutils\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "import random\n",
    "import operator\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "#tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import fasttext.util\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(cachedStopWords))\n",
    "#print(len(cachedStopWords))\n",
    "#print(type(cachedStopWords))\n",
    "\n",
    "nltk_stopwords=set(cachedStopWords)\n",
    "\n",
    "english_alghabet=['b','c','e','f','g','h','j','k','l','n','p','q','r','u','v','w','x','z']\n",
    "\n",
    "numbers_remove=['one','two','three','four','five','six','seven','eight','nine','ten','tens','twenty',\n",
    "                'fourty','fifty','sixty','seventy','eighty','ninety','hundred','hundreds','million','billion','trillion',\n",
    "                'millions','thousand','thousands','second','third','forth','tenth','billions','trillions'] \n",
    "\n",
    "miscellaneous_remove=['absolutely', 'actually', 'adieu', 'ain', \"ain't\", 'aint', 'almost',\n",
    "                       'awesome','awfully','amazing','interesting',\n",
    "                       'alright','alrighty', 'amoungst', 'anybody', 'anymore', 'anyways', 'apart', 'apparently', 'anytime',\n",
    "                       'appropriate',  'approximately', 'arent', 'behold', 'better', 'bravo','briefly','bad','best','brilliant',\n",
    "                       'bye', 'cant', 'certainly', 'chrissakes', 'clearly', 'completely',\n",
    "                       'congrat', 'congrats','congratulation', 'congratulations', 'consequently', 'cool', 'couldnt',\n",
    "                       'darnit', 'de','dear', 'definitely','disappointing', 'didn', 'doesn', 'don', 'downwards',\n",
    "                       'disgusting','dude','down','eg',\"e.g.\",'i.e.',\n",
    "                       'encore','entirely', 'especially', 'et', 'etc', 'everybody', 'ex', 'exactly', 'excellent',\n",
    "                       'fantastic','far', 'farewell','funny',\n",
    "                       'felicitation', 'felicitations','finally', 'fully','furthermore', 'gadzooks', \n",
    "                       'good', 'goodby','goodness', 'gracious', 'great', \n",
    "                       'greetings', 'hallo', 'hardly', 'hasnt', 'haven', 'hello', 'here','hi', 'hither','higher','hopefully',\n",
    "                       'here','there','including',\n",
    "                       'howbeit', 'ie', 'immediately', 'inasmuch', 'inner', 'insofar', 'instead', 'inward', 'important',\n",
    "                       'indeed','just', \"it'd\", \"it'll\", 'inside','kertyschoo', 'kg', 'km', 'lackaday', \n",
    "                       'largely', 'lately', 'later','lovely',\n",
    "                       'lest', 'let', 'lets', 'likely', 'little', 'ltd', 'lower','magnificent', 'mainly', 'marvelous',\n",
    "                       'myself','yourself','yourselves','himself','herself','hisself','ourselves','themsleves',\n",
    "                       'maybe', 'meantime', 'merely', 'minus', 'near', 'nearly', 'necessary', 'never', \n",
    "                       'non', 'normally', 'obviously', 'ok', 'okay', 'ones', 'outside', 'over','other','others','only',\n",
    "                       'overall', 'particular', 'particularly', 'please', 'plus', 'poorly', 'possible','up',\n",
    "                       'possibly', 'potentially', 'predominantly', 'presumably', 'previously','primarily', 'probably',\n",
    "                       'promising',\n",
    "                       'promptly', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref',\n",
    "                       'refs', 'regardless', 'related', 'relatively', 'respectively', 'resulting', 'right', 'sec', \n",
    "                       'secondly','self', 'selves', 'seriously', 'shall', 'shucks','somebody', 'somethan','sorry',\n",
    "                       'somewhat', 'soon', 'late' , 'sorry', 'stupid', 'sub', 'substantially', 'successfully', 'sufficiently',\n",
    "                       \n",
    "                       'super', 'sure', \"t's\", 'th', 'thank', 'thanks', 'thanx', \"that've\", 'thats', 'there', \"there'll\",\n",
    "                       \"there've\", 'thered', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyre', 'thorough',\n",
    "                       'then','thankfully','too','today','yesterday','tomorrow','night',\"morning\",'afternoon','noon','tonight',\n",
    "                       'evening','day','everyday', 'everynight','todays','nights','mornings','noons','afternoons','days',\n",
    "                       'evenings','week','month','year',\n",
    "                       'thoroughly', 'tnx', 'too','truly', 'twice', 'undoubtedly','unfortunately', 'unlike','unlikely',\n",
    "                       'unto',  'usually', 'vs', 'welcome', 'well', 'went', 'werent', 'what', 'whatever', 'wheres', 'widely',\n",
    "                       'wonderful', 'wont', 'wouldnt', 'wrong', 'worst','worse','www', 'yes', 'youd', 'youre', 'yummy', \n",
    "                       'zoinks','shit','literally','literal','pleasure','effective','fabulous','delighted',\n",
    "                       'saturday','sunday','monday','tuesday','wednesday','thursday', 'friday','past','future','suitable',\n",
    "                       'much','many','less','least','few','lots','lot','fewer','fewset','therefore','pm',\n",
    "                       'afaik', 'br', 'idk','smh','qotd', 'ftw','bfn','yw', 'icymi','fomo','smdh', 'b4','imho',\n",
    "                       'urdddd','fab' ,'delightful','absolute','pleasure','huge','latest','nowadays',\n",
    "                       'january','february','april','june','july','august','september','october',\n",
    "                       'november','december', 'autumn' ,'spring','winter','summer',\n",
    "                       'mr','madam','sir','mrs','easy', 'difficult','mr,'\n",
    "                       'weekend','south','north','west','east','asia','africa','europe','america','totally',\n",
    "                       'come', 'comes', 'coming', 'came', 'seems', 'gives', 'gave', 'makes', 'made', 'keeps', 'kept', \n",
    "                       'calls', 'called', 'says', 'saying', 'said', 'goes', 'went', 'gone', 'got', 'saw', 'seen', 'shows',\n",
    "                       'shown', 'took', 'taken', 'uses', 'moved', 'moves', 'puts',\n",
    "                       'using','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "                       'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use',\n",
    "                       'find', 'finds', 'finding','aka',\n",
    "                       'lol' , 'brb', 'lmk', 'ama', 'tbh', 'irl', \"tl;dr\", 'fml', 'bfn' ,' br', 'ht', \"hth\",'j/k', 'lmao' ] #cool\n",
    "                        # small big large useful \n",
    "\n",
    "interjection_remove=['aaaahh', 'aaah', 'aaargh', 'aaay', 'aagh', 'aah',\n",
    "                   'aargh', 'achoo', 'adios', 'ah', 'aha', 'ahem', 'ahh', 'ahhh',\n",
    "                   'ahoy', 'alas', 'allo', 'amen', 'areet', 'argh', 'arrggh',\n",
    "                   'arrividerci', 'asap', 'attaboy', 'avaunt', 'aw', 'aw', 'aww',\n",
    "                   'awww', 'ay', 'ay', 'aye', 'ayeaugh', 'bada', 'badum', 'bah',\n",
    "                   'bahaha', 'bam', 'bazinga', 'behold', 'bingce', 'bingo', 'blah',\n",
    "                   'blech', 'bleh', 'blimey', 'bonjour', 'boo', 'booh', 'boohoo',\n",
    "                   'booyah', 'bravo', 'brr', 'brrrr', 'btw', 'bwahaha', 'capeesh',\n",
    "                   'capisce', 'cheerio', 'cheers', 'ciao', 'cor', 'cowabunga',\n",
    "                   'crikey', 'cripes', 'da', 'dabba', 'dah', 'dammit', 'damn', 'dang',\n",
    "                   'darn', 'de', 'dee', 'di', 'dizamn', 'doh', 'doo', 'drat', 'duh',\n",
    "                   'dum', 'eeeek', 'eek', 'eep', 'egad', 'egads', 'eh', 'ehem', 'em',\n",
    "                   'er', 'eureka', 'eww', 'ewww', 'eyh', 'fiddledeedee', 'fie',\n",
    "                   'fore', 'foul', 'fuff', 'gah', 'gak', 'gee', 'geez', 'gesundheit',\n",
    "                   'giddyap', 'golly', 'gosh', 'grr', 'grrrr', 'ha', 'hah', 'haha',\n",
    "                   'hahaha', 'hallelujah', 'halloa', 'harrumph', 'harumph', 'haw',\n",
    "                   'heck', 'heck', 'heeey', 'heh', 'hehe', 'hey', 'hhh', 'hic', 'hm',\n",
    "                   'hmm', 'hmmm', 'hmmmm', 'hmmph', 'hmpf', 'ho', 'hola', 'hoo',\n",
    "                   'hooray', 'howdy', 'hrmm', 'hrmph', 'hrmph', 'hrrmph', 'hu', 'huh',\n",
    "                   'hullo', 'humph', 'hurrah', 'huzza', 'huzzah', 'ich', 'ick',\n",
    "                   'ixnay', 'jeepers', 'jeez', 'kaboom', 'kapow', 'kerwham', 'la',\n",
    "                   'lala', 'lo', 'lordy', 'meh', 'mhm', 'ml', 'mm', 'mmh', 'mmhm',\n",
    "                   'mmm', 'muahaha', 'mwah', 'mwahaha', 'na','nay','nah', 'nanu', 'nooo', 'nope',\n",
    "                   'nuh', 'oh', 'ohh', 'oho', 'oi', 'okeydoke', 'om', 'oof', 'ooh',\n",
    "                   'oomph', 'oooh', 'ooooh', 'oops', 'ouch', 'ow', 'oww', 'oy',\n",
    "                   'oyez', 'oyh', 'pew', 'pff', 'pffh', 'pfft', 'phew', 'phut',\n",
    "                   'phweep', 'phwoar', 'phwoarr', 'poof', 'poogh', 'prethee',\n",
    "                   'prithee', 'prosit', 'pssh', 'psst', 'queep', 'roger', 'salaam',\n",
    "                   'salam', 'sheesh', 'shh', 'shhh', 'shitfire', 'shoo', 'shoop',\n",
    "                   'shush', 'sigh', 'sssh', 'strewth', 'ta', 'tarnations', 'tchah',\n",
    "                   'teehee', 'tish', 'touché', 'tsk', 'tss', 'tut', 'uggh', 'ugh',\n",
    "                   'uh', 'uhh', 'uhm', 'um', 'umm', 'ummm', 'umph', 'unh', 'upadaisy',\n",
    "                   'upsadaisy', 'ur', 'urgh', 'vay', 'vayf', 'viva', 'voila', 'waa',\n",
    "                   'waaaaah', 'waah', 'wah', 'wahey', 'wassup', 'weee', 'welp',\n",
    "                   'wham', 'whamo', 'whee', 'whew', 'whizz', 'whoa',\n",
    "                   'whoo', 'whoopee','whoop', 'whoops', 'whoopsy', 'whoosh', 'woah', 'woo',\n",
    "                   'woohoo', 'wotcha', 'wotcher', 'wow', 'wowsers', 'wowsers',\n",
    "                   'wuzzup', 'wuzzup', 'wuzzup', 'ya', 'yabba', 'yada', 'yadda',\n",
    "                   'yak', 'yarooh', 'yay', 'yea', 'yeah', 'yech', 'yee', 'yeeeeaah',\n",
    "                   'yeehaw', 'yeow', 'yes', 'yessiree', 'yew', 'yikes', 'yippee',\n",
    "                   'yo', 'yoo', 'yoohoo', 'yow', 'yowza', 'yuck', 'yuh', 'zing',\n",
    "                   'zoiks', 'zomfg', 'zomg', 'zounds', 'zut']\n",
    "             \n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "type(spacy_stopwords)\n",
    "#spacy_exclude=['using','name','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "#               'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use']# serious\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words1 = get_stop_words('english')\n",
    "#print(type(stop_words1))\n",
    "#print()\n",
    "#print(stop_words1)\n",
    "lib_stopwords=set(stop_words1)\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#print(type(ENGLISH_STOP_WORDS))\n",
    "#print()\n",
    "#print(set(ENGLISH_STOP_WORDS))\n",
    "#sklearn_exclude=['find','get','found','go','see','seem','seems','give','seemed','take','keep','show','put','made'] # system  cry\n",
    "sklearn_stopwords=set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "#spacy_stopwords.difference_update(set(spacy_exclude))\n",
    "#sklearn_stopwords.difference_update(set(sklearn_stopwords))\n",
    "#for removing \"just\" one item, use \"remove\"\n",
    "temp_1=set([])\n",
    "#######################\n",
    "temp_1.update(nltk_stopwords)\n",
    "temp_1.update(lib_stopwords)\n",
    "temp_1.update(sklearn_stopwords)\n",
    "temp_1.update(spacy_stopwords)\n",
    "temp_1.update(set(english_alghabet))\n",
    "temp_1.update(set(numbers_remove)) \n",
    "#temp_1.update(set(miscellaneous_remove))\n",
    "temp_1.update(set(interjection_remove))\n",
    "temp_1.update(['rt','be','will','was','were','is','am','are','have','has','had','do','does','done'])\n",
    "###              ,'thing'  ,'want','getting', 'looking','way'])\n",
    "###temp_1.update(['rt','like','look','get','take'])\n",
    "cachedStopWords=temp_1\n",
    "len(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "#print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "#print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "#print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    \n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    \n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)rt ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "    text=' '.join(text)\n",
    "    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank break break like book thank'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(\"thanks I broke breaking like the books thanks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningA (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower \n",
    "    \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"'\", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    #text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    #text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    #text=re.sub('Dollar|Dollars|Yen|Yens|Euros', ' money ', text)   # not euro \n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    text = re.sub(r'(HTTP://|HTTPS://)\\S+', '', text)\n",
    "\n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    text=re.sub(r'WWW\\.\\S+', '', text)\n",
    "\n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "\n",
    "    #removing common expressions\n",
    "    '''\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    '''\n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    #text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    #text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "    #text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "    #text=re.sub(\"\\+\",' + ', text)\n",
    "    #text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"United States\",'USA', text)\n",
    "    text=re.sub(\"United Kingdom\",'UK', text)\n",
    "    text=re.sub(\" the US \",' USA ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"U\\.S\\.A\", 'USA', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.\\.)+\", ' ',text)   \n",
    "\n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "#    text=re.sub('#', ' ', text)  \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    '''\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    '''\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)     \n",
    "    \n",
    "    text=re.sub('(^)RT | RT ','',text)    # if we do not want to remove stopwords\n",
    "\n",
    "#    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "#    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "#    text=' '.join(text)\n",
    "#    text=re.sub(\"''\",'''\"''', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "#    text=re.sub(\"``\",'''\"''', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_punc(text):\n",
    "    '''\n",
    "    text=re.sub(\"looking forward|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text) \n",
    "    '''\n",
    "#    text= re.sub(\"[\\\"\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘';•«»,@:~!\\=%&]+\", ' ',text)  #except _ -\n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", ' ',text) #except  \\- _\n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    '''\n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    text= re.sub(\">\", ' ',text)\n",
    "    text= re.sub(\"<\", ' ',text)\n",
    "    text= re.sub(\" - \", ' ',text)\n",
    "    text= re.sub(\" --\", ' ',text)\n",
    "    '''\n",
    "    \n",
    "#    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "\n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47604376, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "Number of tweets in ent: 47604376\n"
     ]
    }
   ],
   "source": [
    "#df_ent = pd.read_csv('/archives1/Datasets/TweetsWorld/ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_ent0 = pd.read_csv('ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_ent0.shape)\n",
    "print(df_ent0.columns)\n",
    "#print(df_ent0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_ent=list(df_ent0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_ent= list(zip(df_ent0.user_id, df_ent.tweet))\n",
    "#rows_ent=df_ent0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_ent = df_ent0[['tweet','tweet_created_at']]   \n",
    "\n",
    "\n",
    "del df_ent0\n",
    "\n",
    "print(\"Number of tweets in ent:\",len(df_ent))  #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Memory size of ent:',sys.getsizeof(df_ent)) #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_mng0 = pd.read_csv('mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#df_mng0 = pd.read_csv('/archives1/Datasets/TweetsWorld/mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "print(df_mng0.shape)\n",
    "print(df_mng0.columns)\n",
    "#print(df_mng0.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "#rows_mng=list(df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']].itertuples(index=False, name=None)) #rows_mng0\n",
    "#rows_mng= list(zip(df_mng0.user_id, df_mng.tweet))\n",
    "#rows_mng=df_mng0[['user_id','tweet']].apply(tuple, axis=1) \n",
    "df_mng = df_mng0[['user_id', 'tweet','tweet_created_at','location_profile']]\n",
    "\n",
    "del df_mng0\n",
    "\n",
    "print(len(df_mng)) #rows_mng0\n",
    "print('memry size of mng:', sys.getsizeof(df_mng)) #rows_mng0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./years/ent_cleaning_classification.txt\", \"rb\") as fp:  \n",
    "#    ent_tweets_rows=pickle.load(fp)\n",
    "#len(ent_tweets_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind_2019=pickle.load(fp)\n",
    "\n",
    "len(ent_year_ind_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"_ent_year_ind.txt\", \"rb\") as fp:   \n",
    "    ent_year_ind_2017=pickle.load(fp)\n",
    "\n",
    "len(ent_year_ind_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2019'\n",
    "with open(\"./years/\"+year+\"result_ent_vote.txt\", \"rb\") as fp:  \n",
    "    vote_19=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_ent_label.txt\", \"rb\") as fp:  \n",
    "    label_19=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_2019_rel=[]\n",
    "ent_2019_irrel=[]\n",
    "for i , k in enumerate(ent_year_ind_2019):\n",
    "\n",
    "    if '2019'  not in df_ent['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_19[i]==1:\n",
    "        ent_2019_rel.append(k)\n",
    "    else:\n",
    "        ent_2019_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892698, 1107302)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_2019_rel), len(ent_2019_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2017'\n",
    "with open(\"./years/\"+year+\"result_ent_vote.txt\", \"rb\") as fp:  \n",
    "    vote_17=pickle.load(fp)\n",
    "\n",
    "with open(\"./years/\"+year+\"result_ent_label.txt\", \"rb\") as fp:  \n",
    "    label_17=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_2017_rel=[]\n",
    "ent_2017_irrel=[]\n",
    "for i , k in enumerate(ent_year_ind_2017):\n",
    "    if '2017' not in df_ent['tweet_created_at'][k]:\n",
    "        print(k)\n",
    "    if label_17[i]==1:\n",
    "        ent_2017_rel.append(k)\n",
    "    else:\n",
    "        ent_2017_irrel.append(k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063213, 936787)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_2017_rel), len(ent_2017_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ent['tweet_created_at'][ent_2017_rel[0]], df_ent['tweet'][ent_2017_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509.71567845344543"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0=time.time()\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "#X_train_test=[]\n",
    "\n",
    "tweet_vectors_19=np.zeros([len(ent_2019_rel),embedding_dim],dtype='float32')\n",
    "\n",
    "for i,index in enumerate(ent_2019_rel):\n",
    "    tweet=df_ent['tweet'][index]\n",
    "    \n",
    "    #tweet='I like it.'\n",
    "    text=cleaning(tweet) # the result for cleaning and cleaningA is different \n",
    "\n",
    "    #text=cleaning_punc(text)\n",
    "    #text=cleaning_numbers(text)\n",
    "#train_texts_tokens=[]\n",
    "#for text in train_texts:\n",
    "#    tokens=text.split()\n",
    "    #tokens=nltk.word_tokenize(text)\n",
    "    tokens=tknzr.tokenize(text)\n",
    "    #train_texts_tokens.append(tokens)\n",
    "    \n",
    "#print(len(train_texts_tokens))\n",
    "\n",
    "#for kk,tokens in enumerate(train_texts_tokens):\n",
    "    if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "        a1=[]\n",
    "        num_tok=0\n",
    "        for token in tokens:\n",
    "            num_tok +=1\n",
    "            a1.append( ft.get_word_vector(token) )\n",
    "                    \n",
    "        #print(a1)\n",
    "        #print()\n",
    "        #temp1=np.sum(a1,axis=0)/num_tok\n",
    "        #print(temp1)\n",
    "        #print()\n",
    "        temp=np.mean(a1,axis=0)\n",
    "        #print(abc-temp1)\n",
    "        \n",
    "        \n",
    "        #if len(tokens)>max_tokens:\n",
    "        #    temp=a1[0:max_tokens]\n",
    "        #elif len(tokens)==max_tokens:\n",
    "        #    temp=a1\n",
    "        #else: # if len(tokens)<max_tokens:\n",
    "        #    temp[0:len(tokens)]=a1\n",
    "        #X_train_test.append(temp) \n",
    "        tweet_vectors_19[i]=temp\n",
    "\n",
    "    else:\n",
    "        #print(index)\n",
    "        #print(tweet)\n",
    "        #print('strange')\n",
    "        temp=np.zeros([1,embedding_dim])\n",
    "#        X_train_test.append(temp)\n",
    "        tweet_vectors_19[i]=temp\n",
    "        \n",
    "time.time()-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892698, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_umap = umap.UMAP().fit_transform(tweet_vectors_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.665408372879028\n",
      "7.667405128479004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "t0=time.time()\n",
    "\n",
    "#kpca = KernelPCA(n_components=2, kernel='linear')\n",
    "#X_kpca = kpca.fit_transform(tweet_vectors_19)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2019 = pca.fit_transform(tweet_vectors_19)\n",
    "\n",
    "print(time.time()-t0)\n",
    "\n",
    "#tsne = TSNE()\n",
    "#X_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892698, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = LocallyLinearEmbedding(n_components=2)\n",
    "#X_transformed = embedding.fit_transform(tweet_vectors_19)\n",
    "#X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "x_svd=svd.fit_transform(tweet_vectors_19)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "x_svd.shape\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = Isomap(n_components=2)\n",
    "#X_transformed_iso = embedding.fit_transform(tweet_vectors_19)\n",
    "#X_transformed_iso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.manifold import MDS\n",
    "embedding = MDS(n_components=2)\n",
    "X_transformed = embedding.fit_transform(tweet_vectors_19)\n",
    "X_transformed.shape\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer_19 = hdbscan.HDBSCAN(min_cluster_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.64150881767273\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "clusterer_19.fit(X_pca2019)\n",
    "#clusterer_19.fit(x_svd)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer_19.labels_\n",
    "clusterer_19.labels_[0:10]\n",
    "clusterer_19.labels_.max()\n",
    "np.unique(clusterer_19.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of tweets in each cluster for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725778\n",
      "543\n",
      "671\n",
      "541\n",
      "743\n",
      "6812\n",
      "993\n",
      "509\n",
      "551\n",
      "1220\n",
      "1283\n",
      "852\n",
      "594\n",
      "949\n",
      "686\n",
      "688\n",
      "795\n",
      "1199\n",
      "780\n",
      "1446\n",
      "554\n",
      "655\n",
      "1080\n",
      "1125\n",
      "141651\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "cluster_index=[None]*(clusterer_19.labels_.max()+1)\n",
    "#display(cluster_index)\n",
    "\n",
    "for i in range(-1,clusterer_19.labels_.max()+1):\n",
    "    aa1=np.where(clusterer_19.labels_ == i)\n",
    "    if i!=-1:\n",
    "        cluster_index[i]=aa1[0]\n",
    "    print(len(aa1[0]))\n",
    "\n",
    "print('----')\n",
    "#len(cluster_index[4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_index[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=cachedStopWords\n",
    "stop.update(set(english_alghabet))\n",
    "stop.update(set(numbers_remove))\n",
    "stop.update(set(['rt','u']))\n",
    "\n",
    "exclude_punc = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "def clean(doc):\n",
    "    \n",
    "    doc=cleaning(doc)\n",
    "    doc=cleaning_punc(doc)\n",
    "    doc=cleaning_numbers(doc)\n",
    "    doc= re.sub(\" 00am | 00pm | pm | am \", '',doc)\n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() re.sub(\"(\\d+)\", '',i))\n",
    "\n",
    "    stop_free_=[]                      \n",
    "    for i in stop_free.lower().split():\n",
    "                          \n",
    "        if not i.isdigit():\n",
    "            stop_free_.append(i)\n",
    "                          \n",
    "    stop_free = \" \".join(stop_free_)\n",
    "\n",
    "    #text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude_punc)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    cleaned = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Topics and thier words for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for ii in range(0,clusterer_19.labels_.max()+1):\n",
    "    print(\"cluster index : \",ii)\n",
    "\n",
    "    set_x=[]\n",
    "    for i in cluster_index[ii]:\n",
    "        text=df_ent.tweet[ent_2019_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 1, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whoa! @Tesla’s 40.2.1 now shows your cars name when connecting though Bluetooth. Nice touch, @elonmusk and team! https://t.co/QMeOJE10PU'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ent.tweet[ent_2019_rel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316821"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "316821"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['work' 'people' 'good' 'life' 'time' 'success' 'thing' 'way' 'business'\n",
      " 'need' 'change' 'learn' 'goal' 'entrepreneur' 'know' 'want' 'help'\n",
      " 'world' 'think' 'great']\n",
      "----------\n",
      "Topic 2 ['trump' 'agree' 'people' 'president' 'true' 'news' 'law' 'country' 'know'\n",
      " 'point' 'need' 'right' 'medium' 'good' 'fact' 'truth' 'money' 'usa'\n",
      " 'think' 'government']\n",
      "----------\n",
      "Topic 3 ['post' 'new' 'video' 'check' 'add' 'design' 'fashion' 'link' 'shop'\n",
      " 'poshmark' 'shopmycloset' 'available' 'facebook' 'realestate' 'order'\n",
      " 'closet' 'construction' 'style' 'code' 'home']\n",
      "----------\n",
      "Topic 4 ['nigeria' 'vote' 'state' 'india' 'election' 'president' 'nigerian'\n",
      " 'minister' 'government' 'country' 'people' 'buhari' 'south' 'police'\n",
      " 'govt' 'know' 'party' 'indian' 'pakistan' 'leader']\n",
      "----------\n",
      "Topic 5 ['join' 'event' 'new' 'day' 'today' 'meet' 'come' 'business' 'week'\n",
      " 'great' 'thank' 'excite' 'year' 'city' 'host' 'tomorrow' 'ticket'\n",
      " 'conference' 'open' 'free']\n",
      "----------\n",
      "Topic 6 ['startup' 'ai' 'business' 'tech' 'technology' 'new' 'innovation'\n",
      " 'entrepreneur' 'data' 'company' 'need' 'read' 'problem' 'digital' 'late'\n",
      " 'help' 'learn' 'fund' 'work' 'money']\n",
      "----------\n",
      "Topic 7 ['thank' 'share' 'great' 'job' 'check' 'idea' 'learn' 'support' 'student'\n",
      " 'podcast' 'good' 'work' 'course' 'opportunity' 'new' 'article'\n",
      " 'appreciate' 'need' 'apply' 'interview']\n",
      "----------\n",
      "Topic 8 ['money' 'pay' 'tax' 'year' 'bitcoin' 'bank' 'business' 'price' 'time'\n",
      " 'new' 'need' 'buy' 'trade' 'market' 'company' 'people' 'good' 'cost'\n",
      " 'invest' 'work']\n",
      "----------\n",
      "Topic 9 ['follow' 'market' 'business' 'thank' 'social' 'medium' 'content' 'brand'\n",
      " 'help' 'tip' 'late' 'instagram' 'check' 'online' 'facebook' 'email'\n",
      " 'free' 'blog' 'new' 'need']\n",
      "----------\n",
      "Topic 10 ['book' 'team' 'congratulation' 'award' 'thank' 'congrats' 'new' 'winner'\n",
      " 'bos' 'win' 'great' 'player' 'read' 'league' 'write' 'champion' 'late'\n",
      " 'author' 'copy' 'preach']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    set_x=[]\n",
    "    for i in range(0,len(ent_2019_rel)):\n",
    "        #print(ent_2019_rel[i])\n",
    "        text=df_ent.tweet[ent_2019_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 10, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: 82892,\n",
       " 8: 139139,\n",
       " 7: 47692,\n",
       " 2: 130684,\n",
       " 1: 158348,\n",
       " 9: 90274,\n",
       " 4: 78162,\n",
       " 5: 98345,\n",
       " 3: 37786,\n",
       " 10: 29376}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To view what topics are assigned to the douments:\n",
    "\n",
    "doc_topic = lda_model.transform(tf_idf_arr)  \n",
    "topic_num=dict()\n",
    "\n",
    "# iterating over ever value till the end value\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    \n",
    "    # argmax() gives maximum index value\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    \n",
    "    if topic_doc+1 in topic_num:\n",
    "        topic_num[topic_doc+1] +=1\n",
    "    else:\n",
    "        topic_num[topic_doc+1]=1\n",
    "        \n",
    "        \n",
    "    \n",
    "    # document is n+1  \n",
    "    #print (\"Document\", n+1, \" -- Topic:\" ,topic_doc)\n",
    "\n",
    "topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusterer_optics = OPTICS(min_samples=5).fit(X_pca)\n",
    "#clusterer_dbscan = DBSCAN(eps=5, min_samples=2).fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#kmeans = KMeans(n_clusters=3) \n",
    "#ent_kmeans_clustering = kmeans.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592.3119382858276"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0=time.time()\n",
    "embedding_dim=300\n",
    "max_tokens=50\n",
    "#X_train_test=[]\n",
    "\n",
    "tweet_vectors_17=np.zeros([len(ent_2017_rel),embedding_dim],dtype='float32')\n",
    "\n",
    "for i,index in enumerate(ent_2017_rel):\n",
    "    tweet=df_ent['tweet'][index]\n",
    "    \n",
    "    #tweet='I like it.'\n",
    "    text=cleaning(tweet) # the result for cleaning and cleaningA is different \n",
    "    #text=cleaning_punc(text)\n",
    "    #text=cleaning_numbers(text)\n",
    "#train_texts_tokens=[]\n",
    "#for text in train_texts:\n",
    "#    tokens=text.split()\n",
    "    #tokens=nltk.word_tokenize(text)\n",
    "    tokens=tknzr.tokenize(text)\n",
    "    #train_texts_tokens.append(tokens)\n",
    "    \n",
    "#print(len(train_texts_tokens))\n",
    "\n",
    "#for kk,tokens in enumerate(train_texts_tokens):\n",
    "    if tokens !=\"\" and tokens!=' ' and  tokens!=[]:   \n",
    "        a1=[]\n",
    "        num_tok=0\n",
    "        for token in tokens:\n",
    "            num_tok +=1\n",
    "            a1.append( ft.get_word_vector(token) )\n",
    "                    \n",
    "        #print(a1)\n",
    "        #print()\n",
    "        #temp1=np.sum(a1,axis=0)/num_tok\n",
    "        #print(temp1)\n",
    "        #print()\n",
    "        temp=np.mean(a1,axis=0)\n",
    "        #print(abc-temp1)\n",
    "        \n",
    "        \n",
    "        #if len(tokens)>max_tokens:\n",
    "        #    temp=a1[0:max_tokens]\n",
    "        #elif len(tokens)==max_tokens:\n",
    "        #    temp=a1\n",
    "        #else: # if len(tokens)<max_tokens:\n",
    "        #    temp[0:len(tokens)]=a1\n",
    "        #X_train_test.append(temp) \n",
    "        tweet_vectors_17[i]=temp\n",
    "\n",
    "    else:\n",
    "        #print(index)\n",
    "        #print(tweet)\n",
    "        #print('strange')\n",
    "        temp=np.zeros([1,embedding_dim])\n",
    "#        X_train_test.append(temp)\n",
    "        tweet_vectors_17[i]=temp\n",
    "        \n",
    "time.time()-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063213, 300)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tweet_vectors_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.745985984802246\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2017 = pca.fit_transform(tweet_vectors_17)\n",
    "\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer_17 = hdbscan.HDBSCAN(min_cluster_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.10701513290405\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "clusterer_17.fit(X_pca2017)\n",
    "#clusterer.fit(x_svd)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "       33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
       "       50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer_17.labels_\n",
    "\n",
    "clusterer_17.labels_[0:10]\n",
    "\n",
    "clusterer_17.labels_.max()\n",
    "\n",
    "np.unique(clusterer_17.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of tweets in each cluster for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905613\n",
      "863\n",
      "595\n",
      "848\n",
      "1701\n",
      "1427\n",
      "1408\n",
      "4136\n",
      "54532\n",
      "577\n",
      "541\n",
      "596\n",
      "1174\n",
      "625\n",
      "552\n",
      "1442\n",
      "1658\n",
      "3972\n",
      "877\n",
      "1061\n",
      "643\n",
      "797\n",
      "527\n",
      "695\n",
      "934\n",
      "687\n",
      "520\n",
      "562\n",
      "756\n",
      "935\n",
      "2329\n",
      "533\n",
      "1317\n",
      "2284\n",
      "1226\n",
      "628\n",
      "751\n",
      "1778\n",
      "887\n",
      "712\n",
      "661\n",
      "973\n",
      "1222\n",
      "761\n",
      "1224\n",
      "650\n",
      "1110\n",
      "1025\n",
      "681\n",
      "796\n",
      "1797\n",
      "2732\n",
      "1905\n",
      "1863\n",
      "1123\n",
      "1197\n",
      "1905\n",
      "2534\n",
      "799\n",
      "3665\n",
      "578\n",
      "1400\n",
      "843\n",
      "1030\n",
      "27040\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "cluster_index=[None]*(clusterer_17.labels_.max()+1)\n",
    "#display(cluster_index)\n",
    "\n",
    "for i in range(-1,clusterer_17.labels_.max()+1):\n",
    "    aa1=np.where(clusterer_17.labels_ == i)\n",
    "    if i!=-1:\n",
    "        cluster_index[i]=aa1[0]\n",
    "    print(len(aa1[0]))\n",
    "\n",
    "print('----')\n",
    "#len(cluster_index[4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics and thier words for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for ii in range(0,clusterer_17.labels_.max()+1):\n",
    "    print(\"cluster index : \",ii)\n",
    "\n",
    "    set_x=[]\n",
    "    for i in cluster_index[ii]:\n",
    "        text=df_ent.tweet[ent_2017_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2017_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 1, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303322"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "303322"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['trump' 'money' 'bitcoin' 'president' 'tax' 'news' 'usa' 'people'\n",
      " 'country' 'good' 'need' 'vote' 'government' 'india' 'pay' 'new' 'know'\n",
      " 'time' 'state' 'year']\n",
      "----------\n",
      "Topic 2 ['follow' 'thank' 'connect' 'link' 'recent' 'money' 'agree' 'happy'\n",
      " 'great' 'check' 'want' 'video' 'hi' 'click' 'free' 'appreciate' 'run'\n",
      " 'quickly' 'startup' 'libra']\n",
      "----------\n",
      "Topic 3 ['thank' 'new' 'great' 'join' 'event' 'share' 'business' 'today' 'video'\n",
      " 'day' 'week' 'startup' 'entrepreneur' 'meet' 'post' 'come' 'team'\n",
      " 'excite' 'community' 'support']\n",
      "----------\n",
      "Topic 4 ['market' 'business' 'social' 'medium' 'content' 'tip' 'seo' 'socialmedia'\n",
      " 'startup' 'new' 'brand' 'late' 'use' 'google' 'digital' 'sale' 'strategy'\n",
      " 'facebook' 'tool' 'thank']\n",
      "----------\n",
      "Topic 5 ['book' 'new' 'ai' 'capricorn' 'machinelearning' 'today' 'available'\n",
      " 'read' 'write' 'iphone' 'good' 'money' 'free' 'author' 'datascience'\n",
      " 'apple' 'learn' 'suesparrow' 'mean' 'bigdata']\n",
      "----------\n",
      "Topic 6 ['new' 'photo' 'post' 'city' 'energy' 'money' 'world' 'true' 'travel'\n",
      " 'car' 'facebook' 'startup' 'solar' 'late' 'york' 'year' 'oil' 'build'\n",
      " 'home' 'water']\n",
      "----------\n",
      "Topic 7 ['thank' 'check' 'follow' 'late' 'thx' 'crowdfunding' 'pisces' 'people'\n",
      " 'automatically' 'aries' 'daily' 'work' 'twist' 'party' 'join' 'gemini'\n",
      " 'support' 'washington' 'lisa' 'wo']\n",
      "----------\n",
      "Topic 8 ['business' 'work' 'entrepreneur' 'success' 'good' 'time' 'people' 'life'\n",
      " 'learn' 'need' 'way' 'thing' 'change' 'money' 'start' 'leadership' 'want'\n",
      " 'help' 'goal' 'great']\n",
      "----------\n",
      "Topic 9 ['follower' 'new' 'nigeria' 'quote' 'problem' 'people' 'africa' 'need'\n",
      " 'today' 'day' 'stats' 'work' 'help' 'power' 'know' 'week' 'unfollowers'\n",
      " 'peace' 'nigerian' 'sir']\n",
      "----------\n",
      "Topic 10 ['read' 'new' 'shoutout' 'check' 'growthhacking' 'money' 'solution'\n",
      " 'cybersecurity' 'intelligence' 'follow' 'artificial' 'business' 'time'\n",
      " 'aquarius' 'security' 'valley' 'tweet' 'stock' 'silicon' 'today']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "    set_x=[]\n",
    "    for i in range(0,len(ent_2017_rel)):\n",
    "        #print(ent_2017_rel[i])\n",
    "        text=df_ent.tweet[ent_2017_rel[i]]\n",
    "        #print(text)\n",
    "        #print(df_ent['tweet'][ent_2019_rel[i]])\n",
    "        #print('')\n",
    "        set_x.append(text)\n",
    "\n",
    "    #len(set_x),type(set_x)\n",
    "\n",
    "    # clean data stored in a new list\n",
    "    cleaned_set = [clean(doc).split() for doc in set_x] \n",
    "\n",
    "    #len(cleaned_set)\n",
    "    \n",
    "\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    # Converting text into numerical representation\n",
    "    tf_idf_arr = tf_idf_vectorizer.fit_transform(cleaned_set)\n",
    "    # Creating vocabulary array which will represent all the corpus \n",
    "    vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "    display(len(vocab_tf_idf))\n",
    "\n",
    "\n",
    "    cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    cv_arr = cv_vectorizer.fit_transform(cleaned_set)\n",
    "    vocab_cv = cv_vectorizer.get_feature_names()\n",
    "    display(len(vocab_cv))\n",
    "    \n",
    "    \n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components = 10, max_iter = 20, random_state = 20)\n",
    "    X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "    topic_words = lda_model.components_\n",
    "    \n",
    "    \n",
    "\n",
    "    #  Define the number of Words that we want to print in every topic : n_top_words\n",
    "    n_top_words = 21\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "        # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "        sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "        # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "        topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "        # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "        # obtaining topics + words\n",
    "        # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "        topic_words = topic_words[:-n_top_words:-1]\n",
    "        print (\"Topic\", str(i+1), topic_words)\n",
    "        \n",
    "        print('----------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 54373,\n",
       " 3: 179367,\n",
       " 1: 179395,\n",
       " 6: 78715,\n",
       " 10: 46261,\n",
       " 7: 39935,\n",
       " 4: 148022,\n",
       " 8: 206749,\n",
       " 9: 90881,\n",
       " 2: 39515}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To view what topics are assigned to the douments:\n",
    "\n",
    "doc_topic = lda_model.transform(tf_idf_arr)  \n",
    "topic_num=dict()\n",
    "\n",
    "# iterating over ever value till the end value\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    \n",
    "    # argmax() gives maximum index value\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    \n",
    "    if topic_doc+1 in topic_num:\n",
    "        topic_num[topic_doc+1] +=1\n",
    "    else:\n",
    "        topic_num[topic_doc+1]=1\n",
    "        \n",
    "        \n",
    "    \n",
    "    # document is n+1  \n",
    "    #print (\"Document\", n+1, \" -- Topic:\" ,topic_doc)\n",
    "\n",
    "topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment codes (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def noun_phrases(text):\n",
    "    # noun phrase by nltk\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "    grammar = \"\"\"NP: {<DT>?<JJ>*<NN.*>+}\"\"\"\n",
    "\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    #for i in sentences:\n",
    "    #    result = cp.parse(i)\n",
    "    #    print(result)\n",
    "    #    print(type(result))\n",
    "        #result.draw() \n",
    "   # noun_phrases_list = [[' '.join(leaf[0].lower() for leaf in tree.leaves()) \n",
    "\n",
    "    noun_phrases_list = [[' '.join(leaf[0] for leaf in tree.leaves()) \n",
    "                          for tree in cp.parse(sent).subtrees() \n",
    "                          if tree.label()=='NP'] \n",
    "                          for sent in sentences]\n",
    "\n",
    "\n",
    "    phrases_all=[]\n",
    "    for i in noun_phrases_list:\n",
    "        phrases_all.extend(i)\n",
    "    \n",
    "    return phrases_all\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def noun_phrases1(text):\n",
    "    # noun phrase by Spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    phrases_all=[]\n",
    "    for phrase in doc.noun_chunks:\n",
    "        phrases_all.append(phrase.text)\n",
    "        #phrases_all.append(phrase.text.lower())\n",
    "\n",
    "    return phrases_all\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./ent_results300_np.txt\", \"rb\") as fp:   \n",
    "    ent_results = pickle.load(fp)\n",
    "\n",
    "num_ent_positive=0\n",
    "for i in ent_results:\n",
    "    if i>=0.5:\n",
    "        num_ent_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to ent:\",num_ent_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"./mng_results300.txt\", \"rb\") as fp:   \n",
    "    mng_results = pickle.load(fp)\n",
    "\n",
    "num_mng_positive=0\n",
    "for i in mng_results:\n",
    "    if i>=0.5:\n",
    "        num_mng_positive +=1\n",
    "\n",
    "print(\"Number of detected relevant tweets to mng:\", num_mng_positive)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequnent hashtags in ENT (with thier frequencies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "ent_hash_2017=[]\n",
    "for i,value in enumerate(ent_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2017.extend(hashtags)\n",
    "        \n",
    "ent_Hashtags=dict()\n",
    "\n",
    "for ph in ent_hash:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags:\n",
    "            ent_Hashtags[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags = sorted(ent_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "ent_hash_2017=[]\n",
    "\n",
    "for i in range(0,len(ent_2017_rel)):\n",
    "    #print(ent_2017_rel[i])\n",
    "    text=df_ent.tweet[ent_2017_rel[i]]\n",
    "    \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "    text=re.sub(\"…\", '' ,text)\n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.)+\", ' ',text)  \n",
    "    \n",
    "    hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "    ent_hash_2017.extend(hashtags)\n",
    "'''\n",
    "'''\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2017.extend(hashtags)\n",
    "'''\n",
    "'''\n",
    "     \n",
    "ent_Hashtags_2017=dict()\n",
    "\n",
    "for ph in ent_hash_2017:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags_2017:\n",
    "            ent_Hashtags_2017[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags_2017[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags_2017 = sorted(ent_Hashtags_2017.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "\n",
    "ent_Hashtags_2017[0:100]\n",
    "'''\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_ent=pd.DataFrame(ent_Hashtags_2017)\n",
    "df_hash_ent.columns =['Hashtags in ent','frequency']\n",
    "df_hash_ent=df_hash_ent.head(100)\n",
    "df_hash_ent.to_csv('hashtags_ent.csv')\n",
    "df_hash_ent_styled = df_hash_ent.style \n",
    "dfi.export(df_hash_ent_styled,\"hashtags_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "ent_hash_2019=[]\n",
    "\n",
    "for i in range(0,len(ent_2019_rel)):\n",
    "    #print(ent_2019_rel[i])\n",
    "    text=df_ent.tweet[ent_2019_rel[i]]\n",
    "    \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "    text=re.sub(\"…\", '' ,text)\n",
    "    text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    text= re.sub(\"(!)+\", '! ',text)     \n",
    "    text= re.sub(\"(\\.)+\", ' ',text)  \n",
    "    \n",
    "    hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "    ent_hash_2019.extend(hashtags)\n",
    "'''\n",
    "\n",
    "'''\n",
    "for i,value in enumerate(ent_year_ind_2019):\n",
    "    if label_19[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "        ent_hash_2019.extend(hashtags)\n",
    "'''        \n",
    "'''        \n",
    "        \n",
    "ent_Hashtags_2019=dict()\n",
    "\n",
    "for ph in ent_hash_2019:\n",
    "    if ph !='':\n",
    "        if ph not in ent_Hashtags_2019:\n",
    "            ent_Hashtags_2019[ph]=1\n",
    "        else:\n",
    "            ent_Hashtags_2019[ph]+=1  \n",
    "\n",
    "        \n",
    "ent_Hashtags_2019 = sorted(ent_Hashtags_2019.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0) \n",
    "\n",
    "ent_Hashtags_2019[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 most frequent hashtags in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "mng_hash=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘';•«,@:~!\\=%&]+\", '',text) #except  \\- _\n",
    "\n",
    "        hashtags = [i[1:].lower()  for i in text.split() if i.startswith(\"#\") ]\n",
    "\n",
    "        mng_hash.extend(hashtags)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "mng_Hashtags=dict()\n",
    "\n",
    "for ph in mng_hash:\n",
    "    if ph != '':\n",
    "        if ph not in mng_Hashtags:\n",
    "            mng_Hashtags[ph]=1\n",
    "        else:\n",
    "            mng_Hashtags[ph]+=1  \n",
    "\n",
    "mng_Hashtags = sorted(mng_Hashtags.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_Hashtags[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_hash_mng=pd.DataFrame(mng_Hashtags)\n",
    "df_hash_mng.columns =['Hashtags in mng','frequency']\n",
    "df_hash_mng=df_hash_mng.head(100)\n",
    "\n",
    "df_hash_mng.to_csv('hashtags_mng.csv')\n",
    "\n",
    "df_hash_mng_styled = df_hash_mng.style \n",
    "dfi.export(df_hash_mng_styled,\"hashtags_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignore below lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "ent_spacy=[]\n",
    "for i,value in enumerate(ent_year_ind_2017):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        ent_spacy.extend(concepts)\n",
    "    \n",
    "\n",
    "ent_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in ent_spacy:\n",
    "    if ph not in ent_noun_phrase_spacy:\n",
    "        ent_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        ent_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "ent_noun_phrase_spacy = sorted(ent_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "# finding concepts by spacy \n",
    "mng_spacy=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases1(text)\n",
    "        \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                concept1= re.sub('^a |^an |^the ', '', concept)\n",
    "                concepts.append(concept1)\n",
    "                \n",
    "#        print(concepts)\n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "        mng_spacy.extend(concepts)\n",
    "    \n",
    "    \n",
    "mng_noun_phrase_spacy=dict()\n",
    "\n",
    "for ph in mng_spacy:\n",
    "    if ph not in mng_noun_phrase_spacy:\n",
    "        mng_noun_phrase_spacy[ph]=1\n",
    "    else:\n",
    "        mng_noun_phrase_spacy[ph]+=1   \n",
    "            \n",
    "mng_noun_phrase_spacy = sorted(mng_noun_phrase_spacy.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in ENT found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "## finding concepts by nltk\n",
    "#ent_2017_nltk=[]\n",
    "\n",
    "#ent_noun_phrase_2017_nltk=dict()\n",
    "\n",
    "#for i,value in enumerate(ent_year_ind_2017):\n",
    "#    if label_17[i]==1:\n",
    "#        text=cleaningA(df_ent['tweet'][value])\n",
    "##        print(text)\n",
    "##        print('----')\n",
    "#        #text=re.sub('#', ' ', text)\n",
    "#        concepts_x=noun_phrases(text)\n",
    "    \n",
    "#        concepts=[]\n",
    "#        for concept in concepts_x:\n",
    "#            if concept not in cachedStopWords:\n",
    "#                concept= re.sub('^a |^an |^the ', '', concept)\n",
    "#                concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "#                if concept !='':\n",
    "#                    if concept not in ent_noun_phrase_2017_nltk:\n",
    "#                        ent_noun_phrase_2017_nltk[concept]=1\n",
    "#                    else:\n",
    "#                        ent_noun_phrase_2017_nltk[concept]+=1\n",
    "                \n",
    "#                #concepts.append(concept)\n",
    "                \n",
    "##        print(concepts)\n",
    "##        print('++++++++++++++++++++++++++++')\n",
    "\n",
    "#        #ent_2017_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "'''\n",
    "for word in ent_2017_nltk:\n",
    "    if word !='':\n",
    "        if word not in ent_noun_phrase_2017_nltk:\n",
    "            ent_noun_phrase_2017_nltk[word]=1\n",
    "        else:\n",
    "            ent_noun_phrase_2017_nltk[word]+=1   \n",
    "'''\n",
    "\n",
    "#ent_noun_phrase_2017_nltk = sorted(ent_noun_phrase_2017_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "#print( time.time() - t0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_noun_phrase_2017_nltk[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i,value in enumerate(ent_year_ind_2017[0:1000000]):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        #text= re.sub(\"#\", '',text0)\n",
    "        if 'Great ' in text:\n",
    "            if 'Great' in noun_phrases(text):\n",
    "                \n",
    "                print(text)\n",
    "                print(noun_phrases(text))\n",
    "                print('-----')\n",
    "                print(noun_phrases1(text))\n",
    "                print()\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_noun_phrase_2017_nltk_non[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "yes_17=[]\n",
    "for i in ent_noun_phrase_2017_nltk[0:50]:\n",
    "    yes_17.append(i[0])\n",
    "\n",
    "non_17=[]\n",
    "for i in ent_noun_phrase_2017_nltk_non[0:50]:\n",
    "    non_17.append(i[0])\n",
    "A\n",
    "aaa=set(yes_17)-set(non_17)\n",
    "aaa\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())\n",
    "\n",
    "aaa=list(set(yes)-set(non))\n",
    "aaa=list(set(yes))\n",
    "concept_vec=np.zeros((len(aaa),300))\n",
    "\n",
    "for i,concept in enumerate(aaa):\n",
    "    concept_vec[i]=ft.get_word_vector(concept)\n",
    "\n",
    "\n",
    "np.shape(concept_vec)\n",
    "\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustering_dbscan = DBSCAN(eps=5, min_samples=2).fit(concept_vec)\n",
    "print(clustering_dbscan.labels_)\n",
    "\n",
    "clustering_optics = OPTICS(min_samples=3).fit(concept_vec)\n",
    "print(clustering_optics.labels_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2) \n",
    "ent_kmeans_clustering = kmeans.fit(concept_vec)\n",
    "print(ent_kmeans_clustering.labels_)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_ent=pd.DataFrame(ent_noun_phrase_nltk)\n",
    "df_ph_ent.columns =['noun_phrase in ent by NLTK','frequency']\n",
    "df_ph_ent=df_ph_ent.head(100)\n",
    "df_ph_ent.to_csv('noun_phrase_ent.csv')\n",
    "df_ph_ent_styled = df_ph_ent.style \n",
    "dfi.export(df_ph_ent_styled,\"noun_phrase_ent_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "yes_19=[]\n",
    "for i in ent_noun_phrase_2019_nltk[0:50]:\n",
    "    yes_19.append(i[0])\n",
    "\n",
    "non_19=[]\n",
    "for i in ent_noun_phrase_2019_nltk_non[0:50]:\n",
    "    non_19.append(i[0])\n",
    "\n",
    "bbb=set(yes_19)-set(non_19)\n",
    "bbb\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 noun phrases in MNG found by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "#finding concepts by nltk \n",
    "mng_nltk=[]\n",
    "for i,value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=re.sub('#', ' ', text)\n",
    "#        print(text)\n",
    "#        print('----')\n",
    "        concepts_x=noun_phrases(text)\n",
    "    \n",
    "        concepts=[]\n",
    "        for concept in concepts_x:\n",
    "            if concept not in cachedStopWords:\n",
    "                #concept= re.sub('^a |^an |^the ', '', concept)\n",
    "                #concept= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',concept)\n",
    "                concepts.append(concept)\n",
    "        \n",
    "#        print(concepts)       \n",
    "#        print('++++++++++++++++++++++++++++')\n",
    "        mng_nltk.extend(concepts)\n",
    "    \n",
    "\n",
    "mng_noun_phrase_nltk=dict()\n",
    "\n",
    "for word in mng_nltk:\n",
    "    if word !='':\n",
    "        if word not in mng_noun_phrase_nltk:\n",
    "            mng_noun_phrase_nltk[word]=1\n",
    "        else:\n",
    "            mng_noun_phrase_nltk[word]+=1   \n",
    "            \n",
    "mng_noun_phrase_nltk = sorted(mng_noun_phrase_nltk.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_noun_phrase_nltk[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_ph_mng=pd.DataFrame(mng_noun_phrase_nltk)\n",
    "df_ph_mng.columns =['noun_phrase in mng by NLTK','frequency']\n",
    "df_ph_mng=df_ph_mng.head(100)\n",
    "\n",
    "df_ph_mng.to_csv('noun_phrase_mng.csv')\n",
    "\n",
    "df_ph_mng_styled = df_ph_mng.style \n",
    "dfi.export(df_ph_mng_styled,\"noun_phrase_mng_nltk.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "words_ent=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][value])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                word= re.sub(\"[\\\"\\“\\”\\+\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}`′’‘'><;•«,@:~!\\=%&]+\", '',word)\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "    \n",
    "        for word in words_nostop:\n",
    "            #word= lemm(wordx)\n",
    "            if word not in words_ent:\n",
    "                words_ent[word]=1\n",
    "            else:\n",
    "                words_ent[word] +=1\n",
    "            \n",
    "words_ent = sorted(words_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_ent))\n",
    "\n",
    "print('First 100 frequent words in ent:')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_ent[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_ent=pd.DataFrame(words_ent)\n",
    "df_word_ent.columns =['word in ent','frequency']\n",
    "df_word_ent=df_word_ent.head(100)\n",
    "df_word_ent.to_csv('word_ent.csv')\n",
    "df_word_ent_styled = df_word_ent.style \n",
    "dfi.export(df_word_ent_styled,\"words_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 (nonstop) words in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "words_mng=dict()\n",
    "Sentences=[]\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        #Sentences.append(text)\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                    \n",
    "        for word in words_nostop:\n",
    "            #word=lemm(wordx)\n",
    "            if word not in words_mng:\n",
    "                words_mng[word]=1\n",
    "            else:\n",
    "                words_mng[word] +=1\n",
    "                \n",
    "words_mng = sorted(words_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of words:',len(words_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('First 100 frequent words in mng:')\n",
    "#words_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_word_mng=pd.DataFrame(words_mng)\n",
    "df_word_mng.columns =['word in mng','frequency']\n",
    "df_word_mng=df_word_mng.head(100)\n",
    "\n",
    "df_word_mng.to_csv('word_mng.csv')\n",
    "\n",
    "df_word_mng_styled = df_word_mng.style \n",
    "dfi.export(df_word_mng_styled,\"words_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_ent=dict()\n",
    "\n",
    "for i, value in enumerate(ent_year_ind_2017):\n",
    "    if label_17[i]==1:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_ent:\n",
    "                bigrams_ent[combi]=1\n",
    "            else:\n",
    "                bigrams_ent[combi] +=1\n",
    "                \n",
    "bigrams_ent = sorted(bigrams_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_ent))\n",
    "\n",
    "print('first 100 frequent bigrams in ent:')\n",
    "\n",
    "bigrams_ent[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_ent=pd.DataFrame(bigrams_ent)\n",
    "df_bigram_ent.columns =['bigram in ent','frequency']\n",
    "df_bigram_ent=df_bigram_ent.head(100)\n",
    "df_bigram_ent.to_csv('bigram_ent.csv')\n",
    "df_bigram_ent_styled = df_bigram_ent.style \n",
    "dfi.export(df_bigram_ent_styled,\"bigrams_ent.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 bigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "bigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 2))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #combi=(w0,w1)\n",
    "            if combi not in bigrams_mng:\n",
    "                bigrams_mng[combi]=1\n",
    "            else:\n",
    "                bigrams_mng[combi] +=1\n",
    "                \n",
    "bigrams_mng = sorted(bigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of bigrams:',len(bigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent bigrams in mng:')\n",
    "#bigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_bigram_mng=pd.DataFrame(bigrams_mng)\n",
    "df_bigram_mng.columns =['bigram in mng','frequency']\n",
    "df_bigram_mng=df_bigram_mng.head(100)\n",
    "\n",
    "df_bigram_mng.to_csv('bigram_mng.csv')\n",
    "\n",
    "df_bigram_mng_styled = df_bigram_mng.style \n",
    "dfi.export(df_bigram_mng_styled,\"bigrams_mng.png\")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bigram_mng_styled.to_excel('jjdsfhj.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in ENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_ent=dict()\n",
    "\n",
    "for i, value in enumerate(ent_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_ent['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)  \n",
    "            if combi not in trigrams_ent:\n",
    "                trigrams_ent[combi]=1\n",
    "            else:\n",
    "                trigrams_ent[combi] +=1\n",
    "                \n",
    "trigrams_ent = sorted(trigrams_ent.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('Number of trigrams:',len(trigrams_ent))\n",
    "\n",
    "print('first 100 frequent trigrams in ent:')\n",
    "trigrams_ent[0:100]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 100 trigrams (without stopwords) in MNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0=time.time()\n",
    "\n",
    "trigrams_mng=dict()\n",
    "\n",
    "for i, value in enumerate(mng_results):\n",
    "    if value>=0.5:\n",
    "        text=cleaningA(df_mng['tweet'][i])\n",
    "        #text=cleaning_punc(text.lower())\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        words_nostop=[]\n",
    "        for word in words:\n",
    "            if word not in cachedStopWords:\n",
    "                words_nostop.append(word)\n",
    "                \n",
    "        output = list(ngrams(words_nostop, 3))\n",
    "        \n",
    "        for combi in output:\n",
    "            #w0=lemm(combix[0])\n",
    "            #w1=lemm(combix[1])\n",
    "            #w2=lemm(combix[2])\n",
    "            #combi=(w0,w1,w2)\n",
    "            if combi not in trigrams_mng:\n",
    "                trigrams_mng[combi]=1\n",
    "            else:\n",
    "                trigrams_mng[combi] +=1\n",
    "                \n",
    "trigrams_mng = sorted(trigrams_mng.items(), key=operator.itemgetter(1), reverse=True) \n",
    "print( time.time() - t0)\n",
    "print('number of trigrams:',len(trigrams_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('first 100 frequent trigrams in mng:')\n",
    "#trigrams_mng[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
