{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jafar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "from scipy import stats\n",
    "import random\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database opened successfully\n",
      "5983238\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "q='''SELECT screen_name, tweet FROM manager_world_tweets_ph2 \n",
    "    Where \n",
    "    tweet_created_at like '%2019%';'''\n",
    "cur.execute(q)\n",
    "rows_mng = cur.fetchall()\n",
    "# there is no repetative tweet\n",
    "con.close()\n",
    "\n",
    "print(len(rows_mng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database opened successfully\n",
      "5237437\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "q='''SELECT screen_name, tweet FROM ent_world_tweets_ph2 \n",
    "    Where \n",
    "    tweet_created_at like '%2019%';'''\n",
    "cur.execute(q)\n",
    "rows_ent = cur.fetchall()\n",
    "# there is no repetative tweet\n",
    "con.close()\n",
    "\n",
    "print(len(rows_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text)   \n",
    "\n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "\n",
    "    #removing some special charachter  \n",
    "    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`;•«,@:~!=%&]+\", ' ',text) # except ' ’\n",
    "    \n",
    "    #removing hashtag sign\n",
    "    text=re.sub('#', '', text) \n",
    "    \n",
    "    #removing numbers\n",
    "    text=re.sub(\"(\\d+)\",' ',text)\n",
    "#    print(text)\n",
    "    #removing numbers (not attached)\n",
    "#    text=re.sub(r\"(^|\\s)-?(\\d+)\\.?(\\d*)(\\s)\", '', text)\n",
    "#    text=re.sub(\"(^||\\d+)\\.(\\d+) \",'',text)\n",
    "#    text=re.sub(\"[0-9+]\\.?(\\d+)(\\.)\",' ',text)\n",
    "#    text=re.sub(\"[0-9+]\\.?(\\d+)($)\",' ',text)\n",
    "#    print(text)\n",
    "    \n",
    "#    return text\n",
    "\n",
    "#def lower_case (text):\n",
    "    text = text.lower() \n",
    "#    print(text)\n",
    "#    return text_lower\n",
    "\n",
    "#def tokenization (text):\n",
    "    text= nltk.word_tokenize(text)\n",
    "#    print (text)\n",
    "#    return text_tokens\n",
    "\n",
    "#def removing_stopwords (text):\n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()]\n",
    "#    text = [word for word in text if word not in stopwords.words()]\n",
    "    text = [word for word in text if word not in cachedStopWords]\n",
    "#    print(text)\n",
    "#    return text_without_sw\n",
    "\n",
    "#def stemming (text):\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    #text_stems=stemmer.stem(tokens_without_sw)\n",
    "#    text = [stemmer.stem(word) for word in text]\n",
    "    text = [Stem(word) for word in text]\n",
    "\n",
    "#    print(text)\n",
    "    \n",
    "    text=' '.join(text)\n",
    "#    print(text)\n",
    "#    print(text,'\\n')   \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ent=[]\n",
    "for row in rows_ent:\n",
    "    names_ent.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_mng=[]\n",
    "for row in rows_mng:\n",
    "    names_mng.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_cleaned=[]\n",
    "for row in rows_mng:\n",
    "    ans=cleaning(row[1])\n",
    "    mng_cleaned.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_cleaned=[]\n",
    "for row in rows_ent:\n",
    "    ans=cleaning(row[1])\n",
    "    ent_cleaned.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweet_mng5M_cleaned.txt\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(mng_cleaned, fp)\n",
    "\n",
    "with open(\"tweet_ent5M_cleaned.txt\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(ent_cleaned, fp)\n",
    "\n",
    "with open(\"names_mng5M.txt\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(names_mng, fp)\n",
    "\n",
    "with open(\"names_ent5M.txt\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(names_ent, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmng_cleaned=[]\\nfor tweet in rows_mng:\\n    ans=cleaning(tweet[0])\\n    if ans!=[]:\\n        mng_cleaned.append(ans)\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "mng_cleaned=[]\n",
    "for tweet in rows_mng:\n",
    "    ans=cleaning(tweet[0])\n",
    "    if ans!=[]:\n",
    "        mng_cleaned.append(ans)\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nent_cleaned=[]\\nfor tweet in rows_ent:\\n    ans=cleaning(tweet[0])\\n    if ans!=[]:\\n        ent_cleaned.append(ans)\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ent_cleaned=[]\n",
    "for tweet in rows_ent:\n",
    "    ans=cleaning(tweet[0])\n",
    "    if ans!=[]:\n",
    "        ent_cleaned.append(ans)\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows_ent[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_words=[]\n",
    "for tweet in ent_cleaned:\n",
    "    ent_words += tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_words=[]\n",
    "for tweet in mng_cleaned:\n",
    "    mng_words += tweet.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the similarity between two datasets we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2=[]\n",
    "p_value=[]\n",
    "ent_size=2600000\n",
    "mng_size=2600000\n",
    "most_frequent=10\n",
    "\n",
    "for kk in range(10):\n",
    "    \n",
    "#    ent_size=floor(len(ent_words)/2)\n",
    "    ent_words_random=random.sample(ent_words,ent_size)\n",
    "    \n",
    "#    mng_size=floor(len(mng_words)/2)\n",
    "    mng_words_random=random.sample(mng_words,mng_size)\n",
    "    \n",
    "    all_words=mng_words_random+ent_words_random\n",
    "\n",
    "\n",
    "    d_mng = dict()\n",
    "    for c in mng_words_random:\n",
    "        if c not in d_mng:\n",
    "            d_mng[c] = 1\n",
    "        else:\n",
    "            d_mng[c] = d_mng[c] + 1\n",
    "\n",
    "    d_mng_new=dict(sorted(d_mng.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_mng=list(d_mng_new.keys())\n",
    "    values_list_mng=list(d_mng_new.values())\n",
    "    frequent_words_mng=set(list(d_mng_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng_new.values())[0:most_frequent])        \n",
    "\n",
    "    d_ent = dict()\n",
    "    for c in ent_words_random:\n",
    "        if c not in d_ent:\n",
    "            d_ent[c] = 1\n",
    "        else:\n",
    "            d_ent[c] = d_ent[c] + 1\n",
    "\n",
    "    d_ent_new=dict(sorted(d_ent.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_ent=list(d_ent_new.keys())\n",
    "    values_list_ent=list(d_ent_new.values())\n",
    "    frequent_words_ent=set(list(d_ent_new.keys())[0:most_frequent])\n",
    "#    print(list(d_ent_new.keys())[0:most_frequent])\n",
    "#    print(list(d_ent_new.values())[0:most_frequent])       \n",
    "\n",
    "    d_all = dict()\n",
    "    for c in all_words:\n",
    "        if c not in d_all:\n",
    "            d_all[c] = 1\n",
    "        else:\n",
    "            d_all[c] = d_all[c] + 1\n",
    "\n",
    "    d_all_new=dict(sorted(d_all.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_all=list(d_all_new.keys())\n",
    "    values_list_all=list(d_all_new.values())\n",
    "#    print(list(d_all_new.keys())[0:most_frequent])\n",
    "#    print(list(d_all_new.values())[0:most_frequent])        \n",
    "\n",
    "#    print(len(key_list_all))\n",
    "#    print(len(key_list_mng))\n",
    "#    print(len(key_list_ent))\n",
    "\n",
    "    frequent=key_list_all[0:most_frequent] # \n",
    "    #frequent\n",
    "\n",
    "    text1_freq_values=[]\n",
    "    text2_freq_values=[]\n",
    "\n",
    "    for ii in frequent:\n",
    "    #    print(ii)\n",
    "    #    print(d_ent_new[ii])\n",
    "    #    print(d_mng_new[ii])\n",
    "        try: \n",
    "            text1_freq_values.append(d_ent_new[ii])\n",
    "        except:\n",
    "            text1_freq_values.append(0)\n",
    "        try:    \n",
    "            text2_freq_values.append(d_mng_new[ii])\n",
    "        except:\n",
    "            text2_freq_values.append(0)\n",
    "            \n",
    "            \n",
    "#        except:\n",
    "#            print(ii)\n",
    "#            print(d_mng_new[ii])\n",
    "#            print(d_ent_new[ii])\n",
    "\n",
    "    #print(text1_freq_values )  \n",
    "    #print(text2_freq_values)    \n",
    "\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency([text1_freq_values,text2_freq_values])\n",
    "#    print(chi2_stat)\n",
    "#    print(p_val)\n",
    "    chi2.append(chi2_stat)\n",
    "    p_value.append(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 10 words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection : {'go', 'rt', 'get', 'amp', \"'s\", \"n't\", 'like', 'one', '’'}\n",
      "\n",
      "mng - ent : {'time'}\n",
      "\n",
      "ent- mng : {'peopl'}\n"
     ]
    }
   ],
   "source": [
    "print(\"intersection :\", frequent_words_mng & frequent_words_ent)\n",
    "print()\n",
    "print(\"mng - ent :\",frequent_words_mng - frequent_words_ent)\n",
    "print()\n",
    "print(\"ent- mng :\",frequent_words_ent - frequent_words_mng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is possible that, for example, \"time\" is 11th word for ent and \"peopl\" (people) is 12th for mng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 500 words we have (since the number of intersetion is high, we just show its length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 424\n",
      "\n",
      "mng - ent : {'beat', 'age', 'eye', 'footbal', 'dog', 'record', 'honest', 'releas', \"'d\", 'absolut', 'yesterday', 'near', 'uk', 'view', 'episod', 'im', 'pretti', 'pictur', '£', 'updat', 'match', 'guess', '\\u2066', 'friday', 'fantast', 'huge', 'light', 'probabl', 'cover', 'luck', 'appreci', 'least', 'class', 'abl', 'vs', 'pick', 'liter', 'leagu', 'a…', 'quit', 'congrat', 'christma', 'late', 'short', 'took', 'finish', 'hell', 'three', 'local', 'coach', 'club', 'road', 'brexit', 'saw', 'goe', 'favorit', 't…', 'heard', 'mate', 'movi', 'star', 'ticket', 'stream', 'special', 'stuff', 'perform', 'drink', 'sorri', 'cool', 'remind', 'half', 'sunday', 'super', 'almost', 'bit', 'photo'}\n",
      "\n",
      "ent- mng : {'network', 'focus', 'startup', 'technolog', 'ass', 'nigga', 'space', 'act', 'googl', 'trust', 'water', 'dear', 'daili', 'energi', 'digit', 'tip', 'travel', 'spend', 'truth', 'sell', 'entrepreneur', 'ai', 'provid', 'key', 'phone', 'brand', 'sale', 'connect', 'peac', 'podcast', 'respect', 'discuss', 'easi', 'rule', 'tech', 'nigerian', 'entrepreneurship', 'innov', 'app', 'ladi', 'univers', 'wan', 'fail', 'blog', 'leadership', 'content', 'africa', 'invest', 'sometim', 'educ', 'respons', 'ur', 'sir', 'tax', 'nigeria', 'rais', 'global', 'dream', 'sleep', 'brother', 'self', 'polic', 'strong', 'fund', 'industri', 'r', 'grow', 'onlin', 'launch', 'program', 'control', 'secur', 'india', 'valu', 'bank', 'trade'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng & frequent_words_ent))\n",
    "print()\n",
    "print(\"mng - ent :\",frequent_words_mng - frequent_words_ent)\n",
    "print()\n",
    "print(\"ent- mng :\",frequent_words_ent - frequent_words_mng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1000 words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 876\n",
      "\n",
      "mng - ent : {'impress', 'blue', 'appear', '£', 'h…', 'cold', 'normal', 'ride', 'johnson', 'tori', 'pre', 'afternoon', 'bed', 'whether', 'sweet', 'cup', 'juli', 'suggest', 'arriv', 'lt', 'liverpool', 'v', 'danc', 'involv', 'sick', 'favourit', 'scene', 'eu', 'racist', 'fm', 'extra', 'usual', 'ice', 'recommend', 'staff', 'champion', 'station', 'obvious', 'legend', 'main', 'chat', 'charact', 'i…', 'labour', 'battl', 'bori', 'weather', 'suppos', 'field', 'bought', 'haha', 'massiv', 'memori', 'weird', 'sent', 's…', 'brilliant', 'spent', 'notic', 'sever', 'roll', 'kick', 'pack', 'edit', 'tour', 'anyway', 'giveaway', 'rain', 'seat', 'west', 'cheer', 'bar', 'titl', 'loss', 'omg', 'cute', 'four', 'defend', 'rd', 'awar', 'cat', 'basic', 'shirt', 'coffe', 'mate', 'lucki', 'plus', 'channel', 'assist', 'coupl', 'version', 'trip', 'straight', 'march', 'except', 'five', 'hospit', 'switch', 'sort', 'met', 'town', 'draw', 'fantast', 'contact', 'race', 'twitch', 'hang', 'donat', 'album', 'howev', 'wors', 'confirm', 'ha', 'doubl', 'pro', 'rock', 'pic', 'wall', 'cross', 'press', 'dark', 'lad', 'pop', 'min'}\n",
      "\n",
      "ent- mng : {'secret', 'everybodi', 'nigga', 'amen', 'youth', 'profit', 'pitch', 'prayer', 'mistak', 'sourc', 'rich', 'vision', 'smile', 'entrepreneurship', 'owner', 'economi', 'econom', 'financi', 'brain', 'dey', 'resourc', 'legal', 'freedom', 'wake', 'common', 'marri', 'lack', 'demand', 'congress', 'startup', 'founder', 'justic', 'born', 'confid', 'sex', 'rise', 'billion', 'soul', 'click', 'pls', 'politician', 'gun', 'arrest', 'forev', 'bitch', 'judg', 'senat', 'dr', 'societi', 'investor', 'author', 'trend', 'size', 'address', 'yo', 'avoid', 'church', 'jesus', 'fashion', 'speaker', 'capit', 'growth', 'protest', 'african', 'knowledg', 'indian', 'sister', 'ceo', 'critic', 'toward', 'daughter', 'nigerian', 'ppl', 'pakistan', 'smart', 'corrupt', 'employe', 'purpos', 'bitcoin', 'applic', 'quot', 'guid', 'broke', 'crime', 'cash', 'blockchain', 'motiv', 'femal', 'muslim', 'wit', 'imag', 'lesson', 'automat', 'entrepreneur', 'realiti', 'simpli', 'appl', 'govt', 'allah', 'africa', 'healthi', 'creativ', 'lago', 'privat', 'teacher', 'influenc', 'boss', 'target', 'wast', 'corpor', 'statement', 'pray', 'deep', 'client', 'journey', 'gain', 'buhari', 'handl', 'citizen', 'faith', 'dem', 'afford', 'debat', 'crypto'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng & frequent_words_ent))\n",
    "print()\n",
    "print(\"mng - ent :\",frequent_words_mng - frequent_words_ent)\n",
    "print()\n",
    "print(\"ent- mng :\",frequent_words_ent - frequent_words_mng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 10000 words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 8194\n",
      "length of mng - ent : 1806\n",
      "length of ent- mng : 1806\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng & frequent_words_ent))\n",
    "print(\"length of mng - ent :\",len(frequent_words_mng - frequent_words_ent)\n",
    "print(\"length of ent- mng :\",len(frequent_words_ent - frequent_words_mng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 100000 words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 40149\n",
      "length of mng - ent : 59851\n",
      "length of ent- mng : 59851\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng & frequent_words_ent))\n",
    "print(\"length of mng - ent :\",len(frequent_words_mng - frequent_words_ent))\n",
    "print(\"length of ent- mng :\",len(frequent_words_ent - frequent_words_mng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 100000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection: {'go', 'rt', 'get', 'amp', \"'s\", \"n't\", 'like', 'one', '’'}\n",
      "ent_words - mng_words: {'peopl'}\n",
      "mng_words - ent_words: {'time'}\n",
      "\n",
      "chi2: [1028.7213068384015]\n",
      "CHI2_mean: 1028.7213068384015\n",
      "CHI2_std: 0.0\n",
      "p_value: [1.1034656798308345e-215]\n",
      "P_VALUE_mean: 1.1034656798308345e-215\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [144205.29735274083, 143188.48437141583, 143026.9569510265, 142381.70934693966, 143496.301480799, 142888.19713576583, 143488.67315995716, 143717.22011785215, 143189.74457660157, 143612.69451357206]\n",
      "CHI2_mean: 143319.52790066708\n",
      "CHI2_std: 476.83498706944044\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 1000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [59496.93504468392, 60015.64350626074, 59976.72548266546, 59131.16002254333, 59797.948359375616, 59825.33408178453, 59586.13962031849, 59705.060259595106, 59730.0892863322, 59949.97237651255]\n",
      "CHI2_mean: 59721.500804007206\n",
      "CHI2_std: 252.60307316474265\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the last iteration, we have the following most 20 frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20 most frequnt words in ent random corpus:\n",
      "['rt', '’', \"'s\", \"n't\", 'get', 'like', 'amp', 'one', 'peopl', 'go', 'time', 'make', 'know', 'thank', 'need', 'day', 'year', 'love', 'new', 'see']\n",
      "\n",
      " 20 most frequnt words in mng random corpus:\n",
      "['rt', '’', \"'s\", 'get', 'like', \"n't\", 'one', 'go', 'time', 'amp', 'day', 'thank', 'year', 'peopl', 'good', 'see', 'love', 'look', 'new', 'make']\n"
     ]
    }
   ],
   "source": [
    "print('\\n 20 most frequnt words in ent random corpus:')\n",
    "print(key_list_ent[0:20])\n",
    "\n",
    "print('\\n 20 most frequnt words in mng random corpus:')\n",
    "print(key_list_mng[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 500 frequent words we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [40424.90370163364, 41189.86804521994, 41244.87607402078, 40336.55179638849, 40659.135096578415, 41057.345171925364, 42081.75096789941, 40782.03615662649, 41126.05020491114, 41119.67360871063]\n",
      "CHI2_mean: 41002.21908239143\n",
      "CHI2_std: 472.6780474809556\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the last iteration, we have the following most 20 frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20 most frequnt words in ent random corpus:\n",
      "['rt', '’', \"'s\", \"n't\", 'get', 'like', 'amp', 'one', 'peopl', 'go', 'time', 'make', 'know', 'thank', 'need', 'day', 'see', 'year', 'new', 'love']\n",
      "\n",
      " 20 most frequnt words in mng random corpus:\n",
      "['rt', '’', \"'s\", 'get', 'like', \"n't\", 'one', 'go', 'time', 'amp', 'day', 'peopl', 'thank', 'year', 'good', 'love', 'see', 'look', 'make', 'know']\n"
     ]
    }
   ],
   "source": [
    "print('\\n 20 most frequnt words in ent random corpus:')\n",
    "print(key_list_ent[0:20])\n",
    "\n",
    "print('\\n 20 most frequnt words in mng random corpus:')\n",
    "print(key_list_mng[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 10 frequent words we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [845.7593821951187, 781.0033209526467, 702.422465411873, 778.3694323543484, 818.3645986097752, 758.3394052524558, 694.3793855552625, 738.3464935632435, 734.7549451568664, 804.2575955394715]\n",
      "CHI2_mean: 765.5997024591062\n",
      "CHI2_std: 46.81348509496665\n",
      "p_value: [2.988127514447561e-176, 2.607470991490118e-162, 2.0848843459183727e-145, 9.616996779647768e-162, 2.3668546601682847e-170, 1.9632727264313904e-157, 1.1172732610576256e-143, 3.925394462975115e-153, 2.3248042658715486e-152, 2.576932030903781e-167]\n",
      "P_VALUE_mean: 1.1381221072341727e-144\n",
      "P_VALUE_std: 3.345447684616233e-144\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the last iteration, for the most 10 frequent words, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10 most frequent words in ent random corpus:\n",
      "['rt', '’', \"'s\", \"n't\", 'get', 'like', 'amp', 'one', 'peopl', 'go']\n",
      "\n",
      " 10 most frequent words in mng random corpus:\n",
      "['rt', '’', \"'s\", 'get', 'like', \"n't\", 'one', 'go', 'time', 'amp']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n 10 most frequent words in ent random corpus:')\n",
    "print(key_list_ent[0:10])\n",
    "\n",
    "print('\\n 10 most frequent words in mng random corpus:')\n",
    "print(key_list_mng[0:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null hypothesis**: The null hypothesis states that the proportion of words in ent is identical to the proportion of words in mng, or corpuses are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting results**: Since the P-VALUE, P($\\chi^2$> chi2), is less than the significance level (0.05), we reject the null hypothesis.\n",
    "So, for both 10 and 500 frequent words, the test says that corpuses are not similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For *intra similarity* for mng set (based on random selection of tweets) we have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2=[]\n",
    "p_value=[]\n",
    "\n",
    "Mng1_words=mng_words[:floor(len(mng_words)/2)]\n",
    "Mng2_words=mng_words[floor(len(mng_words)/2):]\n",
    "\n",
    "most_frequent=10\n",
    "\n",
    "for kk in range(10):\n",
    "  \n",
    "    mng1_words_random=random.sample(Mng1_words,floor(len(Mng1_words)/2))\n",
    "    mng2_words_random=random.sample(Mng2_words,floor(len(Mng2_words)/2))   \n",
    "    all_words=mng1_words_random+mng2_words_random\n",
    "    \n",
    "#    rang=range(len(mng_words))\n",
    "#    slice1_ind=random.sample(rang,floor(len(mng_words)/4))\n",
    "#    temp=list( set(rang) - set(slice1_ind)  ) # list(a.diference(b))\n",
    "#    slice2_ind=random.sample(temp,floor(len(temp)/3))\n",
    "    \n",
    "#    mng1_words_random=[]\n",
    "#    for ii in slice1_ind:\n",
    "#        mng1_words_random.append(mng_words[ii])\n",
    "        \n",
    "#    mng2_words_random=[]\n",
    "#    for ii in slice2_ind:\n",
    "#        mng2_words_random.append(mng_words[ii])\n",
    "        \n",
    "#    all_words=mng_words\n",
    "    \n",
    "    \n",
    "    d_mng1 = dict()\n",
    "    for c in mng1_words_random:\n",
    "        if c not in d_mng1:\n",
    "            d_mng1[c] = 1\n",
    "        else:\n",
    "            d_mng1[c] = d_mng1[c] + 1\n",
    "\n",
    "    d_mng1_new=dict(sorted(d_mng1.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_mng1=list(d_mng1_new.keys())\n",
    "    values_list_mng1=list(d_mng1_new.values())\n",
    "    frequent_words_mng1=set(list(d_mng1_new.keys())[0:most_frequent])\n",
    "\n",
    "#    print(list(d_mng1_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng1_new.values())[0:most_frequent]) \n",
    "#    print()\n",
    "    \n",
    "    d_mng2 = dict()\n",
    "    for c in mng2_words_random:\n",
    "        if c not in d_mng2:\n",
    "            d_mng2[c] = 1\n",
    "        else:\n",
    "            d_mng2[c] = d_mng2[c] + 1\n",
    "            \n",
    "#    print(list(d_mng2_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng2_new.values())[0:most_frequent])   \n",
    "#    print()\n",
    "\n",
    "    d_mng2_new=dict(sorted(d_mng2.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_mng2=list(d_mng2_new.keys())\n",
    "    values_list_mng2=list(d_mng2_new.values())\n",
    "    frequent_words_mng2=set(list(d_mng2_new.keys())[0:most_frequent])\n",
    "\n",
    "#    print(list(d_all_new.keys())[0:most_frequent])\n",
    "#    print(list(d_all_new.values())[0:most_frequent]) \n",
    "#    print()\n",
    "\n",
    "    d_all = dict()\n",
    "    for c in all_words:\n",
    "        if c not in d_all:\n",
    "            d_all[c] = 1\n",
    "        else:\n",
    "            d_all[c] = d_all[c] + 1\n",
    "\n",
    "    d_all_new=dict(sorted(d_all.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_all=list(d_all_new.keys())\n",
    "    values_list_all=list(d_all_new.values())\n",
    "        \n",
    "\n",
    "    frequent=key_list_all[0:most_frequent] # \n",
    "#    print(frequent)\n",
    "\n",
    "\n",
    "    text1_freq_values=[]\n",
    "    text2_freq_values=[]\n",
    "\n",
    "    for ii in frequent:\n",
    "\n",
    "        try: \n",
    "            text1_freq_values.append(d_mng1_new[ii])\n",
    "        except:\n",
    "            text1_freq_values.append(0)\n",
    "        try:    \n",
    "            text2_freq_values.append(d_mng2_new[ii])\n",
    "        except:\n",
    "            text2_freq_values.append(0)\n",
    "        \n",
    "        \n",
    "#    print(text1_freq_values)  \n",
    "#    print(text2_freq_values)  \n",
    "\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency([text1_freq_values,text2_freq_values])\n",
    "    chi2.append(chi2_stat)\n",
    "    p_value.append(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 10 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection : {'go', 'time', 'rt', 'get', 'amp', \"'s\", \"n't\", 'like', 'one', '’'}\n",
      "\n",
      "mng1 - mng2 : set()\n",
      "\n",
      "mng2 - mng1 : set()\n"
     ]
    }
   ],
   "source": [
    "print(\"intersection :\", frequent_words_mng1 & frequent_words_mng2)\n",
    "print()\n",
    "print(\"mng1 - mng2 :\",frequent_words_mng1 - frequent_words_mng2)\n",
    "print()\n",
    "print(\"mng2 - mng1 :\",frequent_words_mng2 - frequent_words_mng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 500 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 484\n",
      "\n",
      "mng1 - mng2 : {'congrat', 'forget', 'american', '\\u2066', 'short', 'articl', 'cover', 'hell', 'drink', 'travel', 'price', 'secur', 'near', 'liter', 'room', 'goe'}\n",
      "\n",
      "mng2 - mng1 : {'saturday', 'sport', 'act', 'light', 'discuss', 'london', 'water', 'road', 'respons', 'inspir', 'bless', 'app', 'incred', 'blog', 'reach', 'brand'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len(frequent_words_mng1 & frequent_words_mng2))\n",
    "print()\n",
    "print(\"mng1 - mng2 :\",frequent_words_mng1 - frequent_words_mng2)\n",
    "print()\n",
    "print(\"mng2 - mng1 :\",frequent_words_mng2 - frequent_words_mng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 963\n",
      "\n",
      "mng1 - mng2 : {'https', 'felt', 'switch', 'david', 'fm', 'click', 'aw', 'middl', 'nigerian', 'gun', 'progress', 'ride', 'hang', 'ps', 'feed', 'bitch', 'stock', 'china', 'trend', 'whether', 'cute', 'tag', 'wast', 'volunt', 'battl', 'quot', 'deep', 'wake', 'impeach', 'wall', 'potenti', 'proper', 'press', 'adam', 'till', 'doubt', 'gold'}\n",
      "\n",
      "mng2 - mng1 : {'advic', 'decemb', 'minist', 'teach', 'engin', 'window', 'octob', 'sever', 'sourc', 'draw', 'co', 'dress', 'novemb', 'mobil', 'engag', 'virtual', 'howev', 'rain', 'employe', 'creativ', 'beyond', 'seat', 'afternoon', 'mix', 'loss', 'ha', 'repres', 'thursday', 'june', 'festiv', 'juli', 'regist', 'struggl', 'relationship', 'journey', 'invit', 'strategi'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len(frequent_words_mng1 & frequent_words_mng2))\n",
    "print()\n",
    "print(\"mng1 - mng2 :\",frequent_words_mng1 - frequent_words_mng2)\n",
    "print()\n",
    "print(\"mng2 - mng1 :\",frequent_words_mng2 - frequent_words_mng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 10000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 8927\n",
      "length of mng1 - mng2 : 1073\n",
      "length of mng2 - mng1 : 1073\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng1 & frequent_words_mng2))\n",
    "print(\"length of mng1 - mng2 :\",len(frequent_words_mng1 - frequent_words_mng2))\n",
    "print(\"length of mng2 - mng1 :\",len(frequent_words_mng2 - frequent_words_mng1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 100000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 59999\n",
      "length of mng1 - mng2 : 40001\n",
      "length of mng1 - mng1 : 40001\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng1 & frequent_words_mng2))\n",
    "print(\"length of mng1 - mng2 :\",len(frequent_words_mng1 - frequent_words_mng2))\n",
    "print(\"length of mng2 - mng1 :\",len(frequent_words_mng2 - frequent_words_mng1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is seen, when number of most frequent words increases, we have more different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 100000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [589746.8265019797, 590930.5393329542, 590975.1738484727, 591175.4506700744, 589886.1950987233, 590904.9389806322, 591716.0802972757, 591012.8661516039, 591671.4193496362, 591113.5319510122]\n",
      "CHI2_mean: 590913.3022182364\n",
      "CHI2_std: 613.0033241908357\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20 most frequnt words in subcorpus1:\n",
      "['rt', '’', \"'s\", 'get', 'like', \"n't\", 'one', 'go', 'time', 'amp', 'day', 'thank', 'good', 'peopl', 'year', 'see', 'love', 'look', 'know', 'new']\n",
      "\n",
      " 20 most frequnt words in subcorpus2:\n",
      "['rt', '’', \"'s\", 'get', 'like', 'one', \"n't\", 'go', 'time', 'amp', 'day', 'thank', 'year', 'peopl', 'love', 'good', 'look', 'see', 'make', 'new']\n"
     ]
    }
   ],
   "source": [
    "print('\\n 20 most frequnt words in subcorpus1:')\n",
    "print(key_list_mng1[0:20])\n",
    "\n",
    "print('\\n 20 most frequnt words in subcorpus2:')\n",
    "print(key_list_mng2[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [188231.50521356682, 189139.86373553492, 188572.10254375092, 188357.70851842983, 188456.480721872, 189324.18194097356, 188497.31824392098, 188071.64951424135, 188645.69371089223, 188237.23556281443]\n",
      "CHI2_mean: 188553.3739705997\n",
      "CHI2_std: 378.60346304089353\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 1000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [18580.366580647617, 18820.12459943602, 18665.749469208546, 18283.48818343672, 18570.24513711832, 18668.200382648683, 18933.087325838504, 18593.67454844105, 18909.254256157234, 18952.433103774783]\n",
      "CHI2_mean: 18697.66235867075\n",
      "CHI2_std: 198.5786443110858\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 500 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [11190.190229974132, 11169.521426528012, 11012.31441850915, 11100.940795496463, 11116.596022458994, 10868.532024616634, 10901.63261883241, 10924.225567823047, 11177.477725276927, 10839.836151684602]\n",
      "CHI2_mean: 11030.126698120037\n",
      "CHI2_std: 130.23882518783105\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [667.9561455148167, 687.1878508975444, 752.0081011612443, 668.8904772629913, 723.4915692332352, 686.1690438085022, 668.7695077536769, 640.5890770589607, 725.341181181385, 686.4052548482657]\n",
      "CHI2_mean: 690.6808208720623\n",
      "CHI2_std: 31.73369961775822\n",
      "p_value: [5.334527812476747e-138, 3.926473017781104e-142, 4.5195569939765685e-156, 3.3599031418070885e-138, 6.148659251850639e-150, 6.501074339166632e-142, 3.5671457530002225e-138, 4.0400360892654343e-132, 2.4604386875751727e-150, 5.783812426053629e-142]\n",
      "P_VALUE_mean: 4.040048352463278e-133\n",
      "P_VALUE_std: 1.212010418007844e-132\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For *intra similarity* for mng set based on *screen names* we have** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_uniques_mng=list(np.unique(names_mng))\n",
    "names_mng_arrray = np.array(names_mng)\n",
    "\n",
    "chi2=[]\n",
    "p_value=[]\n",
    "\n",
    "most_frequent=100000\n",
    "\n",
    "for kk in range(3):\n",
    "\n",
    "    mng1_names=random.sample(names_uniques_mng,floor(len(names_uniques_mng)/2))\n",
    "    mng1_ind=[]\n",
    "    mng2_ind=[]\n",
    "\n",
    "    for i in mng1_names:\n",
    "        mng1_ind.extend(list( np.where(names_mng_arrray==i)[0] ))\n",
    "\n",
    "    mng2_ind=list ( set( range(0,len(mng_cleaned)) ) - set(mng1_ind) )\n",
    "\n",
    "#    print(len(mng2_ind)+len(mng1_ind))\n",
    "#    print(len(names_mng))\n",
    "    \n",
    "    mng1_tweets=[]\n",
    "    mng2_tweets=[]\n",
    "\n",
    "    for i in mng1_ind:\n",
    "        mng1_tweets.append(mng_cleaned[i])\n",
    "\n",
    "    for i in mng2_ind:\n",
    "        mng2_tweets.append(mng_cleaned[i])\n",
    "    \n",
    "           \n",
    "    mng1_words_random=[]\n",
    "    for tweet in mng1_tweets:\n",
    "        mng1_words_random += tweet.split()  \n",
    "    \n",
    "    mng2_words_random=[]\n",
    "    for tweet in mng2_tweets:\n",
    "        mng2_words_random += tweet.split()  \n",
    "    \n",
    "    \n",
    "    all_words=mng1_words_random+mng2_words_random\n",
    "    \n",
    "#    rang=range(len(mng_words))\n",
    "#    slice1_ind=random.sample(rang,floor(len(mng_words)/4))\n",
    "#    temp=list( set(rang) - set(slice1_ind)  ) # list(a.diference(b))\n",
    "#    slice2_ind=random.sample(temp,floor(len(temp)/3))\n",
    "    \n",
    "#    mng1_words_random=[]\n",
    "#    for ii in slice1_ind:\n",
    "#        mng1_words_random.append(mng_words[ii])\n",
    "        \n",
    "#    mng2_words_random=[]\n",
    "#    for ii in slice2_ind:\n",
    "#        mng2_words_random.append(mng_words[ii])\n",
    "        \n",
    "#    all_words=mng_words\n",
    "    \n",
    "    \n",
    "    d_mng1 = dict()\n",
    "    for c in mng1_words_random:\n",
    "        if c not in d_mng1:\n",
    "            d_mng1[c] = 1\n",
    "        else:\n",
    "            d_mng1[c] = d_mng1[c] + 1\n",
    "\n",
    "    d_mng1_new=dict(sorted(d_mng1.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_mng1=list(d_mng1_new.keys())\n",
    "    values_list_mng1=list(d_mng1_new.values())\n",
    "    frequent_words_mng1=set(list(d_mng1_new.keys())[0:most_frequent])\n",
    "\n",
    "#    print(list(d_mng1_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng1_new.values())[0:most_frequent]) \n",
    "#    print()\n",
    "    \n",
    "    d_mng2 = dict()\n",
    "    for c in mng2_words_random:\n",
    "        if c not in d_mng2:\n",
    "            d_mng2[c] = 1\n",
    "        else:\n",
    "            d_mng2[c] = d_mng2[c] + 1\n",
    "            \n",
    "#    print(list(d_mng2_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng2_new.values())[0:most_frequent])   \n",
    "#    print()\n",
    "\n",
    "    d_mng2_new=dict(sorted(d_mng2.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_mng2=list(d_mng2_new.keys())\n",
    "    values_list_mng2=list(d_mng2_new.values())\n",
    "    frequent_words_mng2=set(list(d_mng2_new.keys())[0:most_frequent])\n",
    "\n",
    "#    print(list(d_all_new.keys())[0:most_frequent])\n",
    "#    print(list(d_all_new.values())[0:most_frequent]) \n",
    "#    print()\n",
    "\n",
    "    d_all = dict()\n",
    "    for c in all_words:\n",
    "        if c not in d_all:\n",
    "            d_all[c] = 1\n",
    "        else:\n",
    "            d_all[c] = d_all[c] + 1\n",
    "\n",
    "    d_all_new=dict(sorted(d_all.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_all=list(d_all_new.keys())\n",
    "    values_list_all=list(d_all_new.values())\n",
    "        \n",
    "\n",
    "    frequent=key_list_all[0:most_frequent] # \n",
    "#    print(frequent)\n",
    "\n",
    "\n",
    "    text1_freq_values=[]\n",
    "    text2_freq_values=[]\n",
    "\n",
    "    for ii in frequent:\n",
    "\n",
    "        try: \n",
    "            text1_freq_values.append(d_mng1_new[ii])\n",
    "        except:\n",
    "            text1_freq_values.append(0)\n",
    "        try:    \n",
    "            text2_freq_values.append(d_mng2_new[ii])\n",
    "        except:\n",
    "            text2_freq_values.append(0)\n",
    "        \n",
    "        \n",
    "#    print(text1_freq_values)  \n",
    "#    print(text2_freq_values)  \n",
    "\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency([text1_freq_values,text2_freq_values])\n",
    "    chi2.append(chi2_stat)\n",
    "    p_value.append(p_val)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection : {'go', 'time', 'rt', 'get', 'amp', \"'s\", \"n't\", 'like', 'one', '’'}\n",
      "\n",
      "mng1 - mng2 : set()\n",
      "\n",
      "mng2 - mng1 : set()\n"
     ]
    }
   ],
   "source": [
    "print(\"intersection :\", frequent_words_mng1 & frequent_words_mng2)\n",
    "print()\n",
    "print(\"mng1 - mng2 :\",frequent_words_mng1 - frequent_words_mng2)\n",
    "print()\n",
    "print(\"mng2 - mng1 :\",frequent_words_mng2 - frequent_words_mng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [619.5981551475755, 978.6924895832485, 337.66668763536666]\n",
      "CHI2_mean: 645.3191107887302\n",
      "CHI2_std: 262.32892248733833\n",
      "p_value: [1.300243272041132e-127, 6.772512269953577e-205, 2.6068304538820288e-67]\n",
      "P_VALUE_mean: 8.68943484627343e-68\n",
      "P_VALUE_std: 1.2288716608957254e-67\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 500 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 485\n",
      "\n",
      "mng1 - mng2 : {'forget', 'eye', 'light', 'goe', 'hi', 'inspir', 'featur', 'bless', 'secur', 'im', 'blog', 'saturday', 'room', 'sleep', 'woman'}\n",
      "\n",
      "mng2 - mng1 : {'major', 'american', 'sport', 'invest', 'act', 'fund', 'fantast', 'london', '—', 'app', 'near', 'price', 'return', 'cup', 'law'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len(frequent_words_mng1 & frequent_words_mng2))\n",
    "print()\n",
    "print(\"mng1 - mng2 :\",frequent_words_mng1 - frequent_words_mng2)\n",
    "print()\n",
    "print(\"mng2 - mng1 :\",frequent_words_mng2 - frequent_words_mng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [13673.411211268034, 13286.904405736845, 23081.554700226745]\n",
      "CHI2_mean: 16680.623439077208\n",
      "CHI2_std: 4528.891522595166\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 1000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 967\n",
      "mng1 - mng2 : {'except', 'teach', 'engin', 'felt', 'sever', 'david', 'middl', 'gun', 'ride', 'ice', 'bitch', 'text', 'station', 'financi', 'mix', 'whether', 'boss', 'target', 'repres', 'ha', 'tag', 'suck', 'wake', 'regist', 'hair', 'effort', 'lucki', 'proper', 'oper', 'adam', 'till', 'potenti', 'strategi'}\n",
      "mng2 - mng1 : {'advic', 'minist', 'https', 'octob', 'click', 'appl', 'fm', 'aw', 'smile', 'nigerian', 'dress', 'progress', 'tree', 'mobil', 'virtual', 'ps', 'feed', 'trend', 'fear', 'yo', 'shout', 'volunt', 'battl', 'search', 'june', 'quot', 'juli', 'struggl', 'wall', 'press', 'lord', 'debat', 'liverpool'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng1 & frequent_words_mng2))\n",
    "print(\"mng1 - mng2 :\",frequent_words_mng1 - frequent_words_mng2)\n",
    "print(\"mng2 - mng1 :\",frequent_words_mng2 - frequent_words_mng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [27949.95875515436, 23320.17260019394, 22876.969214769306]\n",
      "CHI2_mean: 24715.700190039202\n",
      "CHI2_std: 2294.112560407406\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 8986\n",
      "length of mng1 - mng2 : 1014\n",
      "length of mng2 - mng1 : 1014\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng1 & frequent_words_mng2))\n",
    "print(\"length of mng1 - mng2 :\",len(frequent_words_mng1 - frequent_words_mng2))\n",
    "print(\"length of mng2 - mng1 :\",len(frequent_words_mng2 - frequent_words_mng1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [342901.7868000346, 343853.11857659166, 344769.2932023014]\n",
      "CHI2_mean: 343841.39952630916\n",
      "CHI2_std: 762.4513286024703\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 100000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 65974\n",
      "length of mng1 - mng2 : 34026\n",
      "length of mng2 - mng1 : 34026\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_mng1 & frequent_words_mng2))\n",
    "print(\"length of mng1 - mng2 :\",len(frequent_words_mng1 - frequent_words_mng2))\n",
    "print(\"length of mng2 - mng1 :\",len(frequent_words_mng2 - frequent_words_mng1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [1048804.4350262857, 1067550.064007554, 1069925.2849947955]\n",
      "CHI2_mean: 1062093.2613428782\n",
      "CHI2_std: 9446.51954739531\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For *intra similarity* for ent set (based on random selection of tweets) we have** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2=[]\n",
    "p_value=[]\n",
    "\n",
    "Ent1_words=ent_words[:floor(len(ent_words)/2)]\n",
    "Ent2_words=ent_words[floor(len(ent_words)/2):]\n",
    "\n",
    "most_frequent=100000\n",
    "\n",
    "for kk in range(10):\n",
    "  \n",
    "    ent1_words_random=random.sample(Ent1_words,floor(len(Ent1_words)/2))\n",
    "    ent2_words_random=random.sample(Ent2_words,floor(len(Ent2_words)/2))   \n",
    "    all_words=ent1_words_random+ent2_words_random\n",
    "    \n",
    "#    rang=range(len(ent_words))\n",
    "#    slice1_ind=random.sample(rang,floor(len(ent_words)/4))\n",
    "#    temp=list( set(rang) - set(slice1_ind)  ) # list(a.diference(b))\n",
    "#    slice2_ind=random.sample(temp,floor(len(temp)/3))\n",
    "    \n",
    "#    ent1_words_random=[]\n",
    "#    for ii in slice1_ind:\n",
    "#        ent1_words_random.append(ent_words[ii])\n",
    "        \n",
    "#    ent2_words_random=[]\n",
    "#    for ii in slice2_ind:\n",
    "#        ent2_words_random.append(ent_words[ii])\n",
    "        \n",
    "#    all_words=ent_words\n",
    "    \n",
    "    \n",
    "    d_ent1 = dict()\n",
    "    for c in ent1_words_random:\n",
    "        if c not in d_ent1:\n",
    "            d_ent1[c] = 1\n",
    "        else:\n",
    "            d_ent1[c] = d_ent1[c] + 1\n",
    "\n",
    "    d_ent1_new=dict(sorted(d_ent1.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_ent1=list(d_ent1_new.keys())\n",
    "    values_list_ent1=list(d_ent1_new.values())\n",
    "    frequent_words_ent1=set(list(d_ent1_new.keys())[0:most_frequent])\n",
    "\n",
    "#    print(list(d_ent1_new.keys())[0:10])\n",
    "#    print(list(d_ent1_new.values())[0:10]) \n",
    "#    print()\n",
    "    \n",
    "    d_ent2 = dict()\n",
    "    for c in ent2_words_random:\n",
    "        if c not in d_ent2:\n",
    "            d_ent2[c] = 1\n",
    "        else:\n",
    "            d_ent2[c] = d_ent2[c] + 1\n",
    "            \n",
    "#    print(list(d_ent2_new.keys())[0:most_frequent])\n",
    "#    print(list(d_ent2_new.values())[0:most_frequent])   \n",
    "#    print()\n",
    "\n",
    "    d_ent2_new=dict(sorted(d_ent2.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_ent2=list(d_ent2_new.keys())\n",
    "    values_list_ent2=list(d_ent2_new.values())\n",
    "    frequent_words_ent2=set(list(d_ent2_new.keys())[0:most_frequent])\n",
    "    \n",
    "#    print(list(d_all_new.keys())[0:most_frequent])\n",
    "#    print(list(d_all_new.values())[0:most_frequent]) \n",
    "#    print()\n",
    "\n",
    "    d_all = dict()\n",
    "    for c in all_words:\n",
    "        if c not in d_all:\n",
    "            d_all[c] = 1\n",
    "        else:\n",
    "            d_all[c] = d_all[c] + 1\n",
    "\n",
    "    d_all_new=dict(sorted(d_all.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_all=list(d_all_new.keys())\n",
    "    values_list_all=list(d_all_new.values())\n",
    "        \n",
    "\n",
    "    frequent=key_list_all[0:most_frequent] # \n",
    "#    print(frequent)\n",
    "\n",
    "\n",
    "    text1_freq_values=[]\n",
    "    text2_freq_values=[]\n",
    "\n",
    "    for ii in frequent:\n",
    "\n",
    "        try: \n",
    "            text1_freq_values.append(d_ent1_new[ii])\n",
    "        except:\n",
    "            text1_freq_values.append(0)\n",
    "        try:    \n",
    "            text2_freq_values.append(d_ent2_new[ii])\n",
    "        except:\n",
    "            text2_freq_values.append(0)\n",
    "        \n",
    "        \n",
    "#    print(text1_freq_values)  \n",
    "#    print(text2_freq_values)  \n",
    "\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency([text1_freq_values,text2_freq_values])\n",
    "    chi2.append(chi2_stat)\n",
    "    p_value.append(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10 most frequent words we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection : {'go', 'rt', 'like', 'get', 'amp', \"'s\", \"n't\", 'peopl', 'one', '’'}\n",
      "\n",
      "ent1 - ent2 : set()\n",
      "\n",
      "ent2 - ent1 : set()\n"
     ]
    }
   ],
   "source": [
    "print(\"intersection :\", frequent_words_ent1 & frequent_words_ent2)\n",
    "print()\n",
    "print(\"ent1 - ent2 :\",frequent_words_ent1 - frequent_words_ent2)\n",
    "print()\n",
    "print(\"ent2 - ent1 :\",frequent_words_ent2 - frequent_words_ent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 500 most frequent words we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 476\n",
      "\n",
      "ent1 - ent2 : {'age', 'facebook', 'space', 'career', 'uk', 'pictur', 'yeah', 'damn', 'w', 'christma', 'earn', 'ok', 'took', 'bitcoin', 'quot', 'total', 'sleep', 'rest', 'date', 'strong', 'relationship', 'sorri', 'control', 'came'}\n",
      "\n",
      "ent2 - ent1 : {'song', 'googl', 'african', 'current', 'democrat', 'forc', '\\u2066', 'podcast', 'rule', 'entrepreneurship', 'fail', 'blog', 'content', 'short', 'dey', 'intern', 'global', 'award', 'cours', 'announc', 'special', 'perform', 'program', 'photo'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\",len( frequent_words_ent1 & frequent_words_ent2))\n",
    "print()\n",
    "print(\"ent1 - ent2 :\",frequent_words_ent1 - frequent_words_ent2)\n",
    "print()\n",
    "print(\"ent2 - ent1 :\",frequent_words_ent2 - frequent_words_ent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 1000 most frequent words we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 971\n",
      "\n",
      "ent1 - ent2 : {'justic', 'hustl', 'confid', 'origin', 'everyday', 'tho', 'publish', 'eu', 'throw', 'vision', 'daughter', 'ball', 'ppl', 'gun', 'dress', 'incom', 'environ', 'transform', 'bar', 'loss', 'pro', 'broke', 'construct', 'pic', 'door', 'film', 'coupl', 'amazon', 'congress'}\n",
      "\n",
      "ent2 - ent1 : {'grate', 's…', 'imag', 'lesson', 'fli', 'green', 'oil', 'simpli', 'bbnaija', 'color', 'anim', 'politician', 'contact', 'to…', 'mobil', 'passion', 'session', 'north', 'climat', 'target', 'download', 'legal', 'wine', 'corpor', 'applic', 'ghana', 'doctor', 'gold', 'financ'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len(frequent_words_ent1 & frequent_words_ent2))\n",
    "print()\n",
    "print(\"ent1 - ent2 :\",frequent_words_ent1 - frequent_words_ent2)\n",
    "print()\n",
    "print(\"ent2 - ent1 :\",frequent_words_ent2 - frequent_words_ent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10000 most frequent words we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 8868\n",
      "length of ent1 - ent2 : 1132\n",
      "length of ent2 - ent1 : 1132\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_ent1 & frequent_words_ent2))\n",
    "print(\"length of ent1 - ent2 :\",len(frequent_words_ent1 - frequent_words_ent2))\n",
    "print(\"length of ent2 - ent1 :\",len(frequent_words_ent2 - frequent_words_ent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 100000 most frequent words we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 59810\n",
      "length of ent1 - ent2 : 40190\n",
      "length of ent2 - ent1 : 40190\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_ent1 & frequent_words_ent2))\n",
    "print(\"length of ent1 - ent2 :\",len(frequent_words_ent1 - frequent_words_ent2))\n",
    "print(\"length of ent2 - ent1 :\",len(frequent_words_ent2 - frequent_words_ent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 100000 most  frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [576208.4150456071, 574661.8505981333, 577581.1059035398, 575276.0820123482, 575063.2360434899, 576179.0066607032, 575651.6195951918, 576315.3709312071, 575787.5643277803, 576799.802210231]\n",
      "CHI2_mean: 575952.4053328232\n",
      "CHI2_std: 815.5115925839615\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20 most frequnt words in subcorpus1:\n",
      "['rt', '’', \"'s\", \"n't\", 'get', 'like', 'one', 'amp', 'peopl', 'go', 'time', 'make', 'know', 'thank', 'need', 'day', 'love', 'year', 'new', 'see']\n",
      "\n",
      " 20 most frequnt words in subcorpus2:\n",
      "['rt', '’', \"'s\", \"n't\", 'get', 'like', 'amp', 'one', 'peopl', 'go', 'time', 'make', 'know', 'need', 'day', 'thank', 'year', 'see', 'love', 'new']\n"
     ]
    }
   ],
   "source": [
    "print('\\n 20 most frequnt words in subcorpus1:')\n",
    "print(key_list_ent1[0:20])\n",
    "\n",
    "print('\\n 20 most frequnt words in subcorpus2:')\n",
    "print(key_list_ent2[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [210116.19711700024, 209477.44093518754, 210069.1024264167, 208229.273002893, 209584.5528074744, 209297.62779014735, 209437.87388814669, 209480.86938200204, 209326.94025798247, 208652.25942542887]\n",
      "CHI2_mean: 209367.21370326792\n",
      "CHI2_std: 542.5360397530483\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 1000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [18813.08486138571, 18934.35320638869, 18633.896912288754, 19016.082386651433, 18743.274657388363, 18770.132214410987, 18862.696967252097, 18672.39437745939, 18807.136738967485, 18537.27660420405]\n",
      "CHI2_mean: 18779.032892639698\n",
      "CHI2_std: 134.7242015306235\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 500 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [10325.094093889691, 10455.738005414716, 10442.573969204768, 10438.432011415407, 10231.586420606523, 10742.718978990331, 10514.667883942668, 10321.467940775769, 10145.641933712534, 10177.019247524517]\n",
      "CHI2_mean: 10379.49404854769\n",
      "CHI2_std: 169.51581755154095\n",
      "p_value: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean) \n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [606.4668260531298, 506.79421966525723, 546.9847736533058, 541.7317247365197, 598.7047051520464, 602.0134239534013, 518.8164472149542, 547.661744586614, 552.2387270824483, 509.29640086239715]\n",
      "CHI2_mean: 553.0708992960075\n",
      "CHI2_std: 35.71500314913203\n",
      "p_value: [8.57042817861445e-125, 2.0170250683029095e-103, 4.931768441929706e-112, 6.592855502058916e-111, 3.9720128378974735e-123, 7.742490893518853e-124, 5.365562185531523e-106, 3.5308030159353087e-112, 3.68636810789653e-113, 5.872543580594697e-104]\n",
      "P_VALUE_mean: 2.609645063307674e-104\n",
      "P_VALUE_std: 6.109224059432716e-104\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For *intra similarity* for ent set based on screen names we have** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_uniques_ent=list(np.unique(names_ent))\n",
    "names_ent_arrray = np.array(names_ent)\n",
    " \n",
    "chi2=[]\n",
    "p_value=[]\n",
    "\n",
    "most_frequent=100000\n",
    "\n",
    "for kk in range(3):\n",
    "  \n",
    "    ent1_names=random.sample(names_uniques_ent,floor(len(names_uniques_ent)/2))\n",
    "    ent1_ind=[]\n",
    "    ent2_ind=[]\n",
    "\n",
    "    for i in ent1_names:\n",
    "        ent1_ind.extend(list( np.where(names_ent_arrray==i)[0] ))\n",
    "\n",
    "    ent2_ind=list ( set( range(0,len(ent_cleaned)) ) - set(ent1_ind) )\n",
    "\n",
    "#    print(len(mentng2_ind)+len(ent1_ind))\n",
    "#    print(len(names_ent))\n",
    "    \n",
    "    ent1_tweets=[]\n",
    "    ent2_tweets=[]\n",
    "\n",
    "    for i in ent1_ind:\n",
    "        ent1_tweets.append(ent_cleaned[i])\n",
    "\n",
    "    for i in ent2_ind:\n",
    "        ent2_tweets.append(ent_cleaned[i])\n",
    "        \n",
    "        \n",
    "           \n",
    "    ent1_words_random=[]\n",
    "    for tweet in ent1_tweets:\n",
    "        ent1_words_random += tweet.split()  \n",
    "    \n",
    "    ent2_words_random=[]\n",
    "    for tweet in ent2_tweets:\n",
    "        ent2_words_random += tweet.split() \n",
    "        \n",
    "        \n",
    "    all_words=ent1_words_random+ent2_words_random\n",
    "    \n",
    "#    rang=range(len(ent_words))\n",
    "#    slice1_ind=random.sample(rang,floor(len(ent_words)/4))\n",
    "#    temp=list( set(rang) - set(slice1_ind)  ) # list(a.diference(b))\n",
    "#    slice2_ind=random.sample(temp,floor(len(temp)/3))\n",
    "    \n",
    "#    ent1_words_random=[]\n",
    "#    for ii in slice1_ind:\n",
    "#        ent1_words_random.append(ent_words[ii])\n",
    "        \n",
    "#    ent2_words_random=[]\n",
    "#    for ii in slice2_ind:\n",
    "#        ent2_words_random.append(ent_words[ii])\n",
    "        \n",
    "#    all_words=ent_words\n",
    "    \n",
    "    \n",
    "    d_ent1 = dict()\n",
    "    for c in ent1_words_random:\n",
    "        if c not in d_ent1:\n",
    "            d_ent1[c] = 1\n",
    "        else:\n",
    "            d_ent1[c] = d_ent1[c] + 1\n",
    "\n",
    "    d_ent1_new=dict(sorted(d_ent1.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_ent1=list(d_ent1_new.keys())\n",
    "    values_list_ent1=list(d_ent1_new.values())\n",
    "    frequent_words_ent1=set(list(d_ent1_new.keys())[0:most_frequent])\n",
    "\n",
    "#    print(list(d_ent1_new.keys())[0:10])\n",
    "#    print(list(d_ent1_new.values())[0:10]) \n",
    "#    print()\n",
    "    \n",
    "    d_ent2 = dict()\n",
    "    for c in ent2_words_random:\n",
    "        if c not in d_ent2:\n",
    "            d_ent2[c] = 1\n",
    "        else:\n",
    "            d_ent2[c] = d_ent2[c] + 1\n",
    "            \n",
    "#    print(list(d_ent2_new.keys())[0:most_frequent])\n",
    "#    print(list(d_ent2_new.values())[0:most_frequent])   \n",
    "#    print()\n",
    "\n",
    "    d_ent2_new=dict(sorted(d_ent2.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_ent2=list(d_ent2_new.keys())\n",
    "    values_list_ent2=list(d_ent2_new.values())\n",
    "    frequent_words_ent2=set(list(d_ent2_new.keys())[0:most_frequent])\n",
    "\n",
    "#    print(list(d_all_new.keys())[0:most_frequent])\n",
    "#    print(list(d_all_new.values())[0:most_frequent]) \n",
    "#    print()\n",
    "\n",
    "    d_all = dict()\n",
    "    for c in all_words:\n",
    "        if c not in d_all:\n",
    "            d_all[c] = 1\n",
    "        else:\n",
    "            d_all[c] = d_all[c] + 1\n",
    "\n",
    "    d_all_new=dict(sorted(d_all.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_all=list(d_all_new.keys())\n",
    "    values_list_all=list(d_all_new.values())\n",
    "        \n",
    "\n",
    "    frequent=key_list_all[0:most_frequent] # \n",
    "#    print(frequent)\n",
    "\n",
    "\n",
    "    text1_freq_values=[]\n",
    "    text2_freq_values=[]\n",
    "\n",
    "    for ii in frequent:\n",
    "\n",
    "        try: \n",
    "            text1_freq_values.append(d_ent1_new[ii])\n",
    "        except:\n",
    "            text1_freq_values.append(0)\n",
    "        try:    \n",
    "            text2_freq_values.append(d_ent2_new[ii])\n",
    "        except:\n",
    "            text2_freq_values.append(0)\n",
    "        \n",
    "        \n",
    "#    print(text1_freq_values)  \n",
    "#    print(text2_freq_values)  \n",
    "\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency([text1_freq_values,text2_freq_values])\n",
    "    chi2.append(chi2_stat)\n",
    "    p_value.append(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection : {'go', 'rt', 'like', 'get', 'amp', \"'s\", \"n't\", 'peopl', 'one', '’'}\n",
      "ent1 - ent2 : set()\n",
      "ent2 - ent1 : set()\n"
     ]
    }
   ],
   "source": [
    "print(\"intersection :\", frequent_words_ent1 & frequent_words_ent2)\n",
    "print(\"ent1 - ent2 :\",frequent_words_ent1 - frequent_words_ent2)\n",
    "print(\"ent2 - ent1 :\",frequent_words_ent2 - frequent_words_ent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [182.79885432469325, 462.4556873005797, 1246.1615650840884]\n",
      "CHI2_mean: 630.4720355697872\n",
      "CHI2_std: 450.07938827328525\n",
      "p_value: [1.318731166211504e-34, 6.224644932152249e-94, 1.3096577842596867e-262]\n",
      "P_VALUE_mean: 4.395770554038346e-35\n",
      "P_VALUE_std: 6.216558334601324e-35\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 500 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 477\n",
      "ent1 - ent2 : {'facebook', 'space', 'googl', 'african', 'travel', 'yesterday', 'south', 'ceo', 'damn', 'podcast', 'entrepreneurship', 'blog', 'content', 'w', 'short', 'earn', 'dey', 'quot', 'cost', 'strong', 'motiv', 'program', 'photo'}\n",
      " ent2 - ent1 : {'major', 'age', 'releas', 'deserv', 'forc', 'pictur', '\\u2066', 'articl', 'rule', 'class', 'protect', 'christma', 'creativ', 'global', 'total', 'pray', 'date', 'announc', 'special', 'worth', 'inform', 'dm', 'came'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_ent1 & frequent_words_ent2))\n",
    "print(\"ent1 - ent2 :\",frequent_words_ent1 - frequent_words_ent2)\n",
    "print(\" ent2 - ent1 :\",frequent_words_ent2 - frequent_words_ent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [17386.118028532743, 14952.065389378684, 21635.82656040213]\n",
      "CHI2_mean: 17991.33665943785\n",
      "CHI2_std: 2761.989988060078\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 1000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 973\n",
      "ent1 - ent2 : {'confid', 'involv', 'oil', 'tho', 'simpli', 'publish', 'eu', 'vision', 'contact', 'mobil', 'passion', 'north', 'whether', 'seek', 'loss', 'legal', 'corpor', 'statement', 'de', 'brexit', 'broke', 'awar', 'film', 'channel', 'round', 'ft', 'doubt'}\n",
      " ent2 - ent1 : {'hustl', 'wit', 's…', 'imag', 'origin', 'fli', 'stress', 'color', 'site', 'politician', 'execut', 'to…', 'forev', 'incom', 'environ', 'l', 'master', 'transform', 'wine', 'debt', 'pro', 'construct', 'digitalmarket', 'tune', 'amazon', 'gold', 'financ'}\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_ent1 & frequent_words_ent2))\n",
    "print(\"ent1 - ent2 :\",frequent_words_ent1 - frequent_words_ent2)\n",
    "print(\" ent2 - ent1 :\",frequent_words_ent2 - frequent_words_ent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [31500.899954195218, 38536.08550424666, 32863.48058337947]\n",
      "CHI2_mean: 34300.15534727378\n",
      "CHI2_std: 3046.4716984101715\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 10000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 8923\n",
      "length of ent1 - ent2 : 1077\n",
      "length of ent2 - ent1 : 1077\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_ent1 & frequent_words_ent2))\n",
    "print(\"length of ent1 - ent2 :\",len(frequent_words_ent1 - frequent_words_ent2))\n",
    "print(\"length of ent2 - ent1 :\",len(frequent_words_ent2 - frequent_words_ent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [398049.052280003, 408357.82352811756, 399441.147789167]\n",
      "CHI2_mean: 401949.3411990958\n",
      "CHI2_std: 4566.980534057949\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 100000 most frequent words we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of intersection : 66265\n",
      "length of ent1 - ent2 : 33735\n",
      "length of ent2 - ent1 : 33735\n"
     ]
    }
   ],
   "source": [
    "print(\"length of intersection :\", len (frequent_words_ent1 & frequent_words_ent2))\n",
    "print(\"length of ent1 - ent2 :\",len(frequent_words_ent1 - frequent_words_ent2))\n",
    "print(\"length of ent2 - ent1 :\",len(frequent_words_ent2 - frequent_words_ent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2: [1065781.5214649732, 1049217.985150508, 1042422.4586670494]\n",
      "CHI2_mean: 1052473.9884275103\n",
      "CHI2_std: 9810.287855247478\n",
      "p_value: [0.0, 0.0, 0.0]\n",
      "P_VALUE_mean: 0.0\n",
      "P_VALUE_std: 0.0\n"
     ]
    }
   ],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no inter similarity (between ent and mng sets) and intra similarity (for both mng and ent sets based on random selection of tweets and also based on screen names). Even the difference of frequent words of sets becomes more when the number of most frequent words increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=[20, 6, 30,44]\n",
    "a2=[180, 34,50,36]\n",
    "chi2_stat, p_val, dof, ex = stats.chi2_contingency([a1,a2])\n",
    "print(chi2_stat)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=[20, 6,   30, 44]\n",
    "a2=[21, 6.5, 30, 42]\n",
    "chi2_stat, p_val, dof, ex = stats.chi2_contingency([a1,a2])\n",
    "print(chi2_stat)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=[20, 6,  30, 44]\n",
    "a2=[40, 12, 60, 88]\n",
    "chi2_stat, p_val, dof, ex = stats.chi2_contingency([a1,a2])\n",
    "print(chi2_stat)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
