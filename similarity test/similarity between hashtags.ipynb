{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ent _hashtags is a list of cleaned hashtags for ent set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_words=[]\n",
    "for tweet in ent_hashtags:\n",
    "    ent_words += tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_words=[]\n",
    "for tweet in mng_hashtags:\n",
    "    mng_words += tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2=[]\n",
    "p_value=[]\n",
    "\n",
    "# do this code for different most_frequent\n",
    "most_frequent=10 # 500, 1000, 10000, 100000\n",
    "\n",
    "for kk in range(10):\n",
    "    \n",
    "    ent_size=floor(len(ent_words)/2)\n",
    "    ent_words_random=random.sample(ent_words,ent_size)\n",
    "    \n",
    "    mng_size=floor(len(mng_words)/2)\n",
    "    mng_words_random=random.sample(mng_words,mng_size)\n",
    "    \n",
    "    all_words=mng_words_random+ent_words_random\n",
    "\n",
    "\n",
    "    d_mng = dict()\n",
    "    for c in mng_words_random:\n",
    "        if c not in d_mng:\n",
    "            d_mng[c] = 1\n",
    "        else:\n",
    "            d_mng[c] = d_mng[c] + 1\n",
    "\n",
    "    d_mng_new=dict(sorted(d_mng.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_mng=list(d_mng_new.keys())\n",
    "    values_list_mng=list(d_mng_new.values())\n",
    "    frequent_words_mng=set(list(d_mng_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng_new.keys())[0:most_frequent])\n",
    "#    print(list(d_mng_new.values())[0:most_frequent])        \n",
    "\n",
    "    d_ent = dict()\n",
    "    for c in ent_words_random:\n",
    "        if c not in d_ent:\n",
    "            d_ent[c] = 1\n",
    "        else:\n",
    "            d_ent[c] = d_ent[c] + 1\n",
    "\n",
    "    d_ent_new=dict(sorted(d_ent.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_ent=list(d_ent_new.keys())\n",
    "    values_list_ent=list(d_ent_new.values())\n",
    "    frequent_words_ent=set(list(d_ent_new.keys())[0:most_frequent])\n",
    "#    print(list(d_ent_new.keys())[0:most_frequent])\n",
    "#    print(list(d_ent_new.values())[0:most_frequent])       \n",
    "\n",
    "    d_all = dict()\n",
    "    for c in all_words:\n",
    "        if c not in d_all:\n",
    "            d_all[c] = 1\n",
    "        else:\n",
    "            d_all[c] = d_all[c] + 1\n",
    "\n",
    "    d_all_new=dict(sorted(d_all.items(), key=lambda x: x[1], reverse=True))\n",
    "    key_list_all=list(d_all_new.keys())\n",
    "    values_list_all=list(d_all_new.values())\n",
    "#    print(list(d_all_new.keys())[0:most_frequent])\n",
    "#    print(list(d_all_new.values())[0:most_frequent])        \n",
    "\n",
    "#    print(len(key_list_all))\n",
    "#    print(len(key_list_mng))\n",
    "#    print(len(key_list_ent))\n",
    "\n",
    "    frequent=key_list_all[0:most_frequent] # \n",
    "    #frequent\n",
    "\n",
    "    text1_freq_values=[]\n",
    "    text2_freq_values=[]\n",
    "\n",
    "    for ii in frequent:\n",
    "    #    print(ii)\n",
    "    #    print(d_ent_new[ii])\n",
    "    #    print(d_mng_new[ii])\n",
    "        try: \n",
    "            text1_freq_values.append(d_ent_new[ii])\n",
    "        except:\n",
    "            text1_freq_values.append(0)\n",
    "        try:    \n",
    "            text2_freq_values.append(d_mng_new[ii])\n",
    "        except:\n",
    "            text2_freq_values.append(0)\n",
    "            \n",
    "            \n",
    "#        except:\n",
    "#            print(ii)\n",
    "#            print(d_mng_new[ii])\n",
    "#            print(d_ent_new[ii])\n",
    "\n",
    "    #print(text1_freq_values )  \n",
    "    #print(text2_freq_values)    \n",
    "\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency([text1_freq_values,text2_freq_values])\n",
    "#    print(chi2_stat)\n",
    "#    print(p_val)\n",
    "    chi2.append(chi2_stat)\n",
    "    p_value.append(p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each most_frequent, find the below results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"intersection :\", frequent_words_mng & frequent_words_ent)\n",
    "print()\n",
    "print(\"mng - ent :\",frequent_words_mng - frequent_words_ent)\n",
    "print()\n",
    "print(\"ent- mng :\",frequent_words_ent - frequent_words_mng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHI2_mean=np.mean(chi2)\n",
    "CHI2_std=np.std(chi2)\n",
    "P_VALUE_mean=np.mean(p_value)\n",
    "P_VALUE_std=np.std(p_value)\n",
    "\n",
    "print('chi2:',chi2)\n",
    "print('CHI2_mean:',CHI2_mean)\n",
    "print('CHI2_std:',CHI2_std)\n",
    "print('p_value:',p_value)\n",
    "print('P_VALUE_mean:',P_VALUE_mean)\n",
    "print('P_VALUE_std:',P_VALUE_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
