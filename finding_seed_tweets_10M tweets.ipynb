{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "#### export PYTHONIOENCODING=utf-8  # at cmd of linux\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "import html\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "import operator\n",
    "from statistics import mean, median ,mode,variance,stdev, pvariance, pstdev\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import fpdf\n",
    "import pickle\n",
    "import xml.sax.saxutils as saxutils\n",
    "import sys\n",
    "import pandas\n",
    "import csv\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(cachedStopWords))\n",
    "#print(len(cachedStopWords))\n",
    "#print(type(cachedStopWords))\n",
    "nltk_stopwords=set(cachedStopWords)\n",
    "\n",
    "english_alghabet=['b','c','e','f','g','h','j','k','l','n','p','q','r','u','v','w','x','z']\n",
    "\n",
    "numbers_remove=['one','two','three','four','five','six','seven','eight','nine','ten','tens','twenty',\n",
    "                'fourty','fifty','sixty','seventy','eighty','ninety','hundred','hundreds','million','billion','trillion',\n",
    "                'millions','thousand','thousands','second','third','forth','tenth','billions','trillions'] \n",
    "\n",
    "\n",
    "miscellaneous_remove=['absolutely', 'actually', 'adieu', 'ain', \"ain't\", 'aint', 'almost',\n",
    "                       'awesome','awfully','amazing','interesting',\n",
    "                       'alright','alrighty', 'amoungst', 'anybody', 'anymore', 'anyways', 'apart', 'apparently', 'anytime',\n",
    "                       'appropriate',  'approximately', 'arent', 'behold', 'better', 'bravo','briefly','bad','best','brilliant',\n",
    "                       'bye', 'cant', 'certainly', 'chrissakes', 'clearly', 'completely',\n",
    "                       'congrat', 'congrats','congratulation', 'congratulations', 'consequently', 'cool', 'couldnt',\n",
    "                       'darnit', 'de','dear', 'definitely','disappointing', 'didn', 'doesn', 'don', 'downwards',\n",
    "                       'disgusting','dude','down','eg',\"e.g.\",'i.e.',\n",
    "                       'encore','entirely', 'especially', 'et', 'etc', 'everybody', 'ex', 'exactly', 'excellent',\n",
    "                       'fantastic','far', 'farewell','funny',\n",
    "                       'felicitation', 'felicitations','finally', 'fully','furthermore', 'gadzooks', \n",
    "                       'good', 'goodby','goodness', 'gracious', 'great', \n",
    "                       'greetings', 'hallo', 'hardly', 'hasnt', 'haven', 'hello', 'here','hi', 'hither','higher','hopefully',\n",
    "                       'here','there','including',\n",
    "                       'howbeit', 'ie', 'immediately', 'inasmuch', 'inner', 'insofar', 'instead', 'inward', 'important',\n",
    "                       'indeed','just', \"it'd\", \"it'll\", 'inside','kertyschoo', 'kg', 'km', 'lackaday', \n",
    "                       'largely', 'lately', 'later','lovely','large','big','small',\n",
    "                       'lest', 'let', 'lets', 'likely', 'little', 'ltd', 'lower','magnificent', 'mainly', 'marvelous',\n",
    "                       'myself','yourself','yourselves','himself','herself','hisself','ourselves','themsleves',\n",
    "                       'maybe', 'meantime', 'merely', 'minus', 'near', 'nearly', 'necessary', 'never', \n",
    "                       'non', 'normally', 'obviously', 'ok', 'okay', 'ones', 'outside', 'over','other','others','only',\n",
    "                       'overall', 'particular', 'particularly', 'please', 'plus', 'poorly', 'possible','up',\n",
    "                       'possibly', 'potentially', 'predominantly', 'presumably', 'previously','primarily', 'probably',\n",
    "                       'promising',\n",
    "                       'promptly', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref',\n",
    "                       'refs', 'regardless', 'related', 'relatively', 'respectively', 'resulting', 'right', 'sec', \n",
    "                       'secondly','self', 'selves', 'seriously', 'shall', 'shucks','somebody', 'somethan','sorry',\n",
    "                       'somewhat', 'soon', 'late' , 'sorry', 'stupid', 'sub', 'substantially', 'successfully', 'sufficiently',\n",
    "                       'useful',\n",
    "                       'super', 'sure', \"t's\", 'th', 'thank', 'thanks', 'thanx', \"that've\", 'thats', 'there', \"there'll\",\n",
    "                       \"there've\", 'thered', 'thereof', 'therere', 'theres', 'thereto', 'theyd', 'theyre', 'thorough',\n",
    "                       'then','thankfully','too','today','yesterday','tomorrow','night',\"morning\",'afternoon','noon','tonight',\n",
    "                       'evening','day','everyday', 'everynight','todays','nights','mornings','noons','afternoons','days',\n",
    "                       'evenings','week','month','year',\n",
    "                       'thoroughly', 'tnx', 'too','truly', 'twice', 'undoubtedly','unfortunately', 'unlike','unlikely',\n",
    "                       'unto',  'usually', 'vs', 'welcome', 'well', 'went', 'werent', 'what', 'whatever', 'wheres', 'widely',\n",
    "                       'wonderful', 'wont', 'wouldnt', 'wrong', 'worst','worse','www', 'yes', 'youd', 'youre', 'yummy', \n",
    "                       'zoinks','shit','literally','literal','pleasure','effective','fabulous','delighted',\n",
    "                       'saturday','sunday','monday','tuesday','wednesday','thursday', 'friday','past','future','suitable',\n",
    "                       'much','many','less','least','few','lots','lot','fewer','fewset','therefore','pm',\n",
    "                       'afaik', 'br', 'idk','smh','qotd', 'ftw','bfn','yw', 'icymi','fomo','smdh', 'b4','imho',\n",
    "                       'urdddd','fab' ,'delightful','absolute','pleasure','huge','latest','nowadays',\n",
    "                       'january','february','april','june','july','august','september','october',\n",
    "                       'november','december', 'autumn' ,'spring','winter','summer',\n",
    "                       'mr','madam','sir','mrs','easy', 'difficult',\n",
    "                       'weekend','south','north','west','east','asia','africa','europe','america','totally',\n",
    "                       'come', 'comes', 'coming', 'came', 'seems', 'gives', 'gave', 'makes', 'made', 'keeps', 'kept', \n",
    "                       'calls', 'called', 'says', 'saying', 'said', 'goes', 'went', 'gone', 'got', 'saw', 'seen', 'shows',\n",
    "                       'shown', 'took', 'taken', 'uses', 'moved', 'moves', 'puts',\n",
    "                       'using','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "                       'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use',\n",
    "                       'find', 'finds', 'finding','aka',\n",
    "                       'lol' , 'brb', 'lmk', 'ama', 'tbh', 'irl', \"tl;dr\", 'fml', 'bfn' ,' br', 'ht', \"hth\",'j/k', 'lmao' ] \n",
    "                  \n",
    "                      \n",
    "interjection_remove=['aaaahh', 'aaah', 'aaargh', 'aaay', 'aagh', 'aah',\n",
    "                   'aargh', 'achoo', 'adios', 'ah', 'aha', 'ahem', 'ahh', 'ahhh',\n",
    "                   'ahoy', 'alas', 'allo', 'amen', 'areet', 'argh', 'arrggh',\n",
    "                   'arrividerci', 'asap', 'attaboy', 'avaunt', 'aw', 'aw', 'aww',\n",
    "                   'awww', 'ay', 'ay', 'aye', 'ayeaugh', 'bada', 'badum', 'bah',\n",
    "                   'bahaha', 'bam', 'bazinga', 'behold', 'bingce', 'bingo', 'blah',\n",
    "                   'blech', 'bleh', 'blimey', 'bonjour', 'boo', 'booh', 'boohoo',\n",
    "                   'booyah', 'bravo', 'brr', 'brrrr', 'btw', 'bwahaha', 'capeesh',\n",
    "                   'capisce', 'cheerio', 'cheers', 'ciao', 'cor', 'cowabunga',\n",
    "                   'crikey', 'cripes', 'da', 'dabba', 'dah', 'dammit', 'damn', 'dang',\n",
    "                   'darn', 'de', 'dee', 'di', 'dizamn', 'doh', 'doo', 'drat', 'duh',\n",
    "                   'dum', 'eeeek', 'eek', 'eep', 'egad', 'egads', 'eh', 'ehem', 'em',\n",
    "                   'er', 'eureka', 'eww', 'ewww', 'eyh', 'fiddledeedee', 'fie',\n",
    "                   'fore', 'foul', 'fuff', 'gah', 'gak', 'gee', 'geez', 'gesundheit',\n",
    "                   'giddyap', 'golly', 'gosh', 'grr', 'grrrr', 'ha', 'hah', 'haha',\n",
    "                   'hahaha', 'hallelujah', 'halloa', 'harrumph', 'harumph', 'haw',\n",
    "                   'heck', 'heck', 'heeey', 'heh', 'hehe', 'hey', 'hhh', 'hic', 'hm',\n",
    "                   'hmm', 'hmmm', 'hmmmm', 'hmmph', 'hmpf', 'ho', 'hola', 'hoo',\n",
    "                   'hooray', 'howdy', 'hrmm', 'hrmph', 'hrmph', 'hrrmph', 'hu', 'huh',\n",
    "                   'hullo', 'humph', 'hurrah', 'huzza', 'huzzah', 'ich', 'ick',\n",
    "                   'ixnay', 'jeepers', 'jeez', 'kaboom', 'kapow', 'kerwham', 'la',\n",
    "                   'lala', 'lo', 'lordy', 'meh', 'mhm', 'ml', 'mm', 'mmh', 'mmhm',\n",
    "                   'mmm', 'muahaha', 'mwah', 'mwahaha', 'na','nay','nah', 'nanu', 'nooo', 'nope',\n",
    "                   'nuh', 'oh', 'ohh', 'oho', 'oi', 'okeydoke', 'om', 'oof', 'ooh',\n",
    "                   'oomph', 'oooh', 'ooooh', 'oops', 'ouch', 'ow', 'oww', 'oy',\n",
    "                   'oyez', 'oyh', 'pew', 'pff', 'pffh', 'pfft', 'phew', 'phut',\n",
    "                   'phweep', 'phwoar', 'phwoarr', 'poof', 'poogh', 'prethee',\n",
    "                   'prithee', 'prosit', 'pssh', 'psst', 'queep', 'roger', 'salaam',\n",
    "                   'salam', 'sheesh', 'shh', 'shhh', 'shitfire', 'shoo', 'shoop',\n",
    "                   'shush', 'sigh', 'sssh', 'strewth', 'ta', 'tarnations', 'tchah',\n",
    "                   'teehee', 'tish', 'touché', 'tsk', 'tss', 'tut', 'uggh', 'ugh',\n",
    "                   'uh', 'uhh', 'uhm', 'um', 'umm', 'ummm', 'umph', 'unh', 'upadaisy',\n",
    "                   'upsadaisy', 'ur', 'urgh', 'vay', 'vayf', 'viva', 'voila', 'waa',\n",
    "                   'waaaaah', 'waah', 'wah', 'wahey', 'wassup', 'weee', 'welp',\n",
    "                   'wham', 'whamo', 'whee', 'whew', 'whizz', 'whoa',\n",
    "                   'whoo', 'whoopee','whoop', 'whoops', 'whoopsy', 'whoosh', 'woah', 'woo',\n",
    "                   'woohoo', 'wotcha', 'wotcher', 'wow', 'wowsers', 'wowsers',\n",
    "                   'wuzzup', 'wuzzup', 'wuzzup', 'ya', 'yabba', 'yada', 'yadda',\n",
    "                   'yak', 'yarooh', 'yay', 'yea', 'yeah', 'yech', 'yee', 'yeeeeaah',\n",
    "                   'yeehaw', 'yeow', 'yes', 'yessiree', 'yew', 'yikes', 'yippee',\n",
    "                   'yo', 'yoo', 'yoohoo', 'yow', 'yowza', 'yuck', 'yuh', 'zing',\n",
    "                   'zoiks', 'zomfg', 'zomg', 'zounds', 'zut']\n",
    "             \n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "type(spacy_stopwords)\n",
    "#spacy_exclude=['using','name','seem','give','make','keep','call','say','go','get','see','seems','seeming',\n",
    "#               'seemed','show','take','made','used','move','become','became','becoming','becomes','put','use']# serious\n",
    "\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words1 = get_stop_words('english')\n",
    "#print(type(stop_words1))\n",
    "#print()\n",
    "#print(stop_words1)\n",
    "lib_stopwords=set(stop_words1)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#print(type(ENGLISH_STOP_WORDS))\n",
    "#print()\n",
    "#print(set(ENGLISH_STOP_WORDS))\n",
    "#sklearn_exclude=['find','get','found','go','see','seem','seems','give','seemed','take','keep','show','put','made'] # system  cry\n",
    "sklearn_stopwords=set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "#spacy_stopwords.difference_update(set(spacy_exclude))\n",
    "#sklearn_stopwords.difference_update(set(sklearn_stopwords))\n",
    "#for removing \"just\" one item, use \"remove\"\n",
    "\n",
    "temp_1=set(nltk_stopwords)\n",
    "temp_1.update(lib_stopwords)\n",
    "temp_1.update(sklearn_stopwords)\n",
    "temp_1.update(spacy_stopwords)\n",
    "temp_1.update(set(english_alghabet))\n",
    "temp_1.update(set(numbers_remove)) \n",
    "temp_1.update(set(miscellaneous_remove))\n",
    "temp_1.update(set(interjection_remove))\n",
    "cachedStopWords=temp_1\n",
    "#print(cachedStopWords)\n",
    "cachedStopWords.update(['rt','be','will','was','were','is','am','are','have','has','had','do','does','done'])\n",
    "len(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemm=wordnet_lemmatizer.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "Stem=stemmer.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "#print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "#print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "#print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (text):\n",
    "    \n",
    "    #order of lines is important\n",
    "    \n",
    "    text=strip_tags(text)\n",
    "    #text=html.unescape(text)   # stripping or converting html entities \n",
    "    #text=saxutils.unescape(text)\n",
    "    \n",
    "    #convertings words that their lower and uper cases are different\n",
    "    text=re.sub(\" US | U\\.S\\. \", ' USA ', text) # before lower    \n",
    " \n",
    "    #converting\n",
    "    text = re.sub(\"“|”\", '''\"''', text)  #before next lines\n",
    "    text = re.sub(\"’|′|‘|`\", \"' \", text)  #before next lines\n",
    "    \n",
    "    #removing tabs and lines\n",
    "    text=re.sub('\\t|\\n', ' ', text)\n",
    "    \n",
    "    #converting lower_case\n",
    "    text = text.lower() \n",
    "    \n",
    "    #converting\n",
    "    text=re.sub('\\$|£|€|¥|dollar|dollars|yen|yens|euros', ' money ', text)   # not euro \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    #removing emoji\n",
    "    text = emoji_pattern.sub(r' ', text) \n",
    "\n",
    "    #removing emojis and non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', text)  \n",
    "    \n",
    "    #removeing http and https (URL)\n",
    "    text = re.sub(r'(http://|https://)\\S+', '', text)\n",
    "    \n",
    "    #removing www (URL)\n",
    "    text=re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    #removing targets\n",
    "    text=re.sub('( |^)@\\S+', '', text) \n",
    "    \n",
    "    #removing common expressions\n",
    "    text=re.sub(\"looking forward to|look forward to|make sure|kidding me|\\\n",
    "                |in my opinion|by the way,|as soon as possible|shaking my head|i don't know|I do not know|\\\n",
    "                |in real life|quote of the day|as far as i know|shake my head|\\\n",
    "                |to be honest|in other words|let me know|just kidding|hope that helps|hat tip|\\\n",
    "                |just like that|happy birthday|never mind|well-done|\\\n",
    "                |in my humble opinion|happy new year|you're welcome|you are welcome| \\\n",
    "                |it doesn't matter|it does not matter|i think|i wonder|do you think\", ' ', text)  \n",
    "    \n",
    "    #convertings\n",
    "    text=re.sub(\"can't\", 'cannot', text) # before other n't \n",
    "    text=re.sub(\"can not \", 'cannot ', text)  \n",
    "    text=re.sub(\"'ve\",' have', text)\n",
    "    text=re.sub(\"n't\",' not', text)\n",
    "    text=re.sub(\"'ll\",' will', text)\n",
    "    text=re.sub(\"'d\",' would', text)\n",
    "    text=re.sub(\"'re\",' are', text)\n",
    "    text=re.sub(\"i'm\",'i am', text)\n",
    "    text=re.sub(\"&\",' and ', text)\n",
    "    text=re.sub(\" w/ \",' with ', text)\n",
    "    text=re.sub(\" w/i | w/in \",' within ', text)\n",
    "    text=re.sub(\" w/o \",' without ', text)\n",
    "    text=re.sub(\" c/o \",' care of ', text)\n",
    "    text=re.sub(\" h/t \",' hat tip ', text)\n",
    "    text=re.sub(\" b/c \",' because ', text)\n",
    "#    text=re.sub(\"=\",' equals to ', text)\n",
    "    text=re.sub(\"=\",' = ', text)\n",
    "#    text=re.sub(\"\\+\",' plus ', text)\n",
    "    text=re.sub(\"\\+\",' + ', text)\n",
    "    text=re.sub(\"united states\",'usa', text)\n",
    "    text=re.sub(\"united kingdom\",'uk', text)\n",
    "    text=re.sub(\" the us \",' usa ', text)\n",
    "    text=re.sub(\"start-up|start_up\",'startup', text)\n",
    "    text=re.sub(\"u\\.s\\.a\", 'usa', text)  #try text=re.sub(\"u.s.a\", 'usa', text) with text=substantially \n",
    "    #text=re.sub(\"aka\", 'also known as', text)     \n",
    "    text=re.sub(\"'\",\" ' \", text)     \n",
    "    \n",
    "    #text= re.sub(\"(\\?)+\", '? ',text)     \n",
    "    #text= re.sub(\"(!)+\", '! ',text)     \n",
    "    #text= re.sub(\"(\\.\\.)+\", ' ',text)  \n",
    "    \n",
    "    text = \"\".join(lemmatize_sentence(text))\n",
    "    \n",
    "    #removing some special charachter  \n",
    "    text= re.sub(\"[\\\"\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}''_';•«»,@:~!\\=%&]+\", ' ',text) \n",
    "#    text= re.sub(\"[\\\"\\“\\”\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]\\.{}_`′’‘';•«,@:~!\\=%&]+\", ' ',text) \n",
    "    \n",
    "    #removing hashtag\n",
    "    text=re.sub('#', ' ', text) \n",
    "    \n",
    "    #removing numbers not attached to alphabets\n",
    "    text=re.sub(\"(^)(\\d+)?(\\.)?(\\d+)? \",' ',text)   #removing numer at the beginning\n",
    "    text=re.sub(\"(\\s)[0-9]?(\\.)?(\\d+) \",' ',text) #py6 and py9\n",
    "    text= re.sub(\" (\\.)(\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+) \", ' ',text)\n",
    "    text= re.sub(\" (\\d+)$\", ' ',text)\n",
    "\n",
    "    #text=re.sub(\"\\S+(\\d+) \",' ',text) # alphabet+digit (attached)\n",
    "    #text=re.sub(\" (\\d+)\\S+\",' ',text) # digit+alphabet (attached)\n",
    "    #text=re.sub(\" \\S+(\\d+)\\S+ \",' ',text) # alphabet+digit+alphabet (attached)\n",
    "    #text=re.sub(\"(\\d+)\",' ',text)  #removing any number anywhere but keeps \\. for decimal numbers\n",
    "\n",
    "    #removing space\n",
    "    text=re.sub('\\s+',' ',text)      \n",
    "\n",
    "    text= nltk.word_tokenize(text)\n",
    "    #text= text.split() #sometimes\n",
    "\n",
    "    #removing_stopwords \n",
    "    #text_without_sw = [word.lower() for word in text if word.lower() not in stopwords.words()] #very slow\n",
    "    text = [word for word in text if word not in cachedStopWords]\n",
    "\n",
    "    #lemmatization\n",
    "    #text= [ lemm(word, pos=\"v\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"n\") for word in text]\n",
    "    #text= [ lemm(word, pos=\"a\") for word in text]\n",
    "    \n",
    "    #stemming \n",
    "    #text = [Stem(word) for word in text]\n",
    "    \n",
    "    text=' '.join(text)\n",
    "    text=re.sub(\"''\",' ', text)    #since nltk.tokenize converts second \" to ''\"\n",
    "    text=re.sub(\"``\",' ', text)   # since nltk.tokenize converts first \" to \" ``\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like these book'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_sentence('I liked these books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT user_id, tweet from ent_world_tweets_ph2_light ;\") # ORDER BY floor(random() *47604376  LIMIT 3000000;\")\n",
    "#cur.execute(\"SELECT user_id, tweet from ent_2019_1000k \")\n",
    "rows_ent = cur.fetchall()\n",
    "# there is no repetative tweet\n",
    "con.close()\n",
    "\n",
    "print(len(rows_ent))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47604376, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "47604376\n",
      "Memory size of ent: 402267520\n"
     ]
    }
   ],
   "source": [
    "#df_ent = pandas.read_csv('/archives1/Datasets/TweetsWorld/ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "df_ent = pandas.read_csv('ent_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "print(df_ent.shape)\n",
    "print(df_ent.columns)\n",
    "#print(df_ent.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_ent0=list(df_ent[['user_id', 'tweet']].itertuples(index=False, name=None)) #rows_ent0\n",
    "#rows_ent= list(zip(df_ent.user_id, df_ent.tweet))\n",
    "#rows_ent=df_ent[['user_id','tweet']].apply(tuple, axis=1) \n",
    "del df_ent\n",
    "print(len(rows_ent0))  #rows_ent0\n",
    "print('Memory size of ent:',sys.getsizeof(rows_ent0)) #rows_ent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "memory size of ent: 81528056\n"
     ]
    }
   ],
   "source": [
    "#with open(\"rand_inds1.txt\", \"rb\") as fp:   \n",
    "#    rand_inds1=pickle.load(fp)\n",
    "\n",
    "rand_inds1=np.sort(random.sample(range(0, 47604376), 10000000))\n",
    "\n",
    "rows_ent=[]\n",
    "for i in rand_inds1:\n",
    "    rows_ent.append(rows_ent0[i])\n",
    "print(len(rows_ent) )  \n",
    "\n",
    "del rows_ent0\n",
    "\n",
    "with open(\"rand_inds1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(rand_inds1, fp , protocol=4)\n",
    "\n",
    "print('memory size of ent:',sys.getsizeof(rows_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT user_id, tweet, tweet_created_at from public_world_tweets_ph2_light;\") # ORDER BY floor(random() *72182875  LIMIT 4000000;\")\n",
    "#cur.execute(\"SELECT user_id, tweet from public_2019_1000k \")\n",
    "rows_public = cur.fetchall()\n",
    "con.close()\n",
    "\n",
    "print(len(rows_public))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72182875, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "72182875\n",
      "memory size of public: 644355008\n"
     ]
    }
   ],
   "source": [
    "df_public = pandas.read_csv('public_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "#df_public = pandas.read_csv('/archives1/Datasets/TweetsWorld/public_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False)#,warn_bad_lines=False)\n",
    "\n",
    "print(df_public.shape)\n",
    "print(df_public.columns)\n",
    "#print(df_public.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_public00=list(df_public[['user_id', 'tweet', 'tweet_created_at']].itertuples(index=False, name=None))\n",
    "#rows_public= list(zip(df_public.user_id, df_public.tweet))\n",
    "#rows_public=df_public[['user_id','tweet']].apply(tuple, axis=1) \n",
    "del df_public\n",
    "print(len(rows_public00))\n",
    "print('memory size of public:', sys.getsizeof(rows_public00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_public0=[]\n",
    "for i in rows_public00:\n",
    "    if ('2021' not in i[2] ) and ('2020-12' not in i[2]) and ('2020-11' not in i[2]) and ('2020-10' not in i[2]):\n",
    "        rows_public0.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53069041\n"
     ]
    }
   ],
   "source": [
    "print(len(rows_public0))\n",
    "del rows_public00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000000\n",
      "memory size of public: 103184048\n"
     ]
    }
   ],
   "source": [
    "#with open(\"rand_inds3.txt\", \"rb\") as fp:   \n",
    "#    rand_inds3=pickle.load(fp)\n",
    "    \n",
    "rand_inds3=np.sort(random.sample(range(0, len(rows_public0)), 12000000))  \n",
    "\n",
    "rows_public=[]\n",
    "for i in rand_inds3:\n",
    "    rows_public.append(rows_public0[i])\n",
    "print(len(rows_public))\n",
    "\n",
    "del rows_public0\n",
    "\n",
    "with open(\"rand_inds3.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(rand_inds3, fp , protocol=4)\n",
    "\n",
    "print('memory size of public:',sys.getsizeof(rows_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import psycopg2\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"Jafarsql\", host=\"localhost\", port=\"5432\")\n",
    "print(\"Database opened successfully\")\n",
    "\n",
    "cur = con.cursor()\n",
    "##cur.execute(\"SELECT user_id, tweet from mng_world_tweets_ph2_light ORDER BY floor(random() *46502302  LIMIT 3000000;\")\n",
    "cur.execute(\"SELECT user_id, tweet from mng_2019_1000k \")\n",
    "rows_mng = cur.fetchall()\n",
    "con.close()\n",
    "\n",
    "print(len(rows_mng))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46502302, 4)\n",
      "Index(['user_id', 'tweet', 'tweet_created_at', 'location_profile'], dtype='object')\n",
      "46502302\n",
      "memry size of mng: 402267520\n"
     ]
    }
   ],
   "source": [
    "df_mng = pandas.read_csv('mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#df_mng = pandas.read_csv('/archives1/Datasets/TweetsWorld/mng_tweets_world.csv', delimiter='\\t', na_values=\".\",error_bad_lines=False,warn_bad_lines=False)\n",
    "print(df_mng.shape)\n",
    "print(df_mng.columns)\n",
    "#print(df_mng.head()) # Preview the first 5 lines of the loaded data \n",
    "\n",
    "rows_mng0=list(df_mng[['user_id', 'tweet']].itertuples(index=False, name=None)) #rows_mng0\n",
    "#rows_mng= list(zip(df_mng.user_id, df_mng.tweet))\n",
    "#rows_mng=df_mng[['user_id','tweet']].apply(tuple, axis=1) \n",
    "del df_mng\n",
    "print(len(rows_mng0)) #rows_mng0\n",
    "print('memry size of mng:', sys.getsizeof(rows_mng0)) #rows_mng0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "memory size of mng: 81528056\n"
     ]
    }
   ],
   "source": [
    "#with open(\"rand_inds2.txt\", \"rb\") as fp:   \n",
    "#    rand_inds2=pickle.load(fp)\n",
    "    \n",
    "rand_inds2=np.sort(random.sample(range(0, 46502302), 10000000))\n",
    "\n",
    "rows_mng=[]\n",
    "for i in rand_inds2:\n",
    "    rows_mng.append(rows_mng0[i])\n",
    "print(len(rows_mng))\n",
    "\n",
    "del rows_mng0\n",
    "\n",
    "with open(\"rand_inds2.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(rand_inds2, fp , protocol=4)\n",
    "\n",
    "print('memory size of mng:',sys.getsizeof(rows_mng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24840.911117315292\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "ent_users_rows=[]\n",
    "ent_tweets_rows=[]\n",
    "for i in rows_ent:\n",
    "    ent_users_rows.append(i[0])\n",
    "    ent_tweets_rows.append(cleaning(i[1]))\n",
    "    \n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time.sleep(5*60)\n",
    "\n",
    "with open(\"ent_cleaning10m.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(ent_tweets_rows, fp , protocol=4)\n",
    "\n",
    "time.sleep(5*60)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./ALL/ent_cleaning1.txt\", \"rb\") as fp:\n",
    "#    ent_tweets_rows0 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_tweets_rows=[]    \n",
    "#for i in rand_inds1:\n",
    "#    ent_tweets_rows.append(ent_tweets_rows0[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_users_rows=[]\n",
    "#for i in rows_ent:\n",
    "#    ent_users_rows.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ent users: 23835\n"
     ]
    }
   ],
   "source": [
    "ent_users_rows_np=np.array(ent_users_rows)  \n",
    "ent_users=np.unique(ent_users_rows_np)\n",
    "print('number of ent users:',len(ent_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449189025\n"
     ]
    }
   ],
   "source": [
    "ent_all_words=[]\n",
    "for i in ent_tweets_rows:\n",
    "    ent_all_words.extend(i)\n",
    "print(len(ent_all_words))   \n",
    "\n",
    "dic_ent_user=dict()\n",
    "for user in ent_users_rows:\n",
    "    if user not in dic_ent_user:\n",
    "        dic_ent_user[user]=1\n",
    "    else:\n",
    "        dic_ent_user[user]+=1   \n",
    "sorted_x = sorted(dic_ent_user.items(), key=operator.itemgetter(1), reverse=False) \n",
    "f = open(\"ent_user.txt\", \"w\")\n",
    "for xx in sorted_x:\n",
    "    f.write(str(xx)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27684.34583926201\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "public_users_rows=[]\n",
    "public_tweets_rows=[]\n",
    "for i in rows_public:\n",
    "    public_users_rows.append(i[0])\n",
    "    public_tweets_rows.append(cleaning(i[1]))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time.sleep(5*60)\n",
    "\n",
    "with open(\"public_cleaning10m.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(public_tweets_rows, fp , protocol=4)\n",
    "\n",
    "time.sleep(5*60)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./ALL/public_cleaning1.txt\", \"rb\") as fp:   \n",
    "#    public_tweets_rows0 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#public_tweets_rows=[]    \n",
    "#for i in rand_inds3:\n",
    "#    public_tweets_rows.append(public_tweets_rows0[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#public_users_rows=[]\n",
    "#for i in rows_public:\n",
    "#    public_users_rows.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of public users: 34547\n"
     ]
    }
   ],
   "source": [
    "public_users_rows_np=np.array(public_users_rows)  \n",
    "public_users=np.unique(public_users_rows_np)\n",
    "print('number of public users:',len(public_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400687117\n"
     ]
    }
   ],
   "source": [
    "public_all_words=[]\n",
    "for i in public_tweets_rows:\n",
    "    public_all_words.extend(i)\n",
    "print(len(public_all_words))\n",
    "\n",
    "dic_public_user=dict()\n",
    "for user in public_users_rows:\n",
    "    if user not in dic_public_user:\n",
    "        dic_public_user[user]=1\n",
    "    else:\n",
    "        dic_public_user[user]+=1   \n",
    "sorted_y = sorted(dic_public_user.items(), key=operator.itemgetter(1), reverse=False) \n",
    "f = open(\"public_user.txt\", \"w\")\n",
    "for yy in sorted_y:\n",
    "    f.write(str(yy)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24487.88109111786\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "mng_users_rows=[]\n",
    "mng_tweets_rows=[]\n",
    "for i in rows_mng:\n",
    "    mng_users_rows.append(i[0])\n",
    "    mng_tweets_rows.append(cleaning(i[1]))\n",
    "\n",
    "print( time.time() - t0)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time.sleep(5*60)\n",
    "\n",
    "with open(\"mng_cleaning10.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(mng_tweets_rows, fp , protocol=4)\n",
    "\n",
    "time.sleep(5*60)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./ALL/mng_cleaning1.txt\", \"rb\") as fp:   \n",
    "#    mng_tweets_rows0 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_tweets_rows=[]    \n",
    "#for i in rand_inds2:\n",
    "#    mng_tweets_rows.append(mng_tweets_rows0[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mng_users_rows=[]\n",
    "#for i in rows_mng:\n",
    "#    mng_users_rows.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mng users: 23176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mng_users_rows_np=np.array(mng_users_rows)  \n",
    "mng_users=np.unique(mng_users_rows_np)\n",
    "print('number of mng users:',len(mng_users))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415653078\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mng_all_words=[]\n",
    "for i in mng_tweets_rows:\n",
    "    mng_all_words.extend(i)\n",
    "print(len(mng_all_words))\n",
    "\n",
    "dic_mng_user=dict()\n",
    "for user in mng_users_rows:\n",
    "    if user not in dic_mng_user:\n",
    "        dic_mng_user[user]=1\n",
    "    else:\n",
    "        dic_mng_user[user]+=1  \n",
    "        \n",
    "sorted_z = sorted(dic_mng_user.items(), key=operator.itemgetter(1), reverse=False) \n",
    "f = open(\"mng_user.txt\", \"w\")\n",
    "for zz in sorted_z:\n",
    "    f.write(str(zz)+'\\n')\n",
    "f.close()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1737.2096745967865\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "dic_ent_1 = dict()\n",
    "for k in ent_users:\n",
    "    bb=np.where(ent_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "\n",
    "    words_k=set()\n",
    "\n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(ent_tweets_rows[kk])\n",
    "        for word in tweet_words:\n",
    "            if word not in words_k:\n",
    "                words_k.add(word)\n",
    "                if word not in dic_ent_1:\n",
    "                    dic_ent_1[word] =1\n",
    "                else:\n",
    "                    dic_ent_1[word] +=1\n",
    "\n",
    "dic_ent_1=dict(sorted(dic_ent_1.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3860.4243943691254\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "dic_ent_2 = dict()\n",
    "for k in ent_users:\n",
    "    bb=np.where(ent_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(ent_tweets_rows[kk])\n",
    "        \n",
    "        tweet_combinations_2=set()\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 2):\n",
    "            tweet_combinations_2.add(combi)       \n",
    "        \n",
    "        for combi in tweet_combinations_2:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.add(combi)\n",
    "                if combi not in dic_ent_2:\n",
    "                    dic_ent_2[combi] =1\n",
    "                else:\n",
    "                    dic_ent_2[combi] +=1\n",
    "                    \n",
    "dic_ent_2=dict(sorted(dic_ent_2.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "dic_ent_3 = dict()\n",
    "\n",
    "for k in ent_users:\n",
    "    bb=np.where(ent_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(ent_tweets_rows[kk])\n",
    "        \n",
    "        tweet_combinations_3=set()\n",
    "#        ccc=np.sort(list(set(tweet_words)))\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 3):\n",
    "            tweet_combinations_3.add(combi)       \n",
    "        \n",
    "        for combi in tweet_combinations_3:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.add(combi)\n",
    "                if combi not in dic_ent_3:\n",
    "                    dic_ent_3[combi] =1\n",
    "                else:\n",
    "                    dic_ent_3[combi] +=1\n",
    "                    \n",
    "dic_ent_3=dict(sorted(dic_ent_3.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2267.5659940242767\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "dic_public_1 = dict()\n",
    "for k in public_users:\n",
    "    bb=np.where(public_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    words_k=set()\n",
    "\n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(public_tweets_rows[kk])\n",
    "        for word in tweet_words:\n",
    "            if word not in words_k:\n",
    "                words_k.add(word)\n",
    "                if word not in dic_public_1:\n",
    "                    dic_public_1[word] =1\n",
    "                else:\n",
    "                    dic_public_1[word] +=1\n",
    "\n",
    "dic_public_1=dict(sorted(dic_public_1.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3820.6746406555176\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "dic_public_2 = dict()\n",
    "for k in public_users:\n",
    "    bb=np.where(public_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(public_tweets_rows[kk])\n",
    "        \n",
    "        tweet_combinations_2=set()\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 2):\n",
    "            tweet_combinations_2.add(combi)       \n",
    "        \n",
    "        for combi in tweet_combinations_2:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.add(combi)\n",
    "                if combi not in dic_public_2:\n",
    "                    dic_public_2[combi] =1\n",
    "                else:\n",
    "                    dic_public_2[combi] +=1\n",
    "                    \n",
    "dic_public_2=dict(sorted(dic_public_2.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "dic_public_3 = dict()\n",
    "\n",
    "for k in public_users:\n",
    "    bb=np.where(public_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(public_tweets_rows[kk])\n",
    "        \n",
    "        tweet_combinations_3=set()\n",
    "#        ccc=np.sort(list(set(tweet_words)))\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 3):\n",
    "            tweet_combinations_3.add(combi)       \n",
    "        \n",
    "        for combi in tweet_combinations_3:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.add(combi)\n",
    "                if combi not in dic_public_3:\n",
    "                    dic_public_3[combi] =1\n",
    "                else:\n",
    "                    dic_public_3[combi] +=1\n",
    "                    \n",
    "dic_public_3=dict(sorted(dic_public_3.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1892.6011273860931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "dic_mng_1 = dict()\n",
    "for k in mng_users:\n",
    "    bb=np.where(mng_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    words_k=set()\n",
    "\n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(mng_tweets_rows[kk])\n",
    "        for word in tweet_words:\n",
    "            if word not in words_k:\n",
    "                words_k.add(word)\n",
    "                if word not in dic_mng_1:\n",
    "                    dic_mng_1[word] =1\n",
    "                else:\n",
    "                    dic_mng_1[word] +=1\n",
    "\n",
    "dic_mng_1=dict(sorted(dic_mng_1.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3921.61696600914\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "dic_mng_2 = dict()\n",
    "for k in mng_users:\n",
    "    bb=np.where(mng_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(mng_tweets_rows[kk])\n",
    "        \n",
    "        tweet_combinations_2=set()\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 2):\n",
    "            tweet_combinations_2.add(combi)       \n",
    "        \n",
    "        for combi in tweet_combinations_2:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.add(combi)\n",
    "                if combi not in dic_mng_2:\n",
    "                    dic_mng_2[combi] =1\n",
    "                else:\n",
    "                    dic_mng_2[combi] +=1\n",
    "                    \n",
    "dic_mng_2=dict(sorted(dic_mng_2.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "\n",
    "dic_mng_3 = dict()\n",
    "\n",
    "for k in mng_users:\n",
    "    bb=np.where(mng_users_rows_np==k)\n",
    "    ind=list(bb[0])\n",
    "    combinations_k=set()\n",
    "    \n",
    "    for kk in ind:\n",
    "        tweet_words=nltk.word_tokenize(mng_tweets_rows[kk])\n",
    "        \n",
    "        tweet_combinations_3=set()\n",
    "#        ccc=np.sort(list(set(tweet_words)))\n",
    "        for combi in itertools.combinations(np.sort(list(set(tweet_words))), 3):\n",
    "            tweet_combinations_3.add(combi)       \n",
    "        \n",
    "        for combi in tweet_combinations_3:\n",
    "            if combi not in combinations_k:\n",
    "                combinations_k.add(combi)\n",
    "                if combi not in dic_mng_3:\n",
    "                    dic_mng_3[combi] =1\n",
    "                else:\n",
    "                    dic_mng_3[combi] +=1\n",
    "                    \n",
    "dic_mng_3=dict(sorted(dic_mng_3.items(),key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "print( time.time() - t0)\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"ent1.txt\", \"rb\") as fp:   \n",
    "    dic_ent_1 = pickle.load(fp)\n",
    "\n",
    "with open(\"ent2.txt\", \"rb\") as fp:   \n",
    "    dic_ent_2 = pickle.load(fp)\n",
    "\n",
    "with open(\"mng1.txt\", \"rb\") as fp:   \n",
    "    dic_mng_1 = pickle.load(fp)\n",
    "\n",
    "with open(\"mng2.txt\", \"rb\") as fp:   \n",
    "    dic_mng_2 = pickle.load(fp)\n",
    "\n",
    "with open(\"public1.txt\", \"rb\") as fp:   \n",
    "    dic_public_1 = pickle.load(fp)\n",
    "\n",
    "with open(\"public2.txt\", \"rb\") as fp:   \n",
    "    dic_public_2 = pickle.load(fp)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"ent1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(dic_ent_1, fp , protocol=4)\n",
    "\n",
    "with open(\"ent2.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(dic_ent_2, fp , protocol=4)\n",
    "\n",
    "with open(\"mng1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(dic_mng_1, fp , protocol=4)\n",
    "\n",
    "with open(\"mng2.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(dic_mng_2, fp , protocol=4)\n",
    "\n",
    "with open(\"public1.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(dic_public_1, fp , protocol=4)\n",
    "\n",
    "with open(\"public2.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(dic_public_2, fp , protocol=4)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dics:\n",
      "41943144\n",
      "2684354664\n",
      "41943144\n",
      "2684354664\n",
      "41943144\n",
      "2684354664\n"
     ]
    }
   ],
   "source": [
    "print('size of dics:')\n",
    "print(sys.getsizeof(dic_ent_1))\n",
    "print(sys.getsizeof(dic_ent_2))\n",
    "print(sys.getsizeof(dic_mng_1))\n",
    "print(sys.getsizeof(dic_mng_2))\n",
    "print(sys.getsizeof(dic_public_1))\n",
    "print(sys.getsizeof(dic_public_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the_remove_low=0\n",
    "for word,freq in dic_ent_1.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_ent_1[word]=freq\n",
    "    else:\n",
    "        dic_ent_1[word]=0 #break\n",
    "\n",
    "for word,freq in dic_mng_1.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_mng_1[word]=freq\n",
    "    else:\n",
    "        dic_mng_1[word]=0\n",
    "\n",
    "for word,freq in dic_public_1.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_public_1[word]=freq\n",
    "    else:\n",
    "        dic_public_1[word]=0\n",
    "\n",
    "for word,freq in dic_ent_2.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_ent_2[word]=freq\n",
    "    else:\n",
    "        dic_ent_2[word]=0\n",
    "\n",
    "for word,freq in dic_mng_2.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_mng_2[word]=freq\n",
    "    else:\n",
    "        dic_mng_2[word]=0\n",
    "\n",
    "for word,freq in dic_public_2.items():\n",
    "    if freq > the_remove_low:\n",
    "        dic_public_2[word]=freq\n",
    "    else:\n",
    "        dic_public_2[word]=0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ent users:     23835\n",
      "number of public users:  34547\n",
      "number of mng users:     23176\n"
     ]
    }
   ],
   "source": [
    "print('number of ent users:    ',len(ent_users))\n",
    "print('number of public users: ',len(public_users))\n",
    "print('number of mng users:    ',len(mng_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct words in ent:    1324188\n",
      "number of distinct words in public: 1004704\n",
      "number of distinct words in mng:    1351444\n"
     ]
    }
   ],
   "source": [
    "print('number of distinct words in ent:   ',len(dic_ent_1)) \n",
    "print('number of distinct words in public:', len(dic_public_1)) \n",
    "print('number of distinct words in mng:   ', len(dic_mng_1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten words in ent:\n",
      "[('time', 22447), ('like', 22407), ('people', 22240), ('know', 22130), ('need', 22105), ('new', 21813), ('want', 21811), ('work', 21745), ('look', 21578), ('love', 21527)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f=open(\"ent_words_freqs.txt\",\"w\")\n",
    "for item in dic_ent_1.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "\n",
    "first_ten = list(dic_ent_1.items())[:10]\n",
    "print('first ten words in ent:')\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_ent_1_values=list(dic_ent_1.values())\n",
    "dic_ent_1_keys=list(dic_ent_1.keys())\n",
    "\n",
    "#print('length of dic_ent_1:             ',len(dic_ent_1_values))\n",
    "#print('mean of dic_ent_1:               ',mean(dic_ent_1_values))\n",
    "#print('median of dic_ent_1:             ',median(dic_ent_1_values))\n",
    "#print('mode of dic_ent_1:               ',mode(dic_ent_1_values))\n",
    "#var=variance(dic_ent_1_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_ent_1:           ',var)\n",
    "#print('standard deviation of dic_ent_1: ',std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten words in mng:\n",
      "[('time', 21811), ('like', 21647), ('new', 21488), ('work', 21308), ('need', 21288), ('know', 21241), ('look', 21200), ('people', 21055), ('want', 20946), ('love', 20831)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f=open(\"mng_words_freqs.txt\",\"w\")\n",
    "for item in dic_mng_1.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "\n",
    "first_ten = list(dic_mng_1.items())[:10]\n",
    "print('first ten words in mng:',)\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_mng_1_values=list(dic_mng_1.values())\n",
    "dic_mng_1_keys=list(dic_mng_1.keys())\n",
    "\n",
    "#print('length of dic_mng_1:             ',len(dic_mng_1_values))\n",
    "#print('mean of dic_mng_1:               ',mean(dic_mng_1_values))\n",
    "#print('median of dic_mng_1:             ',median(dic_mng_1_values))\n",
    "#print('mode of dic_mng_1:               ',mode(dic_mng_1_values))\n",
    "#var=variance(dic_mng_1_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_mng_1:           ',var)\n",
    "#print('standard deviation of dic_mng_1: ',std)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten words in public\n",
      "[('like', 31467), ('time', 30532), ('love', 30360), ('know', 30291), ('want', 29994), ('people', 29768), ('need', 29504), ('look', 29470), ('new', 28649), ('think', 28472)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f=open(\"public_words_freqs.txt\",\"w\")\n",
    "for item in dic_public_1.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "\n",
    "first_ten = list(dic_public_1.items())[:10]\n",
    "print('first ten words in public')\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_public_1_values=list(dic_public_1.values())\n",
    "dic_public_1_keys=list(dic_public_1.keys())\n",
    "\n",
    "#print('length of dic_public_1:             ',len(dic_public_1_values))\n",
    "#print('mean of dic_public_1:               ',mean(dic_public_1_values))\n",
    "#print('median of dic_public_1:             ',median(dic_public_1_values))\n",
    "#print('mode of dic_public_1:               ',mode(dic_public_1_values))\n",
    "#var=variance(dic_public_1_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_public_1:           ',var)\n",
    "#print('standard deviation of dic_public_1: ',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique words in mng (without public):  955361\n",
      "length of unique words in public (without mng):  608621\n",
      "length of common words in mng and public:        396083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mng_1_unique=set(dic_mng_1_keys)-set(dic_public_1_keys)\n",
    "public_1_unique_exclude_mng=set(dic_public_1_keys)-set(dic_mng_1_keys)\n",
    "mng_public_1_common=set(dic_mng_1_keys) & set(dic_public_1_keys)\n",
    "\n",
    "print('length of unique words in mng (without public): ',len(mng_1_unique))\n",
    "print('length of unique words in public (without mng): ',len(public_1_unique_exclude_mng))\n",
    "print('length of common words in mng and public:       ',len(mng_public_1_common))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique words in ent (without public):  936949\n",
      "length of unique words in public (without ent):  617465\n",
      "length of common words in ent and public:        387239\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ent_1_unique=set(dic_ent_1_keys)-set(dic_public_1_keys)\n",
    "public_1_unique_exclude_ent=set(dic_public_1_keys)-set(dic_ent_1_keys)\n",
    "ent_public_1_common=set(dic_ent_1_keys) & set(dic_public_1_keys)\n",
    "\n",
    "print('length of unique words in ent (without public): ',len(ent_1_unique))\n",
    "print('length of unique words in public (without ent): ',len(public_1_unique_exclude_ent))\n",
    "print('length of common words in ent and public:       ',len(ent_public_1_common))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dic_ent_1_unique_frequent=dict()\n",
    "for i in list(ent_1_unique):\n",
    "    temp=dic_ent_1[i] \n",
    "    if temp>=2:\n",
    "        dic_ent_1_unique_frequent[i]=temp\n",
    "        \n",
    "dic_ent_1_unique_frequent=dict(sorted(dic_ent_1_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f1=open(\"ent unique words and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_ent_1_unique_frequent.items():\n",
    "    f1.write(str(item)+'\\n')\n",
    "f1.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_mng_1_unique_frequent=dict()\n",
    "for i in list(mng_1_unique):\n",
    "    temp=dic_mng_1[i] \n",
    "    if temp>=2:\n",
    "        dic_mng_1_unique_frequent[i]=temp\n",
    "        \n",
    "dic_mng_1_unique_frequent=dict(sorted(dic_mng_1_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f2=open(\"frequent mng unique words and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_mng_1_unique_frequent.items():\n",
    "    f2.write(str(item)+'\\n')\n",
    "f2.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_public_1_unique_exclude_ent_frequent=dict()\n",
    "for i in list(public_1_unique_exclude_ent):\n",
    "    temp=dic_public_1[i] \n",
    "    if temp>=2:\n",
    "        dic_public_1_unique_exclude_ent_frequent[i]=temp\n",
    "        \n",
    "dic_public_1_unique_exclude_ent_frequent=dict(sorted(dic_public_1_unique_exclude_ent_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f3=open(\"frequent public unique words (excluding ent words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_1_unique_exclude_ent_frequent.items():\n",
    "    f3.write(str(item)+'\\n')\n",
    "f3.close  \n",
    "pass\n",
    "\n",
    "\n",
    "dic_public_1_unique_exclude_mng_frequent=dict()\n",
    "for i in list(public_1_unique_exclude_mng):\n",
    "    temp=dic_public_1[i] \n",
    "    if temp>=2:\n",
    "        dic_public_1_unique_exclude_mng_frequent[i]=temp\n",
    "        \n",
    "dic_public_1_unique_exclude_mng_frequent=dict(sorted(dic_public_1_unique_exclude_mng_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f4=open(\"frequent public unique words (excluding mng words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_1_unique_exclude_mng_frequent.items():\n",
    "    f4.write(str(item)+'\\n')\n",
    "f4.close  \n",
    "pass\n",
    "\n",
    "\n",
    "dic_ent_public_1_frequent=dict()\n",
    "for word in ent_public_1_common:\n",
    "    e=dic_ent_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    dic_ent_public_1_frequent[word]=[e,p]\n",
    "    \n",
    "dic_ent_public_1_frequent=dict(sorted(dic_ent_public_1_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f5=open(\"frequent ent words (common with public) and thier frequencies.txt\",\"w\")\n",
    "f5.write('first number shows frequency of the combination in ent and second number shows that of in public')\n",
    "for item in dic_ent_public_1_frequent.items():\n",
    "    e=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (e>50 and p<10):\n",
    "#        print(item[0], e,p)\n",
    "        f5.write(str(item)+'\\n')\n",
    "f5.close  \n",
    "pass    \n",
    "\n",
    "\n",
    "dic_mng_public_1_frequent=dict()\n",
    "for word in mng_public_1_common:\n",
    "    m=dic_mng_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    dic_mng_public_1_frequent[word]=[m,p]\n",
    "    \n",
    "dic_mng_public_1_frequent=dict(sorted(dic_mng_public_1_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f6=open(\"frequent mng words (common with public) and thier frequencies.txt\",\"w\")\n",
    "f6.write('first number shows frequency of the combination in mng and second number shows that of in public')\n",
    "for item in dic_mng_public_1_frequent.items():\n",
    "    m=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (m>50 and p<10):\n",
    "#        print(item[0], m,p)\n",
    "        f6.write(str(item)+'\\n')\n",
    "f6.close \n",
    "'''\n",
    "pass   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mng_public_1_freq=[]\n",
    "for word in dic_mng_1:\n",
    "    m=dic_mng_1[word]\n",
    "    try:\n",
    "        p=dic_public_1[word]\n",
    "    except:\n",
    "        p=0\n",
    "    if m>5 or p>5:    \n",
    "        mng_public_1_freq.append([m,p])\n",
    "    \n",
    "with open(\"mng_public_1_freq.txt\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(mng_public_1_freq, fp)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of distinct combinations of two-words in ent:    ', len(dic_ent_2))\n",
    "print('number of distinct combinations of two-words in mng:    ',len(dic_mng_2))\n",
    "print('number of distinct combinations of two-words in public: ',len(dic_public_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('like', 'look'), 15804), (('like', 'people'), 13432), (('know', 'people'), 12233), (('feel', 'like'), 11830), (('medium', 'social'), 11596), (('know', 'like'), 11436), (('like', 'time'), 11107), (('know', 'need'), 11093), (('know', 'want'), 11090), (('people', 'want'), 11019)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f=open(\"ent_combinations_freqs.txt\",\"w\")\n",
    "for item in dic_ent_2.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "\n",
    "first_ten = list(dic_ent_2.items())[:10]\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_ent_2_values=list(dic_ent_2.values())\n",
    "dic_ent_2_keys=list(dic_ent_2.keys())\n",
    "\n",
    "#print('length of dic_ent_2:             ',len(dic_ent_2_values))\n",
    "#print('mean of dic_ent_2:               ',mean(dic_ent_2_values))\n",
    "#print('median of dic_ent_2:             ',median(dic_ent_2_values))\n",
    "#print('mode of dic_ent_2:               ',mode(dic_ent_2_values))\n",
    "#var=variance(dic_ent_2_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_ent_2:           ',var)\n",
    "#print('standard deviation of dic_ent_2: ',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('like', 'look'), 16596), (('feel', 'like'), 12330), (('like', 'people'), 12099), (('like', 'time'), 11604), (('know', 'like'), 11243), (('know', 'people'), 10721), (('know', 'want'), 10325), (('know', 'need'), 10224), (('hard', 'work'), 9586), (('like', 'need'), 9581)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f=open(\"mng_combinations_freqs.txt\",\"w\")\n",
    "for item in dic_mng_2.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "\n",
    "first_ten = list(dic_mng_2.items())[:10]\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_mng_2_values=list(dic_mng_2.values())\n",
    "dic_mng_2_keys=list(dic_mng_2.keys())\n",
    "\n",
    "#print('length of dic_mng_2:             ',len(dic_mng_2_values))\n",
    "#print('mean of dic_mng_2:               ',mean(dic_mng_2_values))\n",
    "#print('median of dic_mng_2:             ',median(dic_mng_2_values))\n",
    "#print('mode of dic_mng_2:               ',mode(dic_mng_2_values))\n",
    "#var=variance(dic_mng_2_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_mng_2:           ',var)\n",
    "#print('standard deviation of dic_mng_2: ',std)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('like', 'look'), 22270), (('feel', 'like'), 19739), (('like', 'people'), 18184), (('know', 'like'), 17140), (('like', 'time'), 15216), (('know', 'people'), 14980), (('like', 'want'), 14677), (('like', 'love'), 13657), (('know', 'want'), 13103), (('like', 'think'), 12594)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f=open(\"public_combinations_freqs.txt\",\"w\")\n",
    "for item in dic_public_2.items():\n",
    "    f.write(str(item)+'\\n')\n",
    "f.close\n",
    "\n",
    "first_ten = list(dic_public_2.items())[:10]\n",
    "print(first_ten)\n",
    "print()\n",
    "\n",
    "dic_public_2_values=list(dic_public_2.values())\n",
    "dic_public_2_keys=list(dic_public_2.keys())\n",
    "\n",
    "#print('length of dic_public_2:             ',len(dic_public_2_values))\n",
    "#print('mean of dic_public_2:               ',mean(dic_public_2_values))\n",
    "#print('median of dic_public_2:             ',median(dic_public_2_values))\n",
    "#print('mode of dic_public_2:               ',mode(dic_public_2_values))\n",
    "#var=variance(dic_public_2_values)\n",
    "#std=np.sqrt(var)\n",
    "#print('variance of dic_public_2:           ',var)\n",
    "#print('standard deviation of dic_public_2: ',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique combinations_2 in ent (without public):  49760667\n",
      "length of unique combinations_2 in public (without ent):  31213340\n",
      "length of common combinations_2 in ent and public:        18027457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ent_2_unique=set(dic_ent_2_keys)-set(dic_public_2_keys)\n",
    "public_2_unique_exclude_ent=set(dic_public_2_keys)-set(dic_ent_2_keys)\n",
    "ent_public_2_common=set(dic_ent_2_keys) & set(dic_public_2_keys)\n",
    "\n",
    "print('length of unique combinations_2 in ent (without public): ',len(ent_2_unique))\n",
    "print('length of unique combinations_2 in public (without ent): ',len(public_2_unique_exclude_ent))\n",
    "print('length of common combinations_2 in ent and public:       ',len(ent_public_2_common))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique combinations_2 in mng (without public):  48234878\n",
      "length of unique combinations_2 in public (without mng):  31159314\n",
      "length of common combinations_2 in mng and public:        18081483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mng_2_unique=set(dic_mng_2_keys)-set(dic_public_2_keys)\n",
    "public_2_unique_exclude_mng=set(dic_public_2_keys)-set(dic_mng_2_keys)\n",
    "mng_public_2_common=set(dic_mng_2_keys) & set(dic_public_2_keys)\n",
    "\n",
    "print('length of unique combinations_2 in mng (without public): ',len(mng_2_unique))\n",
    "print('length of unique combinations_2 in public (without mng): ',len(public_2_unique_exclude_mng))\n",
    "print('length of common combinations_2 in mng and public:       ',len(mng_public_2_common))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dic_ent_2_unique_frequent=dict()\n",
    "for i in list(ent_2_unique):\n",
    "    temp=dic_ent_2[i] \n",
    "    if temp>=2:\n",
    "        dic_ent_2_unique_frequent[i]=temp\n",
    "        \n",
    "dic_ent_2_unique_frequent=dict(sorted(dic_ent_2_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f1=open(\"ent unique combinations and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_ent_2_unique_frequent.items():\n",
    "    f1.write(str(item)+'\\n')\n",
    "f1.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_mng_2_unique_frequent=dict()\n",
    "for i in list(mng_2_unique):\n",
    "    temp=dic_mng_2[i] \n",
    "    if temp>=2:\n",
    "        dic_mng_2_unique_frequent[i]=temp\n",
    "        \n",
    "dic_mng_2_unique_frequent=dict(sorted(dic_mng_2_unique_frequent.items(),key=operator.itemgetter(1),reverse=True)) \n",
    "\n",
    "f2=open(\"frequent mng unique 2-words combinations and thier frequencies.txt\",\"w+\")\n",
    "for item in dic_mng_2_unique_frequent.items():\n",
    "    f2.write(str(item)+'\\n')\n",
    "f2.close\n",
    "pass\n",
    "\n",
    "\n",
    "dic_public_2_unique_exclude_ent_frequent=dict()\n",
    "for i in list(public_2_unique_exclude_ent):\n",
    "    temp=dic_public_2[i] \n",
    "    if temp>=2:\n",
    "        dic_public_2_unique_exclude_ent_frequent[i]=temp\n",
    "        \n",
    "dic_public_2_unique_exclude_ent_frequent=dict(sorted(dic_public_2_unique_exclude_ent_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f3=open(\"frequent public unique 2-words combinations (excluding ent words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_2_unique_exclude_ent_frequent.items():\n",
    "    f3.write(str(item)+'\\n')\n",
    "f3.close  \n",
    "pass\n",
    "\n",
    "dic_public_2_unique_exclude_mng_frequent=dict()\n",
    "for i in list(public_2_unique_exclude_mng):\n",
    "    temp=dic_public_2[i] \n",
    "    if temp>=2:\n",
    "        dic_public_2_unique_exclude_mng_frequent[i]=temp\n",
    "        \n",
    "dic_public_2_unique_exclude_mng_frequent=dict(sorted(dic_public_2_unique_exclude_mng_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f4=open(\"frequent public unique 2-words combinations (excluding mng words) and thier frequencies.txt\",\"w\")\n",
    "for item in dic_public_2_unique_exclude_mng_frequent.items():\n",
    "    f4.write(str(item)+'\\n')\n",
    "f4.close  \n",
    "pass\n",
    "\n",
    "dic_ent_public_2_frequent=dict()\n",
    "for word in ent_public_2_common:\n",
    "    e=dic_ent_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    dic_ent_public_2_frequent[word]=[e,p]\n",
    "    \n",
    "dic_ent_public_2_frequent=dict(sorted(dic_ent_public_2_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f5=open(\"frequent ent 2-words combinations (common with public) and thier frequencies.txt\",\"w\")\n",
    "f5.write('first number shows frequency of the combination in ent and second number shows that of in public')\n",
    "for item in dic_ent_public_2_frequent.items():\n",
    "    e=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (e>50 and p<10):\n",
    "#        print(item[0], e,p)\n",
    "        f5.write(str(item)+'\\n')\n",
    "f5.close  \n",
    "pass    \n",
    "\n",
    "dic_mng_public_2_frequent=dict()\n",
    "for word in mng_public_2_common:\n",
    "    m=dic_mng_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    dic_mng_public_2_frequent[word]=[m,p]\n",
    "    \n",
    "dic_mng_public_2_frequent=dict(sorted(dic_mng_public_2_frequent.items(),key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "f6=open(\"frequent mng 2-words combinations (common with public) and thier frequencies.txt\",\"w\")\n",
    "f6.write('first number shows frequency of the combination in mng and second number shows that of in public')\n",
    "for item in dic_mng_public_2_frequent.items():\n",
    "    m=item[1][0]\n",
    "    p=item[1][1]\n",
    "    if (m>50 and p<10):\n",
    "#        print(item[0], m,p)\n",
    "        f6.write(str(item)+'\\n')\n",
    "f6.close \n",
    "'''\n",
    "pass      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rank_temp=1\n",
    "freq=dic_ent_1_values[0]\n",
    "rank_ent_1=dict()\n",
    "for i,j in dic_ent_1.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_ent_1[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_ent_1[i]=rank_temp\n",
    "\n",
    "#list(rank_ent_1.items())[0:10]\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_ent_2_values[0]\n",
    "rank_ent_2=dict()\n",
    "for i,j in dic_ent_2.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_ent_2[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_ent_2[i]=rank_temp\n",
    "\n",
    "#list(rank_ent_2.items())[0:10]\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_mng_1_values[0]\n",
    "rank_mng_1=dict()\n",
    "for i,j in dic_mng_1.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_mng_1[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_mng_1[i]=rank_temp\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_mng_2_values[0]\n",
    "rank_mng_2=dict()\n",
    "for i,j in dic_mng_2.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_mng_2[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_mng_2[i]=rank_temp\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_public_1_values[0]\n",
    "rank_public_1=dict()\n",
    "for i,j in dic_public_1.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_public_1[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_public_1[i]=rank_temp\n",
    "\n",
    "rank_temp=1\n",
    "freq=dic_public_2_values[0]\n",
    "rank_public_2=dict()\n",
    "for i,j in dic_public_2.items():\n",
    "    if j!=freq:\n",
    "        rank_temp +=1\n",
    "        rank_public_2[i]=rank_temp\n",
    "        freq=j\n",
    "    else:\n",
    "        rank_public_2[i]=rank_temp\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pdf = fpdf.FPDF(format='letter')\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', size=12)\n",
    "temp='word,    ' + 'frequency in ent,    '+'rank in ent,   '+'frequency  in public,  '+'rank in public'\n",
    "pdf.write(5,temp)\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "for word in dic_ent_1_keys:\n",
    "    if word in ent_public_1_common:\n",
    "        if dic_ent_1[word] !=1:\n",
    "            e=dic_ent_1[word]\n",
    "            p=dic_public_1[word]\n",
    "            re=rank_ent_1[word]\n",
    "            rp=rank_public_1[word]\n",
    "            temp=(word,e,re,p,rp)\n",
    "            pdf.write(5,str(temp))\n",
    "            pdf.ln()\n",
    "\n",
    "pdf.output(\"ent_public_word_ranks.pdf\")   \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pdf = fpdf.FPDF(format='letter')\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', size=12)\n",
    "temp='combination,    ' + 'frequency in ent,    '+'rank in ent,   '+'frequency  in public,  '+'rank in public'\n",
    "pdf.write(5,temp)\n",
    "pdf.ln()\n",
    "pdf.ln()\n",
    "\n",
    "for combi in dic_ent_2_keys:\n",
    "    if dic_ent_2[combi] !=1:\n",
    "        if combi in ent_public_2_common:\n",
    "            e=dic_ent_2[combi]\n",
    "            p=dic_public_2[combi]\n",
    "            re=rank_ent_2[combi]\n",
    "            rp=rank_public_2[combi]\n",
    "            temp=(combi,e,re,p,rp)\n",
    "            pdf.write(5,str(temp))\n",
    "            pdf.ln()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pdf.output(\"ent_public_combinations_rank.pdf\")   \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "frequency_ent_public_1=dict()\n",
    "for word in ent_public_1_common:\n",
    "    e=dic_ent_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    frequency_ent_public_1[word]=[e,p]\n",
    "\n",
    "frequency_mng_public_1=dict()\n",
    "for word in mng_public_1_common:\n",
    "    m=dic_mng_1[word]\n",
    "    p=dic_public_1[word]\n",
    "    frequency_mng_public_1[word]=[m,p]\n",
    "\n",
    "frequency_ent_public_2=dict()\n",
    "for word in ent_public_2_common:\n",
    "    e=dic_ent_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    frequency_ent_public_2[word]=[e,p]\n",
    "\n",
    "frequency_mng_public_2=dict()\n",
    "for word in mng_public_2_common:\n",
    "    m=dic_mng_2[word]\n",
    "    p=dic_public_2[word]\n",
    "    frequency_mng_public_2[word]=[m,p]\n",
    "\n",
    "frequency_ent_1_unique=dict()\n",
    "for word in ent_1_unique:\n",
    "    e=dic_ent_1[word]\n",
    "    frequency_ent_1_unique[word]=e\n",
    "\n",
    "frequency_mng_1_unique=dict()\n",
    "for word in mng_1_unique:\n",
    "    m=dic_mng_1[word]\n",
    "    frequency_mng_1_unique[word]=m\n",
    "\n",
    "frequency_public_1_unique_exclude_ent=dict()\n",
    "for word in public_1_unique_exclude_ent:\n",
    "    p=dic_public_1[word]\n",
    "    frequency_public_1_unique_exclude_ent[word]=p\n",
    "\n",
    "frequency_public_1_unique_exclude_mng=dict()\n",
    "for word in public_1_unique_exclude_mng:\n",
    "    p=dic_public_1[word]\n",
    "    frequency_public_1_unique_exclude_mng[word]=p\n",
    "\n",
    "frequency_ent_2_unique=dict()\n",
    "for word in ent_2_unique:\n",
    "    e=dic_ent_2[word]\n",
    "    frequency_ent_2_unique[word]=e\n",
    "\n",
    "frequency_mng_2_unique=dict()\n",
    "for word in mng_2_unique:\n",
    "    m=dic_mng_2[word]\n",
    "    frequency_mng_2_unique[word]=m\n",
    "\n",
    "frequency_public_2_unique_exclude_ent=dict()\n",
    "for word in public_2_unique_exclude_ent:\n",
    "    p=dic_public_2[word]\n",
    "    frequency_public_2_unique_exclude_ent[word]=p\n",
    "\n",
    "frequency_public_2_unique_exclude_mng=dict()\n",
    "for word in public_2_unique_exclude_mng:\n",
    "    p=dic_public_2[word]\n",
    "    frequency_public_2_unique_exclude_mng[word]=p\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "th_h_public=50\n",
    "th_l_public=5\n",
    "th_h_ent=50\n",
    "th_l_ent=5\n",
    "th_h_mng=50\n",
    "th_l_mng=5\n",
    "th_h=50\n",
    "th_l=5\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_ent_2=dict()\n",
    "for combi_2 in dic_ent_2:\n",
    "    \n",
    "    if combi_2 in ent_2_unique:\n",
    "        if dic_ent_2[combi_2]>th_h:\n",
    "            #print(combi_2)\n",
    "            weight_ent_2[combi_2]=2\n",
    "        else:\n",
    "            weight_ent_2[combi_2]=0\n",
    "                \n",
    "    else:\n",
    "        [e,p]=frequency_ent_public_2[combi_2]\n",
    "        if e>th_h and p<th_l:\n",
    "            #print(e,p,combi_2)\n",
    "            weight_ent_2[combi_2]=2\n",
    "        elif e<th_l and p>th_h:\n",
    "            weight_ent_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_ent_2[combi_2]=0\n",
    "'''       \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th=0.03\n",
    "weight_ent_2=dict()\n",
    "len_ent_user=len(ent_users)\n",
    "len_public_user=len(public_users)\n",
    "for combi_2 in dic_ent_2:\n",
    "    \n",
    "    if combi_2 in ent_2_unique:\n",
    "#        value= dic_ent_2[combi_2]/len_ent_user\n",
    "#        if   value >= th:\n",
    "#            weight_ent_2[combi_2]=2\n",
    "#            print(combi_2)\n",
    "#        else:\n",
    "#            weight_ent_2[combi_2]=0\n",
    "        weight_ent_2[combi_2]=2*dic_ent_2[combi_2]/len_ent_user\n",
    "\n",
    "    else:\n",
    "#        [e,p]=frequency_ent_public_2[combi_2]\n",
    "#        value=((e/len_ent_user)-(p/len_public_user) )\n",
    "#        if  value >= th:\n",
    "#        if (e/len_ent_user)>=th and (p/len_public_user)<=th :\n",
    "#            weight_ent_2[combi_2]=2\n",
    "#        elif  value <=-th:\n",
    "#        elif (e/len_ent_user)<=th and (p/len_public_user)>=th :\n",
    "#            weight_ent_2[combi_2]=-2\n",
    "#        else:\n",
    "#            weight_ent_2[combi_2]=0\n",
    "#        weight_ent_2[combi_2]=2*((e/len_ent_user)-(p/len_public_user) )\n",
    "        weight_ent_2[combi_2]=2*((dic_ent_2[combi_2]/len_ent_user)-(dic_public_2[combi_2]/len_public_user) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_mng_2=dict()\n",
    "for combi_2 in dic_mng_2:\n",
    "    \n",
    "    if combi_2 in mng_2_unique:\n",
    "        if dic_mng_2[combi_2]>th_h:\n",
    "            #print(combi_2)\n",
    "            weight_mng_2[combi_2]=2\n",
    "        else:\n",
    "            weight_mng_2[combi_2]=0\n",
    "                \n",
    "    else:\n",
    "        [m,p]=frequency_mng_public_2[combi_2]\n",
    "        if m>th_h and p<th_l:\n",
    "            #print(m,p,combi_2)\n",
    "            weight_mng_2[combi_2]=2\n",
    "        elif m<th_l and p>th_h:\n",
    "            weight_mng_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_mng_2[combi_2]=0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#th=0.03\n",
    "weight_mng_2=dict()\n",
    "len_mng_user=len(mng_users)\n",
    "len_public_user=len(public_users)\n",
    "for combi_2 in dic_mng_2:\n",
    "    \n",
    "    if combi_2 in mng_2_unique:\n",
    "#        value= dic_mng_2[combi_2]/len_mng_user\n",
    "#        if value >= th:\n",
    "#            weight_mng_2[combi_2]=2\n",
    "#            print(combi_2)\n",
    "#        else:\n",
    "#            weight_mng_2[combi_2]=0\n",
    "        weight_mng_2[combi_2]=2*dic_mng_2[combi_2]/len_mng_user\n",
    "\n",
    "    else:\n",
    "#        [m,p]=frequency_mng_public_2[combi_2]\n",
    "#        value= ((m/len_mng_user)-(p/len_public_user) )\n",
    "#        if value >= th:\n",
    "#        if (m/len_mng_user)>= th and (p/len_public_user)<= th: \n",
    "#            weight_mng_2[combi_2]=2\n",
    "#        elif  value <= -th:\n",
    "#        elif (m/len_mng_user)<=th and (p/len_public_user)>=th:\n",
    "#            weight_mng_2[combi_2]=-2\n",
    "#        else:\n",
    "#            weight_mng_2[combi_2]=0\n",
    "#       weight_mng_2[combi_2]=2*((m/len_mng_user)-(p/len_public_user) )\n",
    "        weight_mng_2[combi_2]=2*((dic_mng_2[combi_2]/len_mng_user)-(dic_public_2[combi_2]/len_public_user) )\n",
    "\n",
    "pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_public_2=dict()\n",
    "for combi_2 in dic_public_2:\n",
    "    \n",
    "    if (combi_2 in public_2_unique_exclude_ent) and (combi_2 in public_2_unique_exclude_mng):\n",
    "        if dic_public_2[combi_2]>50:\n",
    "            #print(combi_2)\n",
    "            weight_public_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_public_2[combi_2]=0\n",
    "                \n",
    "    else:\n",
    "#        p=dic_public_2[combi_2]\n",
    "        if combi_2 in mng_public_2:\n",
    "            [m,p]=frequency_mng_public_2[combi_2]\n",
    "        else:\n",
    "            m=0\n",
    "        if combi_2 in ent_public_2:  \n",
    "            [e,p]=frequency_ent_public_2[combi_2]\n",
    "        else:\n",
    "            e=0\n",
    "\n",
    "        if p>50 and m<5 and e<5:\n",
    "            #print(m,p,combi_2)\n",
    "            weight_public_2[combi_2]=-2\n",
    "        elif p<5 and (e>50:\n",
    "            weight_mng_2[combi_2]=-2\n",
    "        else:\n",
    "            weight_mng_2[combi_2]=0\n",
    "''' \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# collecting words from relevant and irrelevant combination so that we can find the relevant and irrelevant single words based on a relation\n",
    "agg_2_words_rel_ent=set()\n",
    "agg_2_words_irrel_ent=set()\n",
    "for combi_2,weight in weight_ent_2.items():\n",
    "    if weight==2:\n",
    "        agg_2_words_rel_ent.add(combi_2)\n",
    "    elif weight==-2:\n",
    "        agg_2_words_irrel_ent.add(combi_2)\n",
    "\n",
    "\n",
    "# collecting words from relevant and irrelevant combination so that we can find the relevant and irrelevant single words based on a relation\n",
    "agg_2_words_rel_mng=set()\n",
    "agg_2_words_irrel_mng=set()\n",
    "for combi_2,weight in weight_mng_2.items():\n",
    "    if weight==2:\n",
    "        agg_2_words_rel_mng.add(combi_2)\n",
    "    elif weight==-2:\n",
    "        agg_2_words_irrel_mng.add(combi_2)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_ent_1=dict()\n",
    "for word in dic_ent_1:\n",
    "    \n",
    "    if (word in ent_1_unique):\n",
    "        if (dic_ent_1[word]>th_h): # and (word in agg_2_words_rel_ent):  \n",
    "            weight_ent_1[word]=1\n",
    "        else:\n",
    "            weight_ent_1[word]=0\n",
    "                \n",
    "    else:\n",
    "        [e,p]=frequency_ent_public_1[word]\n",
    "        if (e>th_h and p<th_l): # and (word in agg_2_words_rel_ent):\n",
    "            #print(e,p,word)\n",
    "            weight_ent_1[word]=1\n",
    "        elif (e<th_l and p>th_h):# and (word in agg_2_words_irrel_ent):\n",
    "            weight_ent_1[word]=-1\n",
    "        else:\n",
    "            weight_ent_1[word]=0\n",
    "'''    \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th=0.03\n",
    "weight_ent_1=dict()\n",
    "len_ent_user=len(ent_users)\n",
    "len_public_user=len(public_users)\n",
    "for word in dic_ent_1:\n",
    "    \n",
    "    if (word in ent_1_unique):\n",
    "#        value= dic_ent_1[word]/len_ent_user\n",
    "#        if value >=th:\n",
    "#            weight_ent_1[word]=1\n",
    "#            print(word)\n",
    "#        else:\n",
    "#            weight_ent_1[word]=0  \n",
    "        weight_ent_1[word]=dic_ent_1[word]/len_ent_user\n",
    "    else:\n",
    "#        [e,p]=frequency_ent_public_1[word]\n",
    "#        value= (e/len_ent_user)-(p/len_public_user)\n",
    "#        if  value >=th:\n",
    "#        if (e/len_ent_user)>=th and (p/len_public_user)<=th: \n",
    "#            weight_ent_1[word]=1\n",
    "#        elif   value <= -th:\n",
    "#        elif  (e/len_ent_user)<=th and (p/len_public_user)>=th: \n",
    "#            weight_ent_1[word]=-1\n",
    "#        else:\n",
    "#            weight_ent_1[word]=0\n",
    "#        weight_ent_1[word]=(e/len_ent_user)-(p/len_public_user)\n",
    "        weight_ent_1[word]=(dic_ent_1[word]/len_ent_user)-(dic_public_1[word]/len_public_user)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weight_mng_1=dict()\n",
    "for word in dic_mng_1:\n",
    "    \n",
    "    if (word in mng_1_unique):\n",
    "        if (dic_mng_1[word]>th_h): # and (word in agg_2_words_rel_mng):  \n",
    "            weight_mng_1[word]=1\n",
    "        else:\n",
    "            weight_mng_1[word]=0\n",
    "                \n",
    "    else:\n",
    "        [m,p]=frequency_mng_public_1[word]\n",
    "        if (m>th_h and p<th_l): # and (word in agg_2_words_rel_mng):\n",
    "            #print(m,p,word)\n",
    "            weight_mng_1[word]=1\n",
    "        elif (m<th_l and p>th_h):# and (word in agg_2_words_irrel_mng):\n",
    "            weight_mng_1[word]=-1\n",
    "        else:\n",
    "            weight_mng_1[word]=0\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#th=0.03\n",
    "weight_mng_1=dict()\n",
    "len_mng_user=len(mng_users)\n",
    "len_public_user=len(public_users)\n",
    "for word in dic_mng_1:\n",
    "    \n",
    "    if (word in mng_1_unique):      \n",
    "#        value= dic_mng_1[word]/len_mng_user\n",
    "#        if  value >=th:\n",
    "#            weight_mng_1[word]=1\n",
    "#        else:\n",
    "#            weight_mng_1[word]=0\n",
    "        weight_mng_1[word]=dic_mng_1[word]/len_mng_user \n",
    "        \n",
    "    else:\n",
    "#        [m,p]=frequency_mng_public_1[word]\n",
    "#        value= (m/len_mng_user)-(p/len_public_user)\n",
    "#        if  value >=th:\n",
    "#        if (m/len_mng_user)>=th and (p/len_public_user)<=th: \n",
    "#            weight_mng_1[word]=1\n",
    "#        elif  value <= -th:\n",
    "#        elif (m/len_mng_user)<=th and (p/len_public_user)>=th:\n",
    "#            weight_mng_1[word]=-1\n",
    "#        else:\n",
    "#            weight_mng_1[word]=0\n",
    "#        weight_mng_1[word]=(m/len_mng_user)-(p/len_public_user)\n",
    "        weight_mng_1[word]=(dic_mng_1[word]/len_mng_user)-(dic_public_1[word]/len_public_user)\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ttt={('a','b'):25,('b','c'):40,('a','d'):30}\n",
    "\n",
    "for t in ttt:\n",
    "    x=set(t)-set('a')\n",
    "    print(x)\n",
    "\n",
    "strange=['a','b']\n",
    "\n",
    "add_strange=dict()\n",
    "for word in strange:\n",
    "#    print(word)\n",
    "    add=0\n",
    "    for k in ttt:\n",
    "#        print(k)\n",
    "        if word in k:\n",
    "            xxx=set(k)-set(word)\n",
    "            print(xxx)\n",
    "            if xxx in agg_2_words_rel:\n",
    "                print(k)\n",
    "                add += ttt[k]\n",
    "#                print(dic_ent_2[k])\n",
    "    add_strange[word]=add  \n",
    "'''  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ent_tweets_scores=dict()\n",
    "for k,tweet in enumerate(ent_tweets_rows):\n",
    "    words=nltk.word_tokenize(tweet)\n",
    "    score_words=0\n",
    "    if len(words) != 0:\n",
    "        for word in words:\n",
    "            score_words += weight_ent_1[word]       \n",
    "\n",
    "        score_words_norm=score_words/len(words)\n",
    "        len_words=len(words)\n",
    "        \n",
    "    else:\n",
    "        len_words=0\n",
    "        score_words_norm=0\n",
    "\n",
    "\n",
    "    score_combi_2=0\n",
    "    if len(set(words)) >= 2:\n",
    "        combinations_2=[]\n",
    "        for combi in itertools.combinations(np.sort(list(set(words))), 2):\n",
    "            combinations_2.append(combi)\n",
    "    \n",
    "        for combi in combinations_2:\n",
    "            score_combi_2 += weight_ent_2[combi]\n",
    "   \n",
    "\n",
    "        score_combinations2_norm=score_combi_2/len(combinations_2)\n",
    "        len_combination_2=len(combinations_2)           \n",
    "    else:\n",
    "        len_combination_2=0\n",
    "        score_combinations2_norm=0        \n",
    "        \n",
    "\n",
    "#    if len_words != 0:\n",
    "#        dic_ent_tweets_scores[k]= (score_words+score_combi_2)/(len_combination_2+len_words)\n",
    "#    else:\n",
    "#        dic_ent_tweets_scores[k]= 0\n",
    "\n",
    "    dic_ent_tweets_scores[k]=  (score_combinations2_norm +  score_words_norm)\n",
    "    if len_combination_2<10:\n",
    "        dic_ent_tweets_scores[k]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic_ent_tweets_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-11c1fc24bf41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdic_ent_tweets_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic_ent_tweets_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic_ent_tweets_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ment_tweets_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic_ent_tweets_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ment_tweets_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dic_ent_tweets_scores' is not defined"
     ]
    }
   ],
   "source": [
    "dic_ent_tweets_scores=dict(sorted(dic_ent_tweets_scores.items(),key=operator.itemgetter(1),reverse=True))\n",
    "print(len(dic_ent_tweets_scores))\n",
    "\n",
    "ent_tweets_score = list(dic_ent_tweets_scores.items())[0:200000]\n",
    "ent_tweets_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mng_tweets_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-57a004dc47b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdic_mng_tweets_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmng_tweets_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mng_tweets_rows' is not defined"
     ]
    }
   ],
   "source": [
    "dic_mng_tweets_scores=dict()\n",
    "for k,tweet in enumerate(mng_tweets_rows):\n",
    "    ss=tweet\n",
    "\n",
    "    words=nltk.word_tokenize(tweet)\n",
    "    score_words=0\n",
    "    if len(words) != 0:\n",
    "        for word in words:\n",
    "            score_words += weight_mng_1[word]\n",
    "            \n",
    "        score_words_norm=score_words/len(words)\n",
    "        len_words=len(words)\n",
    "    else:\n",
    "        len_words=0\n",
    "        score_words_norm=0\n",
    "    \n",
    "    \n",
    "    score_combi_2=0\n",
    "    if len(set(words)) >= 2:\n",
    "        combinations_2=[]\n",
    "        for combi in itertools.combinations(np.sort(list(set(words))), 2):\n",
    "            combinations_2.append(combi)\n",
    "    \n",
    "        for combi in combinations_2:\n",
    "            score_combi_2 += weight_mng_2[combi]\n",
    "            \n",
    "        score_combinations2_norm=score_combi_2/len(combinations_2)\n",
    "        len_combination_2=len(combinations_2)           \n",
    "    else:\n",
    "        len_combination_2=0\n",
    "        score_combinations2_norm=0        \n",
    "\n",
    "#    if len_words != 0:\n",
    "#        dic_mng_tweets_scores[k]= (score_words+score_combi_2)/(len_combination_2+len_words)\n",
    "#    else:\n",
    "#        dic_mng_tweets_scores[k]= 0\n",
    "        \n",
    "    dic_mng_tweets_scores[k]=   (score_combinations2_norm +  score_words_norm)\n",
    "    if len_combination_2<10:\n",
    "        dic_mng_tweets_scores[k]=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_mng_tweets_scores=dict(sorted(dic_mng_tweets_scores.items(),key=operator.itemgetter(1),reverse=True))\n",
    "print(len(dic_mng_tweets_scores))\n",
    "\n",
    "mng_tweets_score = list(dic_mng_tweets_scores.items())[0:200000]\n",
    "mng_tweets_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets with positive score:  200000\n"
     ]
    }
   ],
   "source": [
    "id_ent_rel=[]\n",
    "for score_id in ent_tweets_score:\n",
    "    if score_id[1]>ent_tweets_score[0][1]/3:\n",
    "        id_ent_rel.append(score_id[0])\n",
    "    else:\n",
    "        break\n",
    "print('number of tweets with positive score: ', len(id_ent_rel))\n",
    "tweets_rel_ent=set()\n",
    "i_id=0\n",
    "\n",
    "#pdf_tw = fpdf.FPDF(format='letter')\n",
    "#pdf_tw.add_page()\n",
    "#pdf_tw.set_font(\"Arial\", 'B', size=18)\n",
    "#pdf_tw.write(5,'50000 first tweets that have been detected as relevant for ent:')\n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.ln()\n",
    "#print('2000 first tweets that have been detected as relevant for ent. In each row, the first item is the user\\\n",
    "#   id and the second item is the original (raw) tweet. The repetitive tweets including retweets have been removed.\\n')\n",
    "\n",
    "#pdf_tw.set_font(\"Arial\",  size=12)\n",
    "\n",
    "seed_tweets_ent=[]\n",
    "\n",
    "for id_tweet in id_ent_rel:\n",
    "    \n",
    "    temp = rows_ent[id_tweet][1]\n",
    "    temp = strip_tags(temp)\n",
    "    temp = re.sub(\"“|”\", '''\"''', temp)  \n",
    "    temp = re.sub(\"’|′|‘|`\", \"'\", temp)  \n",
    "    temp = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', temp)  \n",
    "    temp = temp.lower() \n",
    "    temp = re.sub(r'(http://|https://)\\S+', '', temp) \n",
    "    temp = re.sub(r'www\\.\\S+', '', temp)\n",
    "    temp = re.sub('( |^)@\\S+', '', temp)  \n",
    "    #temp= re.sub(\"[\\\"\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘'_;•«»,@:~!\\=%&]+\", ' ',temp) \n",
    "    temp = temp.encode('utf8').decode('ISO-8859-1') # decode('ISO-8859-1') or decode('latin-1') or decode('windows-1252')\n",
    "\n",
    "    if temp not in tweets_rel_ent:\n",
    "        tweets_rel_ent.add(temp)\n",
    "        seed_tweets_ent.append(rows_ent[id_tweet][1])\n",
    "#        seed_tweets_ent.append(rand_inds1(id_tweet))\n",
    "        i_id +=1\n",
    "#        pdf_tw.write(5,str(temp))\n",
    "#        pdf_tw.ln()\n",
    "#        pdf_tw.write(5,'-----------')\n",
    "#        pdf_tw.ln()\n",
    "    if i_id==50000:\n",
    "        break\n",
    "        \n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.output(name=\"ent_relevant_tweets.pdf\")   \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ent lenght: 50000\n"
     ]
    }
   ],
   "source": [
    "print('seed ent lenght:',len(seed_tweets_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets with positive score:  200000\n"
     ]
    }
   ],
   "source": [
    "id_mng_rel=[]\n",
    "for score_id in mng_tweets_score:\n",
    "    if score_id[1]>mng_tweets_score[0][1]/3:\n",
    "        id_mng_rel.append(score_id[0])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print('number of tweets with positive score: ', len(id_mng_rel))\n",
    "tweets_rel_mng=set()\n",
    "i_id=0\n",
    "\n",
    "#pdf_tw = fpdf.FPDF(format='letter')\n",
    "#pdf_tw.add_page()\n",
    "#pdf_tw.set_font(\"Arial\", 'B', size=18)\n",
    "#pdf_tw.write(5,'50000 first tweets that have been detected as relevant for mng:')\n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.ln()\n",
    "#print('2000 first tweets that have been detected as relevant for mng. In each row, the first item is the user\\\n",
    "#  id and the second item is the original (raw) tweet. The repetitive tweets including retweets have been removed.\\n')\n",
    "\n",
    "#pdf_tw.set_font(\"Arial\",size=12)\n",
    "\n",
    "seed_tweets_mng=[]\n",
    "\n",
    "for id_tweet in id_mng_rel:\n",
    "    temp = rows_mng[id_tweet][1]\n",
    "    temp = strip_tags(temp)\n",
    "    temp = re.sub(\"“|”\", '''\"''', temp)  \n",
    "    temp = re.sub(\"’|′|‘|`\", \"'\", temp)  \n",
    "    temp = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', temp)  \n",
    "    temp = temp.lower() \n",
    "    temp = re.sub(r'(http://|https://)\\S+', '', temp) \n",
    "    temp = re.sub(r'www\\.\\S+', '', temp)\n",
    "    temp = re.sub('( |^)@\\S+', '', temp) \n",
    "    #temp= re.sub(\"[\\\"\\+\\-\\|\\*\\?\\(\\)\\/\\\\\\^\\[\\]``<>\\.{}`′’‘'_;•«»,@:~!\\=%&]+\", ' ',temp) \n",
    "    temp = temp.encode('utf8').decode('ISO-8859-1') # decode('ISO-8859-1') or decode('latin-1') or decode('windows-1252')\n",
    "\n",
    "    if temp not in tweets_rel_mng:\n",
    "        tweets_rel_mng.add(temp)\n",
    "        seed_tweets_mng.append(rows_mng[id_tweet][1])\n",
    "#        seed_tweets_mng.append(rand_inds2(id_tweet))\n",
    "        i_id +=1\n",
    "#        pdf_tw.write(5,str(temp))\n",
    "#        pdf_tw.ln()\n",
    "#        pdf_tw.write(5,'-----------')\n",
    "#        pdf_tw.ln()\n",
    "    if i_id==50000:\n",
    "        break\n",
    "            \n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.output(\"mng_relevant_tweets.pdf\")   \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed mng lenght: 50000\n"
     ]
    }
   ],
   "source": [
    "print('seed mng lenght:',len(seed_tweets_mng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dic_ent_tweets_scores_reverse=dict(sorted(dic_ent_tweets_scores.items(),key=operator.itemgetter(1),reverse=False))\n",
    "print(len(dic_ent_tweets_scores_reverse))\n",
    "\n",
    "ent_tweets_score_reverse = list(dic_ent_tweets_scores_reverse.items())[0:200000]\n",
    "ent_tweets_score_reverse[0]\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets with negative score:  184171\n"
     ]
    }
   ],
   "source": [
    "id_ent_irrel=[]\n",
    "for score_id in ent_tweets_score_reverse:\n",
    "    if score_id[1]<0:\n",
    "        id_ent_irrel.append(score_id[0])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print('number of tweets with negative score: ', len(id_ent_irrel))\n",
    "tweets_irrel_ent=set() \n",
    "i_id=0\n",
    "\n",
    "#pdf = fpdf.FPDF(format='letter')\n",
    "#pdf.add_page()\n",
    "#pdf_tw.set_font(\"Arial\", 'B', size=18)\n",
    "#pdf_tw.write(5,'2000 first tweets that have been detected as irrelevant in ent set:')\n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.ln()\n",
    "#print('100 first tweets that have been detected as irrelevant for ent. In each row, the first item is the user\\\n",
    "#   id and the second item is the original (raw) tweet. The repetitive tweets including retweets have been removed.\\n')\n",
    "\n",
    "#pdf_tw.set_font(\"Arial\",  size=12)\n",
    "\n",
    "seed_irrel_tweets_ent=[]\n",
    "\n",
    "for id_tweet in id_ent_irrel:\n",
    "    \n",
    "    temp = rows_ent[id_tweet][1]\n",
    "    temp = strip_tags(temp)\n",
    "    temp = re.sub(\"“|”\", '''\"''', temp)  \n",
    "    temp = re.sub(\"’|′|‘|`\", \"'\", temp)  \n",
    "    temp = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', temp)  \n",
    "    temp = temp.lower() \n",
    "    temp = re.sub(r'(http://|https://)\\S+', '', temp) \n",
    "    temp = re.sub(r'www\\.\\S+', '', temp)\n",
    "    temp = re.sub('( |^)@\\S+', '', temp)         \n",
    "    temp = temp.encode('utf8').decode('ISO-8859-1') # decode('ISO-8859-1') or decode('latin-1') or decode('windows-1252')\n",
    "\n",
    "    if temp not in tweets_irrel_ent:\n",
    "        tweets_irrel_ent.add(temp)\n",
    "        seed_irrel_tweets_ent.append(temp)\n",
    "#        seed_irrel_tweets_ent.append(rows_ent[id_tweet][1])\n",
    "        i_id +=1\n",
    "#        pdf_tw.write(5,str(temp))\n",
    "#        pdf_tw.ln()\n",
    "#        pdf_tw.write(5,'-----------')\n",
    "#        pdf_tw.ln()\n",
    "    if i_id==50000:\n",
    "        break\n",
    "        \n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.output(\"ent_irrelevant_tweets.pdf\")   \n",
    "        \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dic_mng_tweets_scores_reverse=dict(sorted(dic_mng_tweets_scores.items(),key=operator.itemgetter(1),reverse=False))\n",
    "print(len(dic_mng_tweets_scores_reverse))\n",
    "\n",
    "mng_tweets_score_reverse = list(dic_mng_tweets_scores_reverse.items())[0:200000]\n",
    "mng_tweets_score_reverse[0]\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets with positive score:  183817\n"
     ]
    }
   ],
   "source": [
    "id_mng_irrel=[]\n",
    "for score_id in mng_tweets_score_reverse:\n",
    "    if score_id[1]<0:\n",
    "        id_mng_irrel.append(score_id[0])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print('number of tweets with positive score: ', len(id_mng_irrel))\n",
    "tweets_irrel_mng=set()\n",
    "i_id=0\n",
    "\n",
    "#pdf = fpdf.FPDF(format='letter')\n",
    "#pdf.add_page()\n",
    "#pdf_tw.set_font(\"Arial\", 'B', size=18)\n",
    "#pdf_tw.write(5,'2000 first tweets that have been detected as irrelevant in mng set:')\n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.ln()\n",
    "#print('100 first tweets that have been detected as irrelevant for mng. In each row, the first item is the user\\\n",
    "#   id and the second item is the original (raw) tweet. The repetitive tweets including retweets have been removed.\\n')\n",
    "\n",
    "#pdf_tw.set_font(\"Arial\",  size=12)\n",
    "\n",
    "seed_irrel_tweets_mng=[]\n",
    "\n",
    "for id_tweet in id_mng_irrel:\n",
    "    \n",
    "    temp = rows_mng[id_tweet][1]\n",
    "    temp = strip_tags(temp)\n",
    "    temp = re.sub(\"“|”\", '''\"''', temp)  \n",
    "    temp = re.sub(\"’|′|‘|`\", \"'\", temp)  \n",
    "    temp = re.sub(r'[^\\x00-\\x7F]+|,Ä¶',' ', temp)  \n",
    "    temp = temp.lower() \n",
    "    temp = re.sub(r'(http://|https://)\\S+', '', temp) \n",
    "    temp = re.sub(r'www\\.\\S+', '', temp)\n",
    "    temp = re.sub('( |^)@\\S+', '', temp)         \n",
    "    temp = temp.encode('utf8').decode('ISO-8859-1') # decode('ISO-8859-1') or decode('latin-1') or decode('windows-1252')\n",
    "\n",
    "    if temp not in tweets_irrel_mng:\n",
    "        tweets_irrel_mng.add(temp)\n",
    "        seed_irrel_tweets_mng.append(temp)\n",
    "#        seed_irrel_tweets_mng.append(rows_mng[id_tweet][1])\n",
    "        i_id +=1\n",
    "#        pdf_tw.write(5,str(temp))\n",
    "#        pdf_tw.ln()\n",
    "#        pdf_tw.write(5,'-----------')\n",
    "#        pdf_tw.ln()\n",
    "    if i_id==50000:\n",
    "        break\n",
    "        \n",
    "#pdf_tw.ln()\n",
    "#pdf_tw.output(\"mng_irrelevant_tweets.pdf\")              \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"seed_tweets_mng10m.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(seed_tweets_mng, fp , protocol=4 )\n",
    "\n",
    "with open(\"seed_tweets_ent10m.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(seed_tweets_ent, fp, protocol=4)\n",
    "\n",
    "with open(\"seed_irrel_tweets_mng10m.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(seed_irrel_tweets_mng, fp , protocol=4)\n",
    "\n",
    "with open(\"seed_irrel_tweets_ent10m.txt\", \"wb\") as fp:   \n",
    "  pickle.dump(seed_irrel_tweets_ent, fp , protocol=4)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
